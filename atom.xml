<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>翼叶知秋</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.chentyit.com/"/>
  <updated>2020-01-07T05:13:48.931Z</updated>
  <id>https://www.chentyit.com/</id>
  
  <author>
    <name>Chen Tianyi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>集群环境Jar包冲突解决方案</title>
    <link href="https://www.chentyit.com/2019/12/20/%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83Jar%E5%8C%85%E5%86%B2%E7%AA%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://www.chentyit.com/2019/12/20/集群环境Jar包冲突解决方案/</id>
    <published>2019-12-20T05:12:34.000Z</published>
    <updated>2020-01-07T05:13:48.931Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>开发环境：Maven 3.6 || jdk 1.8（解决方案亲测有效）</p></blockquote><p><img src="/2019/12/20/集群环境Jar包冲突解决方案/D:/Blog/myblog/source/_posts/Maven项目加载不到Mybatis的xml文件报异常的解决方案/christian-lambert-1452519-unsplash.jpg" alt="插图"></p><blockquote><p>摄影：<a href="https://unsplash.com/photos/XR0kq2VDIUo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener">Christian Lambert</a>，来自<a href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener">Unsplash</a></p></blockquote><a id="more"></a><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>集成环境中（比如大数据的 CDH，Oozie）有配套的第三方 Jar 包，在自己编写的工具中使用了相同的包，但是版本不同，就会在运行是报错，包的版本产生冲突</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchFieldError: INSTANCE</span><br></pre></td></tr></table></figure><h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p>使用 <strong>maven-shade-plugin</strong> 对需要的第三方包重命名并重新打包，<strong>映射</strong>成自己定义的名字</p><h2 id="解决步骤"><a href="#解决步骤" class="headerlink" title="解决步骤"></a>解决步骤</h2><h3 id="在-pom-xml-文件中添加-shade-插件"><a href="#在-pom-xml-文件中添加-shade-插件" class="headerlink" title="在 pom.xml 文件中添加 shade 插件"></a>在 pom.xml 文件中添加 shade 插件</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">createDependencyReducedPom</span>&gt;</span>false<span class="tag">&lt;/<span class="name">createDependencyReducedPom</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- Run shade goal on package phase --&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>shade<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">relocations</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>org.elasticsearch<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>my.elasticsearch<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                            <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>org.apache.http<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>my.apache.http<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                            <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.fasterxml.jackson<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>my.fasterxml.jackson<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">relocations</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                        <span class="comment">&lt;!-- 配置主类 --&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">transformers</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">transformer</span></span></span><br><span class="line"><span class="tag">                                         <span class="attr">implementation</span>=<span class="string">"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.xxx.app.EsApp<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">transformer</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">transformers</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 配置编译插件 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>主要配置：</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">relocations</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>org.elasticsearch<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>my.elasticsearch<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>org.apache.http<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>my.apache.http<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.fasterxml.jackson<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>my.fasterxml.jackson<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">relocations</span>&gt;</span></span><br></pre></td></tr></table></figure><p>将官方包的名称及路径修改成为自定义的名称，防止冲突</p><h3 id="重新编译打包"><a href="#重新编译打包" class="headerlink" title="重新编译打包"></a>重新编译打包</h3><p>推荐使用 <strong>luyten</strong> 这个工具打开编译打包好的 jar 包，在使用到对应类的 class 文件中就可以看到，原来的 <code>org.elasticsearch</code> 都被替换成 <code>my.elasticsearch</code> 了，其他同理，也就不会出现在集群环境重产生 jar 包冲突的错误了</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;开发环境：Maven 3.6 || jdk 1.8（解决方案亲测有效）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/2019/12/20/集群环境Jar包冲突解决方案/D:/Blog/myblog/source/_posts/Maven项目加载不到Mybatis的xml文件报异常的解决方案/christian-lambert-1452519-unsplash.jpg&quot; alt=&quot;插图&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;摄影：&lt;a href=&quot;https://unsplash.com/photos/XR0kq2VDIUo?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Christian Lambert&lt;/a&gt;，来自&lt;a href=&quot;https://unsplash.com/?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Unsplash&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Maven" scheme="https://www.chentyit.com/categories/Maven/"/>
    
    
      <category term="开发问题解决" scheme="https://www.chentyit.com/tags/%E5%BC%80%E5%8F%91%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"/>
    
  </entry>
  
  <entry>
    <title>微信小程序-promise</title>
    <link href="https://www.chentyit.com/2019/12/14/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F-promise/"/>
    <id>https://www.chentyit.com/2019/12/14/微信小程序-promise/</id>
    <published>2019-12-14T06:52:44.000Z</published>
    <updated>2019-12-14T07:25:27.630Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2019/12/14/微信小程序-promise/D:/Blog/myblog/source/_posts/SpringBoot学习笔记（Day03）/max-van-den-oetelaar-1328723-unsplash.jpg" alt="插图"></p><blockquote><p>摄影：<a href="https://unsplash.com/photos/F3rDBnQQbQU?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener">Max van den Oetelaar</a>，来自<a href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener">Unsplash</a></p></blockquote><a id="more"></a><h2 id="ES6-中的约定"><a href="#ES6-中的约定" class="headerlink" title="ES6 中的约定"></a>ES6 中的约定</h2><ul><li>pending：既不成功也不失败</li><li>fulfilled：成功状态</li><li>rejected：失败状态</li></ul><p><strong>代码写法</strong></p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>) =&gt;</span> &#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="number">1</span>)</span><br><span class="line">        resolve()</span><br><span class="line">    &#125;, <span class="number">1000</span>)</span><br><span class="line">&#125;).then(<span class="function">(<span class="params">res</span>) =&gt;</span> &#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="number">2</span>)</span><br><span class="line">    &#125;, <span class="number">2000</span>)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>或</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> p1 = <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>) =&gt;</span> &#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">'p1'</span>)</span><br><span class="line">        resolve()</span><br><span class="line">    &#125;, <span class="number">2000</span>)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> p2 = <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>) =&gt;</span> &#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">'p2'</span>)</span><br><span class="line">        resolve()</span><br><span class="line">    &#125;, <span class="number">1000</span>)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> p3 = <span class="keyword">new</span> <span class="built_in">Promise</span>(<span class="function">(<span class="params">resolve, reject</span>) =&gt;</span> &#123;</span><br><span class="line">    setTimeout(<span class="function"><span class="params">()</span> =&gt;</span> &#123;</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">'p3'</span>)</span><br><span class="line">        resolve()</span><br><span class="line">    &#125;, <span class="number">2000</span>)</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="built_in">Promise</span>.all([p1, p2, p3]).then(<span class="function">(<span class="params">res</span>) =&gt;</span> &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'全部完成'</span>)</span><br><span class="line">    <span class="built_in">console</span>.log(res)</span><br><span class="line">&#125;).catch(<span class="function">(<span class="params">err</span>) =&gt;</span> &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'失败'</span>)</span><br><span class="line">    <span class="built_in">console</span>.log(err)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><ul><li><strong>Promise.race：</strong>是要有一个任务完成就算是完成了</li><li><strong>Promise.race：</strong>所有任务开启运行</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2019/12/14/微信小程序-promise/D:/Blog/myblog/source/_posts/SpringBoot学习笔记（Day03）/max-van-den-oetelaar-1328723-unsplash.jpg&quot; alt=&quot;插图&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;摄影：&lt;a href=&quot;https://unsplash.com/photos/F3rDBnQQbQU?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Max van den Oetelaar&lt;/a&gt;，来自&lt;a href=&quot;https://unsplash.com/?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Unsplash&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="微信小程序" scheme="https://www.chentyit.com/categories/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"/>
    
    
      <category term="开发" scheme="https://www.chentyit.com/tags/%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>微信小程序-wx</title>
    <link href="https://www.chentyit.com/2019/12/14/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F-wx/"/>
    <id>https://www.chentyit.com/2019/12/14/微信小程序-wx/</id>
    <published>2019-12-14T05:25:06.000Z</published>
    <updated>2019-12-14T06:48:29.034Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2019/12/14/微信小程序-wx/D:/Blog/myblog/source/_posts/SpringBoot学习笔记（Day03）/max-van-den-oetelaar-1328723-unsplash.jpg" alt="插图"></p><blockquote><p>摄影：<a href="https://unsplash.com/photos/F3rDBnQQbQU?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener">Max van den Oetelaar</a>，来自<a href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener">Unsplash</a></p></blockquote><a id="more"></a><h2 id="wx"><a href="#wx" class="headerlink" title="wx"></a>wx</h2><h3 id="wx-for"><a href="#wx-for" class="headerlink" title="wx:for"></a>wx:for</h3><p><strong>作用：</strong>用于遍历 data 里面的数据</p><h3 id="wx-for-item"><a href="#wx-for-item" class="headerlink" title="wx:for-item"></a>wx:for-item</h3><p><strong>作用：</strong>用于指定遍历的元素名称（默认 item）</p><h3 id="wx-for-index"><a href="#wx-for-index" class="headerlink" title="wx:for-index"></a>wx:for-index</h3><p><strong>作用：</strong>用于指定遍历的索引名称（默认 index）</p><h3 id="wx-key"><a href="#wx-key" class="headerlink" title="wx:key"></a>wx:key</h3><p><strong>官方解释：</strong>当数据改变触发渲染层重新渲染的时候，会校正带有 key 的组件，框架会确保他们被重新排序，而不是重新创建，以确保使组件保持自身的状态，并且提高列表渲染时的效率。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">block</span> <span class="attr">wx:for</span>=<span class="string">"&#123;&#123;arr&#125;&#125;"</span> <span class="attr">wx:key</span>=<span class="string">"*this"</span> <span class="attr">wx:for-item</span>=<span class="string">"data"</span> <span class="attr">wx:for-index</span>=<span class="string">"idx"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">view</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">checkbox</span>/&gt;</span>&#123;&#123;data&#125;&#125;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">view</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">button</span> <span class="attr">bind:tap</span>=<span class="string">"sort"</span>&gt;</span>随机排序<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">block</span> <span class="attr">wx:for</span>=<span class="string">"&#123;&#123;arrObj&#125;&#125;"</span> <span class="attr">wx:key</span>=<span class="string">"id"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">view</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">checkbox</span>/&gt;</span>&#123;&#123;item.name&#125;&#125;</span><br><span class="line">  <span class="tag">&lt;/<span class="name">view</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">button</span> <span class="attr">bind:tap</span>=<span class="string">"sortObj"</span>&gt;</span>随机排序<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="bind"><a href="#bind" class="headerlink" title="bind"></a>bind</h2><h3 id="bind-tap"><a href="#bind-tap" class="headerlink" title="bind:tap"></a>bind:tap</h3><p><strong>作用：</strong>绑定 js 函数</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2019/12/14/微信小程序-wx/D:/Blog/myblog/source/_posts/SpringBoot学习笔记（Day03）/max-van-den-oetelaar-1328723-unsplash.jpg&quot; alt=&quot;插图&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;摄影：&lt;a href=&quot;https://unsplash.com/photos/F3rDBnQQbQU?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Max van den Oetelaar&lt;/a&gt;，来自&lt;a href=&quot;https://unsplash.com/?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Unsplash&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="微信小程序" scheme="https://www.chentyit.com/categories/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"/>
    
    
      <category term="开发" scheme="https://www.chentyit.com/tags/%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>微信小程序-轮播图</title>
    <link href="https://www.chentyit.com/2019/12/09/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F-%E8%BD%AE%E6%92%AD%E5%9B%BE/"/>
    <id>https://www.chentyit.com/2019/12/09/微信小程序-轮播图/</id>
    <published>2019-12-09T13:24:53.000Z</published>
    <updated>2019-12-09T14:03:40.349Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2019/12/09/微信小程序-轮播图/D:/Blog/myblog/source/_posts/SpringBoot学习笔记（Day03）/max-van-den-oetelaar-1328723-unsplash.jpg" alt="插图"></p><blockquote><p>摄影：<a href="https://unsplash.com/photos/F3rDBnQQbQU?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener">Max van den Oetelaar</a>，来自<a href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank" rel="noopener">Unsplash</a></p></blockquote><a id="more"></a><h2 id="轮播图组件"><a href="#轮播图组件" class="headerlink" title="轮播图组件"></a>轮播图组件</h2><p><strong>swiper：</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">swiper</span> <span class="attr">indicator-dots</span>=<span class="string">"true"</span> <span class="attr">autoplay</span>=<span class="string">"true"</span> <span class="attr">interval</span>=<span class="string">"2000"</span> <span class="attr">duration</span>=<span class="string">"1000"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">block</span> <span class="attr">wx:for</span>=<span class="string">"&#123;&#123;swiperImgUrls&#125;&#125;"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">swiper-item</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">image</span> <span class="attr">src</span>=<span class="string">"&#123;&#123;item.url&#125;&#125;"</span> <span class="attr">mode</span>=<span class="string">"widthFix"</span> <span class="attr">class</span>=<span class="string">"img"</span>&gt;</span><span class="tag">&lt;/<span class="name">image</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">swiper-item</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">block</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">swiper</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.img</span> &#123;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">height</span>: <span class="number">100%</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>indicator-dots=”true”：</strong>轮播图的导航器</li><li><strong>autoplay=”true”：</strong>是否自动播放</li><li><strong>interval=”2000”：</strong>设置滚动时间</li><li><strong>duration=”1000”：</strong>滑动时长</li><li><strong>mode：</strong><ul><li>scaleToFill：不保证横纵比例缩放图片，但能让图片覆盖当前容器</li><li>aspectFit：显示图片完整的宽度，但不一定能填满容器高度</li><li>widthFix：保证宽度不变，高度自动变化，保证图片宽高比</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2019/12/09/微信小程序-轮播图/D:/Blog/myblog/source/_posts/SpringBoot学习笔记（Day03）/max-van-den-oetelaar-1328723-unsplash.jpg&quot; alt=&quot;插图&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;摄影：&lt;a href=&quot;https://unsplash.com/photos/F3rDBnQQbQU?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Max van den Oetelaar&lt;/a&gt;，来自&lt;a href=&quot;https://unsplash.com/?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Unsplash&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="微信小程序" scheme="https://www.chentyit.com/categories/%E5%BE%AE%E4%BF%A1%E5%B0%8F%E7%A8%8B%E5%BA%8F/"/>
    
    
      <category term="开发" scheme="https://www.chentyit.com/tags/%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>6种排序</title>
    <link href="https://www.chentyit.com/2019/10/20/6%E7%A7%8D%E6%8E%92%E5%BA%8F/"/>
    <id>https://www.chentyit.com/2019/10/20/6种排序/</id>
    <published>2019-10-20T06:42:01.000Z</published>
    <updated>2019-10-20T07:39:14.564Z</updated>
    
    <content type="html"><![CDATA[<ul><li>各种排序的简介</li><li>冒泡排序</li><li>选择排序</li><li>直接插入排序</li><li>希尔排序</li><li>归并排序</li><li>快速排序</li></ul><a id="more"></a><h2 id="各种排序的简介"><a href="#各种排序的简介" class="headerlink" title="各种排序的简介"></a>各种排序的简介</h2><ul><li><a href="https://baike.baidu.com/item/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/4602306?fr=aladdin" target="_blank" rel="noopener">冒泡排序</a></li><li><a href="https://baike.baidu.com/item/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/9762418?fr=aladdin" target="_blank" rel="noopener">选择排序</a></li><li><a href="https://baike.baidu.com/item/%E7%9B%B4%E6%8E%A5%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F" target="_blank" rel="noopener">直接插入排序</a></li><li><a href="https://baike.baidu.com/item/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F" target="_blank" rel="noopener">希尔排序</a></li><li><a href="https://baike.baidu.com/item/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F" target="_blank" rel="noopener">归并排序</a></li><li><a href="https://baike.baidu.com/item/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95?fromtitle=%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F&amp;fromid=2084344" target="_blank" rel="noopener">快速排序</a></li></ul><p>（声明：最基础的排序公认有 8 种，我目前了解并学习的只有当前的 6 种，剩下的还有<strong>堆排序</strong>， <strong>桶排序</strong> ， <strong>基数排序</strong>等等，算法的范围很大，需要慢慢学习）</p><h2 id="冒泡排序实现代码"><a href="#冒泡排序实现代码" class="headerlink" title="冒泡排序实现代码"></a>冒泡排序实现代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 类冒泡排序，并非正统的冒泡排序，是效率最低的排序方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BobbleSort0</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr) - <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, len(arr)):</span><br><span class="line">            <span class="keyword">if</span> arr[i] &gt; arr[j]:</span><br><span class="line">                arr[i], arr[j] = arr[j], arr[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 冒泡排序方法 1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BobbleSort1</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 指定冒泡的顶点位置</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        <span class="comment"># 开始冒泡</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(arr) - <span class="number">1</span>, i, <span class="number">-1</span>):</span><br><span class="line">            <span class="comment"># 满足条件就开始交换</span></span><br><span class="line">            <span class="keyword">if</span> arr[j] &lt; arr[j - <span class="number">1</span>]:</span><br><span class="line">                arr[j], arr[j - <span class="number">1</span>] = arr[j - <span class="number">1</span>], arr[j]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化的冒泡排序</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BobbleSort2</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 指定一个标志位，表示是否有交换操作</span></span><br><span class="line">    flag = <span class="keyword">True</span></span><br><span class="line">    <span class="comment"># 指定冒泡的顶点位置</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            flag = <span class="keyword">False</span></span><br><span class="line">            <span class="comment"># 开始冒泡</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(arr) - <span class="number">1</span>, i, <span class="number">-1</span>):</span><br><span class="line">                <span class="keyword">if</span> arr[j] &lt; arr[j - <span class="number">1</span>]:</span><br><span class="line">                    arr[j], arr[j - <span class="number">1</span>] = arr[j - <span class="number">1</span>], arr[j]</span><br><span class="line">                    <span class="comment"># 发生交换，修改标志位</span></span><br><span class="line">                    flag = <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    arr = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>]</span><br><span class="line">    BobbleSort2(arr)</span><br><span class="line">    print(arr)</span><br></pre></td></tr></table></figure><h2 id="选择排序实现代码"><a href="#选择排序实现代码" class="headerlink" title="选择排序实现代码"></a>选择排序实现代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SelectSort0</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 第一重循环选择位置</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr) - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 选择当前位置</span></span><br><span class="line">        min = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + <span class="number">1</span>, len(arr)):</span><br><span class="line">            <span class="comment"># 满足条件就重设当前位置</span></span><br><span class="line">            <span class="keyword">if</span> arr[min] &gt; arr[j]:</span><br><span class="line">                min = j</span><br><span class="line">        <span class="comment"># 如果选择值发生了改变说明需要交换位置了</span></span><br><span class="line">        <span class="keyword">if</span> i != min:</span><br><span class="line">            arr[min], arr[i] = arr[i], arr[min]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    arr = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>]</span><br><span class="line">    SelectSort0(arr)</span><br><span class="line">    print(arr)</span><br></pre></td></tr></table></figure><h2 id="直接插入排序实现代码"><a href="#直接插入排序实现代码" class="headerlink" title="直接插入排序实现代码"></a>直接插入排序实现代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">InsertSort0</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 从第一位选择一个值向前插入</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(arr)):</span><br><span class="line">        <span class="comment"># 向前插入该值知道满足规律</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="comment"># 找到合适的位置插入</span></span><br><span class="line">            <span class="keyword">if</span> arr[j] &lt; arr[j - <span class="number">1</span>]:</span><br><span class="line">                arr[j], arr[j - <span class="number">1</span>] = arr[j - <span class="number">1</span>], arr[j]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    arr = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>]</span><br><span class="line">    InsertSort0(arr)</span><br><span class="line">    print(arr)</span><br></pre></td></tr></table></figure><h2 id="希尔排序实现代码"><a href="#希尔排序实现代码" class="headerlink" title="希尔排序实现代码"></a>希尔排序实现代码</h2><p>简要说明：希尔排序就是插入排序的改进版本，因为直接插入排序面向的本来就是基本有序全局无序的序列，如果全局无序，就会造成效率变低，所以就引入了希尔排序，先按照一定的步长排序，然后逐渐减少步长直至步长为 1，提高了一定的效率，但是时间复杂度和直接插入排序相同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ShellSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 设置第一次的步长为排序序列的长度的一半</span></span><br><span class="line">    dk = int(len(arr) / <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 保证最后的步长一定为 1</span></span><br><span class="line">    <span class="keyword">while</span> dk &gt;= <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 这里就是直接插入排序</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(dk, len(arr), dk):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i, <span class="number">0</span>, -dk):</span><br><span class="line">                <span class="keyword">if</span> arr[j] &lt; arr[j - dk]:</span><br><span class="line">                    arr[j], arr[j - <span class="number">1</span>] = arr[j - <span class="number">1</span>], arr[j]</span><br><span class="line">        <span class="comment"># 重设步长为当前步长的一半</span></span><br><span class="line">        dk = int(dk / <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    arr = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>]</span><br><span class="line">    ShellSort(arr)</span><br><span class="line">    print(arr)</span><br></pre></td></tr></table></figure><h2 id="归并排序实现代码"><a href="#归并排序实现代码" class="headerlink" title="归并排序实现代码"></a>归并排序实现代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MergeSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 保证最后切割出来的值只有一个</span></span><br><span class="line">    <span class="keyword">if</span> len(arr) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得中间位置</span></span><br><span class="line">    mid = len(arr) // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得左边的序列</span></span><br><span class="line">    left = arr[:mid]</span><br><span class="line">    <span class="comment"># 获得右边的序列</span></span><br><span class="line">    right = arr[mid:]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 递归切割左边序列</span></span><br><span class="line">    ll = MergeSort(left)</span><br><span class="line">    <span class="comment"># 递归切割右边序列</span></span><br><span class="line">    rl = MergeSort(right)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 合并左右序列</span></span><br><span class="line">    <span class="keyword">return</span> Merge(ll, rl)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并（归并两个序列）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Merge</span><span class="params">(ll, rl)</span>:</span></span><br><span class="line">    <span class="comment"># 用于存储结果的容器</span></span><br><span class="line">    result = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 归并过程</span></span><br><span class="line">    <span class="keyword">while</span> len(ll) &gt; <span class="number">0</span> <span class="keyword">and</span> len(rl) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> ll[<span class="number">0</span>] &lt; rl[<span class="number">0</span>]:</span><br><span class="line">            result.append(ll.pop(<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result.append(rl.pop(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 合并剩下的左序列</span></span><br><span class="line">    result += ll</span><br><span class="line">    <span class="comment"># 合并剩下的右序列</span></span><br><span class="line">    result += rl</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回合并的序列</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    arr = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>]</span><br><span class="line">    arr = MergeSort(arr)</span><br><span class="line">    print(arr)</span><br></pre></td></tr></table></figure><h2 id="快速排序实现代码"><a href="#快速排序实现代码" class="headerlink" title="快速排序实现代码"></a>快速排序实现代码</h2><p>简要说明：归并排序和快速排序使用的都是分治策略，从局部有序到全局有序，但是实现的方法不同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">QuickSort</span><span class="params">(arr, left, right)</span>:</span></span><br><span class="line">    <span class="comment"># 如果左指针等于右指针说明无法继续分区</span></span><br><span class="line">    <span class="keyword">if</span> left &gt;= right:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得一个分区节点</span></span><br><span class="line">    index = left</span><br><span class="line">    <span class="comment"># 存储分区节点的值</span></span><br><span class="line">    value = arr[left]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 快速排序核心</span></span><br><span class="line">    <span class="comment"># 将分区内的序列按照小的放到节点左边，大的放到节点右边</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(left, right + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> value &gt; arr[i]:</span><br><span class="line">            arr[index] = arr[i]</span><br><span class="line">            arr[i] = arr[index + <span class="number">1</span>]</span><br><span class="line">            index = index + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重新放置分区节点的值</span></span><br><span class="line">    arr[index] = value</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 递归调用快排函数</span></span><br><span class="line">    QuickSort(arr, left, index - <span class="number">1</span>)</span><br><span class="line">    QuickSort(arr, index + <span class="number">1</span>, right)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    arr = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>]</span><br><span class="line">    QuickSort(arr, <span class="number">0</span>, len(arr) - <span class="number">1</span>)</span><br><span class="line">    print(arr)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;各种排序的简介&lt;/li&gt;
&lt;li&gt;冒泡排序&lt;/li&gt;
&lt;li&gt;选择排序&lt;/li&gt;
&lt;li&gt;直接插入排序&lt;/li&gt;
&lt;li&gt;希尔排序&lt;/li&gt;
&lt;li&gt;归并排序&lt;/li&gt;
&lt;li&gt;快速排序&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="算法" scheme="https://www.chentyit.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://www.chentyit.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>八皇后问题</title>
    <link href="https://www.chentyit.com/2019/10/20/%E5%85%AB%E7%9A%87%E5%90%8E%E9%97%AE%E9%A2%98/"/>
    <id>https://www.chentyit.com/2019/10/20/八皇后问题/</id>
    <published>2019-10-20T06:28:18.000Z</published>
    <updated>2019-10-20T06:42:52.527Z</updated>
    
    <content type="html"><![CDATA[<ul><li>问题介绍</li><li>解题思路</li><li>实例代码</li></ul><a id="more"></a><h2 id="问题介绍（来源百度百科）"><a href="#问题介绍（来源百度百科）" class="headerlink" title="问题介绍（来源百度百科）"></a>问题介绍（来源百度百科）</h2><p> 八皇后问题，是一个古老而著名的问题，是回溯算法的典型案例。该问题是国际西洋棋棋手<strong>马克斯·贝瑟尔</strong>于 1848 年提出：在 <strong>8×8</strong> 格的国际象棋上摆放八个皇后，使其不能互相攻击，即任意两个皇后都<strong>不能处于同一行、同一列或同一斜线</strong>上，问有多少种摆法。 <a href="https://baike.baidu.com/item/高斯/24098" target="_blank" rel="noopener">高斯</a>认为有 76 种方案。1854年在柏林的象棋杂志上不同的作者发表了40种不同的解，后来有人用<strong>图论</strong>的方法解出 <strong>92</strong> 种结果。计算机发明后，有多种计算机语言可以解决此问题。 </p><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><h3 id="回溯算法"><a href="#回溯算法" class="headerlink" title="回溯算法"></a>回溯算法</h3><p>是一种选优搜索法，又称为试探法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”</p><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ol><li>先选择棋盘中的一个点</li><li>判断该点是否满足放置要求</li><li>满足则将该位置设置标志位，不满足则不放置</li><li>寻找下一个点</li><li>循环 2，3，4 步骤直至遍历整个棋盘</li></ol><h2 id="实例代码（Python）"><a href="#实例代码（Python）" class="headerlink" title="实例代码（Python）"></a>实例代码（Python）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于记录有多少种解法</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="comment"># 生成棋盘</span></span><br><span class="line">arr = [[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">8</span>)] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">8</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查这个坐标是否可以放棋子</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">(row, col)</span>:</span></span><br><span class="line">    <span class="comment"># 设置全局可修改 arr</span></span><br><span class="line">    <span class="keyword">global</span> arr</span><br><span class="line">    <span class="comment"># 检查行列</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(row):</span><br><span class="line">        <span class="keyword">if</span> arr[i][col] == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查左对角线</span></span><br><span class="line">    <span class="keyword">for</span> (i, m) <span class="keyword">in</span> zip(range(row - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>), range(col - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>)):</span><br><span class="line">        <span class="keyword">if</span> arr[i][m] == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 检查右对角线</span></span><br><span class="line">    <span class="keyword">for</span> (i, m) <span class="keyword">in</span> zip(range(row - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>), range(col + <span class="number">1</span>, <span class="number">8</span>)):</span><br><span class="line">        <span class="keyword">if</span> arr[i][m] == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始布局</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findQueen</span><span class="params">(row)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> count</span><br><span class="line">    <span class="keyword">global</span> arr</span><br><span class="line">    <span class="comment"># 如果存在某一种情况则加一</span></span><br><span class="line">    <span class="keyword">if</span> row &gt; <span class="number">7</span>:</span><br><span class="line">        count = count + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">        <span class="keyword">if</span> check(row, col):</span><br><span class="line">            <span class="comment"># 棋子放置的位置设置为 1</span></span><br><span class="line">            arr[row][col] = <span class="number">1</span></span><br><span class="line">            findQueen(row + <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将数据归 0 防止出现脏数据</span></span><br><span class="line">            arr[row][col] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">"八皇后问题"</span>)</span><br><span class="line">    findQueen(<span class="number">0</span>)</span><br><span class="line">    print(count)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;问题介绍&lt;/li&gt;
&lt;li&gt;解题思路&lt;/li&gt;
&lt;li&gt;实例代码&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="算法" scheme="https://www.chentyit.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="回溯算法" scheme="https://www.chentyit.com/tags/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Flink on Yarn 集群 HA 安装及配置</title>
    <link href="https://www.chentyit.com/2019/10/19/Flink-on-Yarn-%E9%9B%86%E7%BE%A4-HA-%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE/"/>
    <id>https://www.chentyit.com/2019/10/19/Flink-on-Yarn-集群-HA-安装及配置/</id>
    <published>2019-10-19T03:46:13.000Z</published>
    <updated>2019-10-19T03:49:31.139Z</updated>
    
    <content type="html"><![CDATA[<ul><li>搭建 Hadoop HA 集群及 ZK 集群</li><li>下载 Flink</li><li>解压安装包</li><li>修改 /etc/profile 添加环境变量</li><li>修改配置文件</li><li>启动 Flink 集群</li><li>HA 容错机制</li><li>On Yarn</li></ul><a id="more"></a><h2 id="搭建-Hadoop-HA-集群及-ZK-集群"><a href="#搭建-Hadoop-HA-集群及-ZK-集群" class="headerlink" title="搭建 Hadoop HA 集群及 ZK 集群"></a>搭建 Hadoop HA 集群及 ZK 集群</h2><p><a href="http://chentyit.com/2019/09/15/Zookeeper%E5%AE%89%E8%A3%85%E5%92%8CHadoop%E5%AE%89%E8%A3%85/" target="_blank" rel="noopener">Zookeeper 安装和 Hadoop 安装</a></p><h2 id="下载-Flink"><a href="#下载-Flink" class="headerlink" title="下载 Flink"></a>下载 Flink</h2><p>我使用的是 <a href="https://www.apache.org/dyn/closer.lua/flink/flink-1.7.2/flink-1.7.2-bin-hadoop28-scala_2.11.tgz" target="_blank" rel="noopener">Flink 1.7.2</a></p><p>这里要注意，如果要搭配 Hadoop 使用，一定要下载带有 <strong>with Hadoop</strong> 或者 <strong>Pre-bundled Hadoop</strong> 的安装包，只有 <code>for Scala</code> 的没法连接到 HDFS（亲测踩坑），应该是少了一些包，因为我测试的时候，两种安装包的大小都相差了 30Mb 左右</p><h2 id="解压安装包"><a href="#解压安装包" class="headerlink" title="解压安装包"></a>解压安装包</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 这里注意，安装包是带有 hadoop 的，没有的话重新去下载</span><br><span class="line">tar -xzvf flink-1.7.2-bin-hadoop28-scala_2.11.tgz</span><br></pre></td></tr></table></figure><h2 id="修改-etc-profile-添加环境变量"><a href="#修改-etc-profile-添加环境变量" class="headerlink" title="修改 /etc/profile 添加环境变量"></a>修改 /etc/profile 添加环境变量</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">export FLINK_HOME="/usr/local/flink-1.7.2"</span><br><span class="line">export PATH=$&#123;PATH&#125;:$&#123;FLINK_HOME&#125;/bin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 更新一下</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 进入解压路径</span><br><span class="line">cd **/flink-1.7.2</span><br></pre></td></tr></table></figure><h3 id="flink-conf-yaml-修改配置项"><a href="#flink-conf-yaml-修改配置项" class="headerlink" title="flink-conf.yaml 修改配置项"></a>flink-conf.yaml 修改配置项</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">vim conf/flink-conf.yaml</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 改成各自节点的名称或者 IP 地址</span><br><span class="line"><span class="meta">#</span> 这个配置项有坑（更坑的是官网还没说），可以借鉴这篇博客：https://www.jianshu.com/p/e48b73221c67</span><br><span class="line">jobmanager.rpc.address: bigdata1</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 开启高可用模式</span><br><span class="line">high-availability: zookeeper</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 指定 HDFS 的路径，用于存储 JobManager 的元数据</span><br><span class="line">high-availability.storageDir: hdfs://ns/flink/ha/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 配置 zk 各个节点的端口</span><br><span class="line">high-availability.zookeeper.quorum: bigdata1:2181,bigdata2:2181,bigdata3:2181</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> zk 节点根目录，放置所有 flink 集群节点的 namespace</span><br><span class="line">high-availability.zookeeper.path.root: /flink</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> zk 节点集群 id，放置了 flink 集群所需要的所有协调数据</span><br><span class="line">high-availability.cluster-id: /cluster_one</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 配置Yarn重试次数</span><br><span class="line">yarn.application-attempts: 10</span><br></pre></td></tr></table></figure><h3 id="slaves-添加-TaskManagerRunner"><a href="#slaves-添加-TaskManagerRunner" class="headerlink" title="slaves 添加 TaskManagerRunner"></a>slaves 添加 TaskManagerRunner</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim conf/slaves</span><br><span class="line"></span><br><span class="line">bigdata1</span><br><span class="line">bigdata2</span><br><span class="line">bigdata3</span><br></pre></td></tr></table></figure><h3 id="masters-添加-StandaloneSessionClusterEntrypoint"><a href="#masters-添加-StandaloneSessionClusterEntrypoint" class="headerlink" title="masters 添加 StandaloneSessionClusterEntrypoint"></a>masters 添加 StandaloneSessionClusterEntrypoint</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim conf/masters</span><br><span class="line"></span><br><span class="line">bigdata5:8081</span><br><span class="line">bigdata6:8081</span><br></pre></td></tr></table></figure><h3 id="修改-yarn-site-xml"><a href="#修改-yarn-site-xml" class="headerlink" title="修改 yarn-site.xml"></a>修改 yarn-site.xml</h3><p><strong>添加以下信息</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 设置提交应用程序的最大尝试次数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.am.max-attempts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">        The maximum number of application master execution attempts.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="启动-Flink-集群"><a href="#启动-Flink-集群" class="headerlink" title="启动 Flink 集群"></a>启动 Flink 集群</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 因为前面已经添加到环境变量了，直接运行命令即可</span><br><span class="line"><span class="meta">#</span> 没有添加就要到 flink 的 bin 目录下运行该命令</span><br><span class="line">start-cluster.sh</span><br></pre></td></tr></table></figure><h3 id="启动结果"><a href="#启动结果" class="headerlink" title="启动结果"></a>启动结果</h3><p><strong>bigdata1</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">14195 TaskManagerRunner</span><br></pre></td></tr></table></figure><p><strong>bigdata2</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">56062 TaskManagerRunner</span><br></pre></td></tr></table></figure><p><strong>bigdata3</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">44918 TaskManagerRunner</span><br></pre></td></tr></table></figure><p><strong>bigdata5</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">44734 StandaloneSessionClusterEntrypoint</span><br></pre></td></tr></table></figure><p><strong>bigdata6</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4177 StandaloneSessionClusterEntrypoint</span><br></pre></td></tr></table></figure><h3 id="WebUI"><a href="#WebUI" class="headerlink" title="WebUI"></a>WebUI</h3><p><img src="/2019/10/19/Flink-on-Yarn-集群-HA-安装及配置/FlinkWebUI.png" alt="FlinkWebUI"></p><h3 id="ZK-节点查看"><a href="#ZK-节点查看" class="headerlink" title="ZK 节点查看"></a>ZK 节点查看</h3><p><img src="/2019/10/19/Flink-on-Yarn-集群-HA-安装及配置/FlinkZK.png" alt="FlinkZK"></p><h3 id="HDFS-查看"><a href="#HDFS-查看" class="headerlink" title="HDFS 查看"></a>HDFS 查看</h3><p><img src="/2019/10/19/Flink-on-Yarn-集群-HA-安装及配置/FlinkHDFS.png" alt="FlinkHDFS"></p><h2 id="HA-容错机制"><a href="#HA-容错机制" class="headerlink" title="HA 容错机制"></a>HA 容错机制</h2><p>直接 kill 一个 Master 进程，另一个 Master 会自动上线（需要点时间，可能有点长，亲测成功）</p><p><strong>重启被 kill 的进程：jobmanager.sh start</strong></p><h2 id="On-Yarn"><a href="#On-Yarn" class="headerlink" title="On Yarn"></a>On Yarn</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn-session.sh -n 2</span><br></pre></td></tr></table></figure><h3 id="WebUI-查看-job"><a href="#WebUI-查看-job" class="headerlink" title="WebUI 查看 job"></a>WebUI 查看 job</h3><p>这里我用 <code>yarn application -kill jobId</code> 关掉了，所以状态是 History（这个命令在我面试的时候被问到过，-list 是列出所有的 job）</p><p><img src="/2019/10/19/Flink-on-Yarn-集群-HA-安装及配置/FlinkOnYarn.png" alt="FlinkOnYarn"></p><h2 id="至此安装完成"><a href="#至此安装完成" class="headerlink" title="至此安装完成"></a>至此安装完成</h2>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;搭建 Hadoop HA 集群及 ZK 集群&lt;/li&gt;
&lt;li&gt;下载 Flink&lt;/li&gt;
&lt;li&gt;解压安装包&lt;/li&gt;
&lt;li&gt;修改 /etc/profile 添加环境变量&lt;/li&gt;
&lt;li&gt;修改配置文件&lt;/li&gt;
&lt;li&gt;启动 Flink 集群&lt;/li&gt;
&lt;li&gt;HA 容错机制&lt;/li&gt;
&lt;li&gt;On Yarn&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flink" scheme="https://www.chentyit.com/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>HBase 概念</title>
    <link href="https://www.chentyit.com/2019/10/12/HBase-%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.chentyit.com/2019/10/12/HBase-概念/</id>
    <published>2019-10-12T06:57:04.000Z</published>
    <updated>2019-10-15T03:37:16.429Z</updated>
    
    <content type="html"><![CDATA[<ul><li>HBase 安装</li><li>简介</li><li>Hbase数据模型</li><li>RowKey 的设计</li><li>列簇设计</li></ul><a id="more"></a><h2 id="HBase-安装"><a href="#HBase-安装" class="headerlink" title="HBase 安装"></a>HBase 安装</h2><p><a href="http://chentyit.com/2019/09/01/HBase-%E5%AE%89%E8%A3%85/" target="_blank" rel="noopener">安装连接</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="什么是-HBase"><a href="#什么是-HBase" class="headerlink" title="什么是 HBase"></a>什么是 HBase</h3><p>Hbase 是一个高可靠性，高性能，面向列，可伸缩的分布式存储系统，目标是存储并处理大型的数据</p><h3 id="与传统数据相比"><a href="#与传统数据相比" class="headerlink" title="与传统数据相比"></a>与传统数据相比</h3><table><thead><tr><th>特点</th><th>MySQL</th><th>HBase</th></tr></thead><tbody><tr><td>数据库的概念</td><td>有数据库 database 的概念和表 table 的概念</td><td>没有数据库的概念，有替代的命名空间 namespace 的概念，有表的概念，所有的表都是在一个 namespace 下的</td></tr><tr><td>主键</td><td>主键唯一决定了一行</td><td>没有主键但是有行建：rowkey</td></tr><tr><td>字段</td><td>表中直接包含字段</td><td>有列簇的概念，每个列簇中才包含了字段，给相同属性的列划分一个组，叫做 column family 列簇，在建表的时候至少要指明一个列簇，可以不给字段</td></tr><tr><td>版本</td><td>行列交叉可到一个唯一的单元格，数据的版本数只有 1</td><td>可以存储多个版本 version（相当于是可以存储多个值），HBase 行列交叉得到一个唯一的单元格组，组中可以有多个单元格，可以设置 HBase 的 version 版本数，是 int 值，当 version 为 1 的时候没就没有单元格组的概念了，就是一个单元格，默认情况下，显示 timestamp 最新的那个单元格的值</td></tr><tr><td>空值</td><td>MySQL 中没有值的话就是 null，是占空间的</td><td>对于 HBase 来说，如果没有这一列的信息，那么就不会存储，不会分配任何空间</td></tr></tbody></table><h3 id="Hbase-架构"><a href="#Hbase-架构" class="headerlink" title="Hbase 架构"></a>Hbase 架构</h3><p><a href="https://blog.csdn.net/lzxlfly/article/details/82229511" target="_blank" rel="noopener"><strong>架构详解</strong></a></p><p><img src="/2019/10/12/HBase-概念/hbase架构.jpg" alt="hbase架构"></p><h3 id="HBase-集群中的角色"><a href="#HBase-集群中的角色" class="headerlink" title="HBase 集群中的角色"></a>HBase 集群中的角色</h3><ul><li>一个或多个主节点 HMaster<ol><li>管理 HRegionServer，通过 ZK 监控其状态，实现负载均衡</li><li>管理和分配 HRegion，比如在 HRegion 进行 spilit 的时候将分配新的 HRegion，当 HRegionServer 退出或者宕机的时候对 HRegionServer 管理的 HRegion 进行迁移</li><li>Admin 职能：创建、修改、删除 Table 的定义，实现 DDL 操作（namespace 和 table 的增删改，列簇的增删改）</li><li>管理 namespace 和表的元数据（存储在 HDFS 上）</li><li>权限控制</li></ol></li><li>多个从节点 RegionServer<ol><li>管理自己负责的 Region 数据的读写</li><li>读写 HDFS，管理 table 中的数据</li><li>Client 直接通过 RegionServer 读写数据（从 ZK 中获取 meta 表的存储位置，从 meta 表中获取 RowKey 所在的 RegionServer 的位置）</li></ol></li><li>Zookeeper<ol><li>存放整个 HBase 集群的元数据</li><li>保证任何时候，集群中只有一个master</li><li>保证 HMaster 的失败恢复</li><li>监控 RegionServer 的运行状态</li><li>存储 HBase 的 schema 和 table 元数据</li></ol></li></ul><h3 id="HBase-中的表的特点"><a href="#HBase-中的表的特点" class="headerlink" title="HBase 中的表的特点"></a>HBase 中的表的特点</h3><ol><li>大：一个表可以有上亿行，上百万列</li><li>面向列：面向列的存储和权限控制，列独立检索</li><li>稀疏：对于为空（null）的列，并不占用存储空间，因此，表可以设计的非常稀疏</li><li>HBase 的存储数据都是二进制的</li></ol><h2 id="Hbase数据模型"><a href="#Hbase数据模型" class="headerlink" title="Hbase数据模型"></a>Hbase数据模型</h2><ul><li><p>单元格（<strong>Cell</strong>）</p><ol><li>cell 由行和列的坐标交叉决定</li><li>单元格是有版本的</li><li>cell 的内容是未解析的字节数组</li><li>cell 由 <strong>{rowkey, column(<code>&lt;column family&gt;</code> + <code>&lt;qualifier&gt;</code>), value}</strong> 来确定唯一的单元</li><li>cell 中的数据是没有类型的，全部是字节码形式存储的</li></ol></li><li><p>RowKey</p><ol><li>决定一行数据，按行检索数据，相当于一级索引</li><li>按照字典顺序排序，说明数据是有序的</li><li>只能存储 64k 的字节数据，RowKey 越短越好</li></ol></li><li><p>列族 / 列簇（Column Family）</p><ol><li>HBase 表中的每个列都归属于某个列簇，列簇必须作为表模式（schema）定义的一部分预先给出，列名以列簇作为前缀，每个列簇都可以有多个列</li><li>权限控制、存储以及调优都是在列簇层面进行的</li><li>HBase 把同一列簇里面的数据存储在同一目录下，由几个文件保存</li></ol></li><li><p>时间戳（Timestamp）</p><ol><li>在 HBase 每个 cell 存储单元对同一份数据可以有多个版本，根据唯一的时间戳来区分每个版本之间的差异，不同本的数据按照时间倒序排序，最新的数据版本排在最前面</li><li>时间戳的类型是 64 位整型，一般由 HBase（在数据写入时自动）赋值，此时时间戳时精确到毫秒的当前系统时间</li></ol><p>时间戳也可以由客户显示赋值，如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳</p></li></ul><h2 id="HBase-读数据流程"><a href="#HBase-读数据流程" class="headerlink" title="HBase 读数据流程"></a>HBase 读数据流程</h2><p><a href="https://www.imooc.com/article/73049" target="_blank" rel="noopener">hbase 读数据</a></p><h2 id="RowKey-的设计"><a href="#RowKey-的设计" class="headerlink" title="RowKey 的设计"></a>RowKey 的设计</h2><p><a href="https://blog.csdn.net/wangshuminjava/article/details/80575864" target="_blank" rel="noopener"><strong>rk设计</strong></a></p><h2 id="列簇设计"><a href="#列簇设计" class="headerlink" title="列簇设计"></a>列簇设计</h2><p><a href="https://www.cnblogs.com/llphhl/p/6609876.html" target="_blank" rel="noopener">列簇设计</a></p><p><strong>一般不建议设计多个列族</strong>，具体原因如下</p><p>假如 HBase 的表设置两个列族，若已一个列族 1000 万行，另一个列族 100 行。当一个要求 region 分裂时候，会导致 100 行的列会同样分布到多个 region 中。这样就出现基数问题，会导致扫描列族A的性能低下。某个列族在 flush 的时候，它邻近的列族也会因关联效应出发 flush，最终导致系统产生更多的I/O。</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;HBase 安装&lt;/li&gt;
&lt;li&gt;简介&lt;/li&gt;
&lt;li&gt;Hbase数据模型&lt;/li&gt;
&lt;li&gt;RowKey 的设计&lt;/li&gt;
&lt;li&gt;列簇设计&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="HBase" scheme="https://www.chentyit.com/tags/HBase/"/>
    
  </entry>
  
  <entry>
    <title>Hive 概念</title>
    <link href="https://www.chentyit.com/2019/10/11/Hive-%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.chentyit.com/2019/10/11/Hive-概念/</id>
    <published>2019-10-11T07:27:51.000Z</published>
    <updated>2019-10-14T06:01:56.499Z</updated>
    
    <content type="html"><![CDATA[<ul><li>简介</li><li>Hive 特点</li><li>Hive 架构</li><li>MetaStore 组件</li><li>Hive 执行流程</li><li>Hive 与传统数据库相比</li><li>创建 Hive 表</li><li>分区表和分桶表</li><li>Hive 优化</li></ul><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="什么是-Hive"><a href="#什么是-Hive" class="headerlink" title="什么是 Hive"></a>什么是 Hive</h3><p>Hive 是基于 Hadoop 的一个<strong>数据仓库</strong>工具，可以将结构化的数据文件映射为一张数据库表，并提供类似于 SQL 的查询功能功能</p><h3 id="为什么使用-Hive"><a href="#为什么使用-Hive" class="headerlink" title="为什么使用 Hive"></a>为什么使用 Hive</h3><p><strong>直接使用 hadoop 所面临的问题：</strong></p><ul><li>人员学习成本太高</li><li>项目周期要求太短</li><li>MapReduce 实现复杂查询逻辑开发难度太大</li></ul><p><strong>为什么要使用 Hive：</strong></p><ul><li>操作接口采用类 SQL 语法，提供快速开发的能力</li><li>避免了去写 MapReduce，减少开发人员的学习成本</li><li>扩展功能很方便</li></ul><h2 id="Hive-特点"><a href="#Hive-特点" class="headerlink" title="Hive 特点"></a>Hive 特点</h2><ul><li><p>可扩展</p><p>Hive 可以自由的扩展集群的规模，一般情况下不需要重启服务</p></li><li><p>延展性</p><p>Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</p></li><li><p>容错性</p><p>良好的容错性，节点出问题，SQL 仍可完成执行</p></li></ul><h2 id="Hive-架构"><a href="#Hive-架构" class="headerlink" title="Hive 架构"></a>Hive 架构</h2><p><img src="/2019/10/11/Hive-概念/hive架构图.png" alt="hive 架构"></p><h3 id="服务端主件"><a href="#服务端主件" class="headerlink" title="服务端主件"></a>服务端主件</h3><ol><li><p>用户接口：Client</p><p>CLI（command-line interface）、JDBC/ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）</p></li><li><p>元数据：Metastore</p><ul><li><p>single user mode：</p><p>默认安装在 hive 的机器上，hive 是使用 derby 内数据库保存 hive 的元数据，这样是不可以并发调用 hive 的</p></li><li><p>multiuser node ：</p><p>通过网络连接到一个数据库中，是最经常使用到的模式，假设使用本机 mysql 服务器存储元数据</p></li><li><p>remote server mode：</p><p>用于非 java 客户端访问元数据库，在服务器端启动一个 MetaStoreServer，客户端利用 Thrift 协议通过 MetaStoreServer 访问元数据库</p></li></ul></li><li><p>存储介质：Hadoop</p><p>Hive 使用 HDFS 进行存储,使用 MapReduce 进行计算</p></li><li><p>驱动器：Driver</p><p>该组件包括Complier、Optimizer 和 Executor，它的作用是将我们写的 HiveQL（类SQL）语句进行解析、编译优化，生成执行计划，然后调用底层的 mapreduce 计算框架。</p></li></ol><h3 id="客户端组件"><a href="#客户端组件" class="headerlink" title="客户端组件"></a>客户端组件</h3><ol><li><p>CLI：</p><p>command line interface，命令行接口</p></li><li><p>Thrift 客户端：</p><p>hive 架构的许多客户端接口是建立在 thrift 客户端之上，包括 JDBC 和 ODBC 接口</p></li><li><p>WebUI：</p><p>hive 客户端提供了一种通过网页的方式访问 hive 所提供的服务，这个接口对应 hive 的 hwi（hive-web-interface），使用前需要启动 hwi 服务</p></li></ol><h2 id="MetaStore-组件"><a href="#MetaStore-组件" class="headerlink" title="MetaStore 组件"></a>MetaStore 组件</h2><p>Hive 的 MetaStore 组件是 hive 元数据集中存放地。MetaStore 组件包括两个部分：</p><ol><li>MetaStore 服务</li><li>后台数据的存储</li></ol><p>后台数据存储的介质就是关系数据库，hive 默认的嵌入式磁盘数据库 derby，还有 mysql 数据库。MetaStore 服务是建立在后台数据存储介质之上，并且可以和 hive 服务是安装在一起的，运行在同一个 JVM 进程当中。</p><p>但也可以将 MetaStore 服务从 Hive 中剥离出来，单独放在一个集群中，为这个集群安装防火墙，提供安全服务，保证元数据安全，客户端只需要通过防火墙认证连接到元数据即可，保证了 hive 的稳定性，也提高了效率</p><h2 id="Hive-执行流程"><a href="#Hive-执行流程" class="headerlink" title="Hive 执行流程"></a>Hive 执行流程</h2><p><img src="/2019/10/11/Hive-概念/hive执行流程.png" alt="Hive 执行流程"></p><h2 id="Hive-与传统数据库相比"><a href="#Hive-与传统数据库相比" class="headerlink" title="Hive 与传统数据库相比"></a>Hive 与传统数据库相比</h2><table><thead><tr><th></th><th>Hive</th><th>RDBMS</th></tr></thead><tbody><tr><td>查询语言</td><td>HQL</td><td>SQL</td></tr><tr><td>数据存储</td><td>HDFS</td><td>Raw Device or Local FS</td></tr><tr><td>执行</td><td>MapReduce</td><td>Excutor</td></tr><tr><td>执行延迟</td><td>高</td><td>低</td></tr><tr><td>处理数据规模</td><td>大</td><td>小</td></tr><tr><td>索引</td><td>0.8 版本后加入位图索引，紧凑索引</td><td>有复杂的索引</td></tr></tbody></table><p><strong>总结：</strong>hive 具有 sql 数据库的外表，但应用场景完全不同，hive 只适合用来做批量数据统计分析</p><h3 id="其他比较"><a href="#其他比较" class="headerlink" title="其他比较"></a>其他比较</h3><ul><li><p>读时模式 VS 写时模式</p><p>在传统数据里，插入数据时，表的模式是强制确定了的，如果不符合，就不允许插入，在写入时检查数据被称为<strong>“写时模式”</strong></p><p>hive 对数据的验证不在加载数据时进行，而在查询时进行，称为<strong>“读时模式”</strong></p><p>写时模式有利于提升查询性能，数据库可以对列进行索引，并对数据库进行压缩，但是加载数据库就会消耗更多的时间</p></li><li><p>更新、事务</p><p>hive 的表更新是通过把数据变换后放入新表实现的</p><p>HDFS 不提供就地文件更新，所以，插入、更新和删除操作因引起的变化都被保存在一个较小的增量文件中，由 metastore 在后台运行的 MapReduce 作业定期将增量的文件合并到 <strong>“基表”</strong> 中。这些功能只能在事务的背景环境下才能使用（hive 0.13.0 版本引入事务)</p></li><li><p>索引</p><ul><li><strong>紧凑索引：</strong>存储每个值的 HDFS 的块号，不是存储文件内的偏移量，不会占用过多的磁盘空间</li><li><strong>位图索引：</strong>使用压缩的<strong>位集合</strong>来存储具有某个特殊值的行，通常用于<strong>具有少取值可能的列</strong></li></ul></li></ul><h2 id="创建-Hive-表"><a href="#创建-Hive-表" class="headerlink" title="创建 Hive 表"></a>创建 Hive 表</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 表中的字段和数据类型</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line">    <span class="keyword">name</span> <span class="keyword">string</span>,</span><br><span class="line">    age <span class="built_in">int</span>,</span><br><span class="line">    fav <span class="built_in">array</span>&lt;<span class="keyword">string</span>&gt;,</span><br><span class="line">    addr <span class="keyword">map</span>&lt;<span class="keyword">string</span>, <span class="keyword">string</span>&gt;</span><br><span class="line">)</span><br><span class="line">// 表的简介</span><br><span class="line"><span class="keyword">comment</span> <span class="string">'This is the person table'</span></span><br><span class="line">// 定义这张表使用的数据文件格式，这里指定为 txt 类型</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line">collection items <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'-'</span></span><br><span class="line"><span class="keyword">map</span> <span class="keyword">keys</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">':'</span></span><br><span class="line"><span class="keyword">stored</span> <span class="keyword">as</span> textfile;</span><br></pre></td></tr></table></figure><ul><li><p><strong>row format delimited fields terminated by ‘\t’</strong></p><p>定义每一个字段的分隔符，这里 ‘\t’ 表示以 Tab 作为分隔符分隔每行字段</p></li><li><p><strong>collection items terminated by ‘-‘</strong></p><p>定义集合中每个对象的分隔符，fav 字段是 String 类型的数组，以 ‘-‘ 为分隔符，分隔每个字符串</p></li><li><p><strong>map keys terminated by ‘:’</strong></p><p>定义 Map 类型键值对的分隔符，这里指定为 txt 类型</p></li></ul><h2 id="分区表和分桶表"><a href="#分区表和分桶表" class="headerlink" title="分区表和分桶表"></a>分区表和分桶表</h2><p><a href="https://blog.csdn.net/shudaqi2010/article/details/90288901" target="_blank" rel="noopener">Hive的分区表和分桶表的区别</a></p><h2 id="Hive-优化"><a href="#Hive-优化" class="headerlink" title="Hive 优化"></a>Hive 优化</h2><p><a href="https://www.cnblogs.com/smartloli/p/4356660.html" target="_blank" rel="noopener">hive优化</a></p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;简介&lt;/li&gt;
&lt;li&gt;Hive 特点&lt;/li&gt;
&lt;li&gt;Hive 架构&lt;/li&gt;
&lt;li&gt;MetaStore 组件&lt;/li&gt;
&lt;li&gt;Hive 执行流程&lt;/li&gt;
&lt;li&gt;Hive 与传统数据库相比&lt;/li&gt;
&lt;li&gt;创建 Hive 表&lt;/li&gt;
&lt;li&gt;分区表和分桶表&lt;/li&gt;
&lt;li&gt;Hive 优化&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Hive" scheme="https://www.chentyit.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Yarn 概念</title>
    <link href="https://www.chentyit.com/2019/10/10/Yarn-%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.chentyit.com/2019/10/10/Yarn-概念/</id>
    <published>2019-10-10T06:48:36.000Z</published>
    <updated>2019-10-10T07:19:39.516Z</updated>
    
    <content type="html"><![CDATA[<ul><li>简介</li><li>重要概念</li><li>执行过程</li><li>YARN的高可用</li></ul><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Yarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序</p><h2 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h2><ol><li>yarn 并不清楚用户提交的程序的运行机制</li><li>yarn 只提供运算资源的调度（用户向 yarn 申请资源，yarn 就负责分配资源）</li><li>yarn 中的主管角色叫 ResourceManager</li><li>yarn 中具体提供运算资源的角色叫 NodeManager</li><li>yarn 于运行用户程序完全解耦，意味着 yarn 上可以运行各种类型的分布式运算程序（MapReduce 只是其中一种）</li><li>spark 等运算框架都可以整合在 yarn 上运行，只要他们各自的框架中有符合 yarn 规范的资源请求机制即可</li></ol><h2 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h2><ol><li>客户端通过 YarnRunner 向 RM 提交 job 任务。申请运行一个 MR 程序，RM 返回一个 job id，资源提交路径</li><li>客户端提交 MR 相关的资源文件：job.xml，job.jar，job.split，job.splitmateinfo</li><li>客户端通知 RM 资源提交完毕，RM 初始化任务创建一个 Container，RM 随机在一台 NM 上启动一个 MRAppMaster，MRAppMaster 向 RM 申请资源分配容器（CPU，RAM，job 等资源）</li><li>在 NM 上启动 MapTask，Task 在执行的时候会向 MRAppMaster 汇报进度和状态，MRAppMaster 会向 RM 注册，用户可以通过 RM 查看当前作业的状态</li><li>MRAppMaster 会向 RM 为各个任务申请资源，并监控状态直到任务完成</li><li>MRAppMaster 等待所有 MapTask 执行完毕，再启动 ReduceTask</li><li>所有任务完成后，MRAppMaster 通知 RM 回收资源</li></ol><p><img src="/2019/10/10/Yarn-概念/yarn.png" alt="yarn"></p><h2 id="YARN的高可用"><a href="#YARN的高可用" class="headerlink" title="YARN的高可用"></a>YARN的高可用</h2><p><strong>ResourceManager：</strong>基于 Zookeeper 实现高可用机制，避免单节点故障</p><p><strong>NodeManager：</strong>执行失败后，ResourceManager 将失败任务告诉对应的 ApplicationMaster，由 ApplicationMaster 来决定如何处理失败的任务</p><p><strong>ApplicationMaster：</strong>执行失败后，由 ResourceManager 负责重启；ApplicationMaster 需处理内部的容错问题，并保存已经运行完成的 Task，重启后无需重新运行</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;简介&lt;/li&gt;
&lt;li&gt;重要概念&lt;/li&gt;
&lt;li&gt;执行过程&lt;/li&gt;
&lt;li&gt;YARN的高可用&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Yarn" scheme="https://www.chentyit.com/tags/Yarn/"/>
    
  </entry>
  
  <entry>
    <title>MR 概念</title>
    <link href="https://www.chentyit.com/2019/10/10/MR-%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.chentyit.com/2019/10/10/MR-概念/</id>
    <published>2019-10-10T03:53:45.000Z</published>
    <updated>2019-10-14T01:09:36.761Z</updated>
    
    <content type="html"><![CDATA[<ul><li>简介</li><li>MR 程序组成部分</li><li>MapTask 并行度</li><li>ReduceTask 的并行度</li><li>Shuffle 机制</li><li>文件太小如何处理</li><li>自定义分区</li><li>二次排序</li></ul><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>MapReduce 是一个分布式运算程序的<strong>编程框架</strong>，是用户开发<strong>基于 hadoop 的数据分析应用</strong>的核心框架</p><p>MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 hadoop 集群上</p><h2 id="MR-程序组成部分"><a href="#MR-程序组成部分" class="headerlink" title="MR 程序组成部分"></a>MR 程序组成部分</h2><ul><li><p>Split</p><p>将程序输入的数据进行切分，每个 split 交给一个 MapTask 执行。split 的数量可以自己定义，默认情况下一个文件一个 split</p></li><li><p>Map</p><p>输入 为一个 split 中的数据，对 split 中的数据进行拆分，并以&lt;key, value&gt; 对的格式保存数据</p></li><li><p>Shuffle / Combine / sort</p><p>这几个过程在简单的 MR 程序中并不需要我们关注，因为源代码中已经给出了一些默认的 Shuffle / Combine / sort 处理器，作用分别是：</p><ul><li>Combine：对 MapTask 产生的结果在本地节点上进行合并、统计等，以减少后续整个集群间的 Shuffle 过程所需要传输的数据量</li><li>Shuffle / Sort：将集群中各个 MapTask 的处理结果在集群间进行传输，排序，数据经过这个阶段之后就作为 Reduce 端的输入</li></ul></li><li><p>Reduce</p><p>ReduceTask 的输入数据是经过排序之后的一系列 key 值相同的 &lt;key, value&gt; 对，ReduceTask 对其进行统计等处理，产生最终的输出。ReduceTask 的数量可以设置</p></li></ul><h2 id="MapTask-并行度"><a href="#MapTask-并行度" class="headerlink" title="MapTask 并行度"></a>MapTask 并行度</h2><p>选择并发数的影响因素：</p><ol><li>运算节点的硬件配置</li><li>运算任务的类型：CPU 密集型还是 IO 密集型</li><li>运算任务的数据量</li></ol><h3 id="Task-并行度的经验"><a href="#Task-并行度的经验" class="headerlink" title="Task 并行度的经验"></a>Task 并行度的经验</h3><ul><li>最好每个 task 执行的时间至少一分钟</li><li>如果 job 的每个 map 或者 reducetask 运行时间比较短，那就应该减少 job 的 map 或者 reduce 的数量，因为每个 task 的 setup 和加入到调度器需要消耗一定的时间，如果每个 task 都花不了太多时间，就没必要有太多的 task</li><li>默认情况下，每个 task 都是一个 jvm 实例，都需要开启和销毁，jvm 的开启和销毁所需要的时间比执行的时间要长，所以配置 jvm 的可重用性可以改善性能</li><li>mapred.job.reuse.jvm.num.tasks，默认是 1，表示一个 JVM 上最可以顺序执行的 task 数目是 1，也就是说一个 task 启动一个 JVM</li><li>如果 input 的文件非常大，可以考虑将 hdfs 上的每个 blocksize 设置大一些，比如 256MB 或者 512MB</li><li>JVM 重用技术不是指同一个 job 的多个 task 可以同时运行于同一个 JVM 上，而是排队按顺序执行</li></ul><h2 id="ReduceTask-的并行度"><a href="#ReduceTask-的并行度" class="headerlink" title="ReduceTask 的并行度"></a>ReduceTask 的并行度</h2><p>maptask 的并发数由切片数决定不同，reducetask 数量是可以手动设置的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认为 1，手动设置为 4</span></span><br><span class="line">job.setNumReduceTask(<span class="number">4</span>);</span><br></pre></td></tr></table></figure><p>如果数据分布不均匀，就有可能在 reduce 阶段产生数据倾斜</p><p><strong>注意：</strong>ReduceTask 数量并不是任意设置的，需要考虑业务逻辑需求，有些情况需要计算全局汇总结果，就只能有一个 reducetask</p><p>尽量不要运行太多的 reducetask。最好 reduce 数量和集群中的 reduce 持平或者比集群中的 reduce slots 小</p><h2 id="Shuffle-机制"><a href="#Shuffle-机制" class="headerlink" title="Shuffle 机制"></a>Shuffle 机制</h2><p><img src="/2019/10/10/MR-概念/shuffer.png" alt="shuffle"></p><p><strong>wordcount 的 shuffle 详细过程：</strong></p><ol><li><strong>读取数据：</strong>MR 默认使用 TextInputFormat 来获取切片的数量，通过 createRecordReader 方法获取 RecordReader，缺省的 RecordReader 是 LineRecordReader，通过调用 LineRecordReader 的 nextKeyValue 方法获取每行的数据</li><li><strong>溢出：</strong>通过 OutputCollector 收集器收集读取的数据，缺省使用的是 Task.CombineOutputCollector，调用其 collect 进行溢出，收集器默认的空间是 100 M，当收集器达到 80% 的时候开始溢出</li><li><strong>分区排序：</strong>mapreduce 默认通过 HashPartitioner 进行分区且有序（在内存中结束）</li><li><strong>输出文件：</strong>maptask 的最终输出文件分区有序</li><li><strong>Reduce拉取文件：</strong>从各个分区中拉取相同的 key 到 reducetask 中，合并归并排序，相同的 key 看作一个 group</li><li><strong>写出数据：</strong>FileOutputFormat 调用 RecordWriter 写出文件，相同的 key 会写到一个分区</li></ol><h2 id="文件太小如何处理"><a href="#文件太小如何处理" class="headerlink" title="文件太小如何处理"></a>文件太小如何处理</h2><p><a href="https://blog.csdn.net/zgc625238677/article/details/51793259" target="_blank" rel="noopener">小文件处理方法</a></p><h2 id="自定义分区"><a href="#自定义分区" class="headerlink" title="自定义分区"></a>自定义分区</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomizeParitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;String, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        map.put(<span class="string">"139"</span>, <span class="number">0</span>);</span><br><span class="line">        map.put(<span class="string">"186"</span>, <span class="number">1</span>);</span><br><span class="line">        map.put(<span class="string">"187"</span>, <span class="number">2</span>);</span><br><span class="line">        map.put(<span class="string">"136"</span>, <span class="number">3</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, NullWritable value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        String key_ = key.toString().subString(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">        <span class="keyword">return</span> map.get(key_) != <span class="keyword">null</span> ? map.get(key_) : <span class="number">4</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="二次排序"><a href="#二次排序" class="headerlink" title="二次排序"></a>二次排序</h2><p>在 hadoop 中一般都是按照 key 进行排序的，但有时候还需要按照 value 进行排序</p><p>有两种方法：buffer and int memory sort 和 value-to-key conversion</p><ul><li>buffer and in memory sort：主要是在 reduce() 函数中，将每个 key 对应的 value 值保存下来，进行排序。缺点是会造成 out of memory</li><li>value-to-key conversion：主要思想是将 key 和 value 拼接成一个组合 key，然后进行排序，这样 reduce() 函数获取结果就实现了按照 key 排序，然后按照 value 排序，但是需要用户自己实现 paritioner，以便只按照 key 进行数据划分</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;简介&lt;/li&gt;
&lt;li&gt;MR 程序组成部分&lt;/li&gt;
&lt;li&gt;MapTask 并行度&lt;/li&gt;
&lt;li&gt;ReduceTask 的并行度&lt;/li&gt;
&lt;li&gt;Shuffle 机制&lt;/li&gt;
&lt;li&gt;文件太小如何处理&lt;/li&gt;
&lt;li&gt;自定义分区&lt;/li&gt;
&lt;li&gt;二次排序&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="MapReduce" scheme="https://www.chentyit.com/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>HDFS 概念</title>
    <link href="https://www.chentyit.com/2019/10/09/HDFS-%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.chentyit.com/2019/10/09/HDFS-概念/</id>
    <published>2019-10-09T10:26:21.000Z</published>
    <updated>2019-10-09T10:39:15.883Z</updated>
    
    <content type="html"><![CDATA[<ul><li>简介</li><li>重要特性</li><li>HDFS 角色说明</li><li>HDFS 高可用机制</li><li>读数据流程</li><li>写数据流程</li></ul><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h3><p>分而治之：将大文件大批量文件，分布式存放在大量服务器上，以便于采取分而治之的方式对海量数据进行运算分析</p><h3 id="重点概念"><a href="#重点概念" class="headerlink" title="重点概念"></a>重点概念</h3><p>文件切块、副本存放、元数据</p><h2 id="重要特性"><a href="#重要特性" class="headerlink" title="重要特性"></a>重要特性</h2><ol><li><p>HDFS 中的文件在物理上是<strong>分块存储</strong>，块的大小可以通过配置参数 dfs.blocksize 来规定，默认大小在 hadoop2.x 版本中是 128M</p></li><li><p>HDFS 文件系统会给客户端提供一个<strong>统一的抽象目录树</strong>，客户端通过路径来访问文件</p></li><li><p>目录结构及文件分块信息（元数据）的管理由 namenode 节点承担</p><p>namenode 是 HDFS 集群主节点，负责维护整个 HDFS 文件系统的目录树，以及每一个路径（文件）所对应的 block 块信息（block 的 id，及所在的 datanode 服务器）</p></li><li><p>文件的各个 block 的存储管理由 datanode 节点承担</p><p>datanode 是 HDFS 集群从节点，每个 block 都可以在多个 datanode 上存储多个副本（可以通过 dfs.replication 设置）</p></li><li><p>HDFS 是设计成适应一次写入，多次读出的场景，且不支持文件的修改</p></li></ol><h2 id="HDFS-角色说明"><a href="#HDFS-角色说明" class="headerlink" title="HDFS 角色说明"></a>HDFS 角色说明</h2><table><thead><tr><th>名称</th><th>作用</th></tr></thead><tbody><tr><td>namenode</td><td>接受客户端的读写请求<br>存储元数据信息<br>接收 datanode 心跳报告<br>负载均衡<br>分配数据块的存储节点</td></tr><tr><td>datanode</td><td>真正处理客户端的读写请求<br>向 namenode 发送心跳<br>向 namenode 发送块报告<br>真正存储数据<br>副本之间的相互复制</td></tr><tr><td>journalnode</td><td>两个 namenode 为了数据同步，会通过一组称作 journalnode 的独立进程相互通信<br>当 active 状态的 namenode 的命名空间有任何修改时，会告知大部分的 journalnode 进程</td></tr><tr><td>客户端</td><td>进行数据块的物理切分<br>向 namenode 发送读写请求<br>向 namenode发送读写响应</td></tr></tbody></table><h2 id="HDFS-高可用机制"><a href="#HDFS-高可用机制" class="headerlink" title="HDFS 高可用机制"></a>HDFS 高可用机制</h2><p><a href="https://blog.csdn.net/u012736748/article/details/79534019" target="_blank" rel="noopener">HDFS的高可用机制详解（journalnode 及 editlog）</a></p><p><a href="https://blog.csdn.net/u012736748/article/details/79541311" target="_blank" rel="noopener">HDFS高可用（HA）之ZKFC详解</a></p><h2 id="读数据流程"><a href="#读数据流程" class="headerlink" title="读数据流程"></a>读数据流程</h2><ol><li><p>客户端通过调用 FileSyste 对象 DistributedFileSystem（以下简称 DFS） 的 open() 方法带打开希望读取的文件</p></li><li><p>DFS 对象通过远程调用（RPC）来调用 namenode，以确定文件起始块的位置</p><p>namenode 返回存储该数据块副本的 datanod 的地址，datanode 也会根据与客户端的距离来排序（就近原则读取信息）</p></li><li><p>DFS 类返回一个 FSDataInputStream 对象给客户端用于读取数据，FSDataInputStream 封装 DFSInputStream 对象，该对象管理 datanode 和 namenode 的 I/O</p></li><li><p>客户端调用 read() 方法</p></li><li><p>DFSInputStream 连接地址最近的一个 datanode，然后反复调用 read() 方法，将数据从 datanode 中传输到客户端。</p></li><li><p>读到块末端后，DFSInputStream 关闭连接，并开始寻找下一个最佳的 datanode，执行 4-5-6 步直至读取完成</p></li><li><p>读取完成后，就调用 close() 方法关闭 FSDataInputStream</p></li></ol><p><img src="/2019/10/09/HDFS-概念/读数据.png" alt="读数据"></p><h2 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h2><ol><li><p>客户端对 DistributedFileSystem（以下简称 DFS） 对象调用 create() 方法</p></li><li><p>DFS 对 namenode 创建一个 RPC 调用，在文件系统命名空间中创建一个文件，但没有对应的数据块</p><p>namenode 检查文件系统中是否存在这个文件，若不存在且客户端有权限创建，则创建一条记录，反之抛出异常</p><p>DFS 向客户端返回一个 FSDataOutputStream 对象，FSDataOutputStream 封装了 DFSOutputStream 对象</p></li><li><p>客户端调用 write 写数据</p></li><li><p>DFSOutputStream 将客户端的数据分包写入内部队列，称为<strong>“数据队列”</strong></p><p>队列的作用是选择一组合适的 datanode，要求 namenode 分配新的数据块，按照顺序发送数据到 datanode 中</p></li><li><p>DFSOutputStream 同时维护着一个<strong>“确认队列”</strong>，所有 datanode 确认信息后，数据包才会从确认队列中删除</p></li><li><p>客户端完成数据写入后调用 close() 方法</p></li><li><p>联系 namenode 写入完成，等待确认</p></li></ol><p><img src="/2019/10/09/HDFS-概念/写数据.png" alt="写数据"></p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;简介&lt;/li&gt;
&lt;li&gt;重要特性&lt;/li&gt;
&lt;li&gt;HDFS 角色说明&lt;/li&gt;
&lt;li&gt;HDFS 高可用机制&lt;/li&gt;
&lt;li&gt;读数据流程&lt;/li&gt;
&lt;li&gt;写数据流程&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="HDFS" scheme="https://www.chentyit.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 整合 Flume &amp; Kafka</title>
    <link href="https://www.chentyit.com/2019/10/05/Spark-Streaming-%E6%95%B4%E5%90%88-Flume-Kafka/"/>
    <id>https://www.chentyit.com/2019/10/05/Spark-Streaming-整合-Flume-Kafka/</id>
    <published>2019-10-05T08:58:21.000Z</published>
    <updated>2019-10-05T09:01:27.383Z</updated>
    
    <content type="html"><![CDATA[<ul><li>大致流程</li><li>使用代码生成 Log4j 日志文件</li><li>Flume 配置文件</li></ul><a id="more"></a><h2 id="大致流程"><a href="#大致流程" class="headerlink" title="大致流程"></a>大致流程</h2><p><img src="/2019/10/05/Spark-Streaming-整合-Flume-Kafka/Spark Streaming 整合 Flume &amp; Kafka.png" alt="大致流程"></p><h2 id="使用代码生成-Log4j-日志文件"><a href="#使用代码生成-Log4j-日志文件" class="headerlink" title="使用代码生成 Log4j 日志文件"></a>使用代码生成 Log4j 日志文件</h2><p>在 log4j 的配置文件中指定将日志文件发送到 flume</p><p><strong>Java 代码</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoggerGenerator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Logger logger = Logger.getLogger(LoggerGenerator.class.getName());</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            logger.info(<span class="string">"value : "</span> + index++);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>log4j.properties</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO,stdout,flume</span><br><span class="line"></span><br><span class="line">log4j.appender.stdout = org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.target = System.out</span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br><span class="line"></span><br><span class="line">log4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender</span><br><span class="line">log4j.appender.flume.Hostname = 192.168.10.114</span><br><span class="line">log4j.appender.flume.Port = 41414</span><br><span class="line">log4j.appender.flume.UnsafeMode = true</span><br></pre></td></tr></table></figure><h2 id="Flume-配置文件"><a href="#Flume-配置文件" class="headerlink" title="Flume 配置文件"></a>Flume 配置文件</h2><p><strong>streaming.conf</strong>（用来测试 log4j -&gt; flume）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">agent1.sources=avro-source</span><br><span class="line">agent1.channels=logger-channel</span><br><span class="line">agent1.sinks=log-sink</span><br><span class="line"></span><br><span class="line">#define source</span><br><span class="line">agent1.sources.avro-source.type=avro</span><br><span class="line">agent1.sources.avro-source.bind=hadoop000</span><br><span class="line">agent1.sources.avro-source.port=41414</span><br><span class="line"></span><br><span class="line">#define channel</span><br><span class="line">agent1.channels.logger-channel.type=memory</span><br><span class="line"></span><br><span class="line">#define sink</span><br><span class="line">agent1.sinks.log-sink.type=logger</span><br><span class="line"></span><br><span class="line">agent1.sources.avro-source.channels=logger-channel</span><br><span class="line">agent1.sinks.log-sink.channel=logger-channel</span><br></pre></td></tr></table></figure><p><strong>streaming2.conf</strong>（log4j -&gt; flume -&gt; kafka)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">agent1.sources=avro-source</span><br><span class="line">agent1.channels=logger-channel</span><br><span class="line">agent1.sinks=kafka-sink</span><br><span class="line"></span><br><span class="line">#define source</span><br><span class="line">agent1.sources.avro-source.type=avro</span><br><span class="line">agent1.sources.avro-source.bind=hadoop000</span><br><span class="line">agent1.sources.avro-source.port=41414</span><br><span class="line"></span><br><span class="line">#define channel</span><br><span class="line">agent1.channels.logger-channel.type=memory</span><br><span class="line"></span><br><span class="line">#define sink</span><br><span class="line">agent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">agent1.sinks.kafka-sink.topic = streamingtopic_cty</span><br><span class="line">agent1.sinks.kafka-sink.brokerList = hadoop000:9092</span><br><span class="line">agent1.sinks.kafka-sink.requiredAcks = 1</span><br><span class="line"># 到达 20 条数据才 sink</span><br><span class="line">agent1.sinks.kafka-sink.batchSize = 20</span><br><span class="line"></span><br><span class="line">agent1.sources.avro-source.channels=logger-channel</span><br><span class="line">agent1.sinks.kafka-sink.channel=logger-channel</span><br></pre></td></tr></table></figure><h2 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h2><p><strong>KafkaStreamingApp.scala</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** 输入参数 192.168.10.114:9092 streamingtopic_cty */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaStreamingApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: KafkaStreamingApp &lt;brokers&gt; &lt;topics&gt;"</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> <span class="type">Array</span>(brokers, topics) = args</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaStreamingApp"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topicsSet = topics.split(<span class="string">","</span>).toSet</span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">"metadata.broker.list"</span> -&gt; brokers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc, kafkaParams, topicsSet)</span><br><span class="line"></span><br><span class="line">    messages.map(_._2).count().print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;大致流程&lt;/li&gt;
&lt;li&gt;使用代码生成 Log4j 日志文件&lt;/li&gt;
&lt;li&gt;Flume 配置文件&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flume" scheme="https://www.chentyit.com/tags/Flume/"/>
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
      <category term="Kafka" scheme="https://www.chentyit.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Java面试题03</title>
    <link href="https://www.chentyit.com/2019/10/03/Java%E9%9D%A2%E8%AF%95%E9%A2%9803/"/>
    <id>https://www.chentyit.com/2019/10/03/Java面试题03/</id>
    <published>2019-10-03T02:06:39.000Z</published>
    <updated>2019-10-03T10:41:01.557Z</updated>
    
    <content type="html"><![CDATA[<ul><li>SpringBoot / SpringCloud</li><li>MyBatis 模块</li><li>Kafka 模块</li><li>Zookeeper 模块</li><li>MySQL 模块</li><li>Redis 模块</li><li>JVM 模块</li></ul><p>题库来源于 <a href="https://www.javazhiyin.com/42272.html" target="_blank" rel="noopener">Java知音</a></p><a id="more"></a><h2 id="SpringBoot-SpringCloud"><a href="#SpringBoot-SpringCloud" class="headerlink" title="SpringBoot / SpringCloud"></a>SpringBoot / SpringCloud</h2><h3 id="104-什么是-springboot？"><a href="#104-什么是-springboot？" class="headerlink" title="104. 什么是 springboot？"></a>104. 什么是 springboot？</h3><p>springboot 是为 spring 服务的，是用来简化新 spring 应用的初始搭建一斤开发过程的</p><h3 id="105-为什么要用-springboot？"><a href="#105-为什么要用-springboot？" class="headerlink" title="105. 为什么要用 springboot？"></a>105. 为什么要用 springboot？</h3><ul><li>配置简单</li><li>独立运行</li><li>自动装配</li><li>无代码生成和 xml 配置</li><li>提供应用监控</li><li>易上手</li><li>提升开发效率</li></ul><h3 id="106-springboot-核心配置文件是什么？"><a href="#106-springboot-核心配置文件是什么？" class="headerlink" title="106. springboot 核心配置文件是什么？"></a>106. springboot 核心配置文件是什么？</h3><ul><li>bootstrap（.yml 或者 .properties）：bootstrap 有父 ApplicationContext 加载的，比 application 优先加载，且 bootstrap 里面的属性不能被覆盖</li><li>application（.yml 获取 .properties）：用于 springboot 项目的自动化配置</li></ul><h3 id="107-springboot-配置文件有哪几种类型？他们有什么区别？"><a href="#107-springboot-配置文件有哪几种类型？他们有什么区别？" class="headerlink" title="107. springboot 配置文件有哪几种类型？他们有什么区别？"></a>107. springboot 配置文件有哪几种类型？他们有什么区别？</h3><p>配置文件有 .properties 和 .yml 格式，主要区别是熟悉风格不同</p><p>yml 格式不支持 @PropertySource 注解导入</p><h3 id="108-springboot-有哪些方式可以实现热部署？"><a href="#108-springboot-有哪些方式可以实现热部署？" class="headerlink" title="108. springboot 有哪些方式可以实现热部署？"></a>108. springboot 有哪些方式可以实现热部署？</h3><p>使用 devtools 启动热部署，添加 devtools 库，在配置文件中把 spring.devtools.restart.enable 设置为 true</p><p>使用 Intellij Idea 编译器，勾选上自动编译或手动重新编译</p><h3 id="109-JPA-全称-Java-Persistence-API，是-Java-持久化接口规范，hibernate-属于-jpa-的具体实现"><a href="#109-JPA-全称-Java-Persistence-API，是-Java-持久化接口规范，hibernate-属于-jpa-的具体实现" class="headerlink" title="109. JPA 全称 Java Persistence API，是 Java 持久化接口规范，hibernate 属于 jpa 的具体实现"></a>109. JPA 全称 Java Persistence API，是 Java 持久化接口规范，hibernate 属于 jpa 的具体实现</h3><h3 id="110-什么是-springcloud？"><a href="#110-什么是-springcloud？" class="headerlink" title="110. 什么是 springcloud？"></a>110. 什么是 springcloud？</h3><p>springcloud 是一系列框架的有序集合，它利用 springboot 的开发便利性，简化了分布式系统基础设施的开发，如服务发现注册，配置中心，消息总线，负载均衡、断路器、数据监控等，都可以用 springboot 的开发风格做到意见启动和部署</p><h3 id="111-springcloud-阻断器的作用是什么？"><a href="#111-springcloud-阻断器的作用是什么？" class="headerlink" title="111. springcloud 阻断器的作用是什么？"></a>111. springcloud 阻断器的作用是什么？</h3><p>在分布式架构中，住短期模式的作用也是类似的，当某个服务单元发生故障之后，通过断路器的故障监控，向调用方返回一个错误响应，而不是长时间的等待，这样就不会使得线程因调用故障服务长时间占用不释放，避免了故障在分布式系统中的蔓延</p><h2 id="MyBatis-模块"><a href="#MyBatis-模块" class="headerlink" title="MyBatis 模块"></a>MyBatis 模块</h2><h3 id="125-MyBatis-中-和-的区别是什么？"><a href="#125-MyBatis-中-和-的区别是什么？" class="headerlink" title="125. MyBatis 中 #{} 和 ${} 的区别是什么？"></a>125. MyBatis 中 #{} 和 ${} 的区别是什么？</h3><p>#{} 是预编译处理，${} 是字符串替换</p><p>再使用 #{} 时，MyBatis 会将 SQL 中的 #{} 替换成 “?”，配合 PreparedStatement 的 set 方法赋值，有效防止 SQL 注入，保证程序的运行安全</p><h3 id="126-MyBatis-有几种分页模式？"><a href="#126-MyBatis-有几种分页模式？" class="headerlink" title="126. MyBatis 有几种分页模式？"></a>126. MyBatis 有几种分页模式？</h3><ul><li><strong>逻辑分页：</strong>使用 MyBatis 自带的 RowBounds 进行分页，他是一次性查询很多数据，然后在数据中再进行检索</li><li><strong>物理分页：</strong>自己手写 SQL 分页或使用分页插件 PageHelper，去数据库查询指定条数的分页数据的形式</li></ul><h3 id="127-RowBounds-是一次性查询全部结果吗？"><a href="#127-RowBounds-是一次性查询全部结果吗？" class="headerlink" title="127. RowBounds 是一次性查询全部结果吗？"></a>127. RowBounds 是一次性查询全部结果吗？</h3><p>RowBounds 表面是在所有数据中检索数据，其实并非是一次性查询出所有数据，因为 MyBatis 是对 jdbc 的封装，在 jdbc 驱动中有一个 Fetch Size 的配置，它规定了每次最多从数据库中查询多少条数据，会在执行 next() 的时候，去查询更多的数据</p><p>对于 jdbc 来说，当调用 next() 的时候回自动完成查询工作，有效防止内存溢出</p><h3 id="128-MyBatis-逻辑分页和物理分页的区别？"><a href="#128-MyBatis-逻辑分页和物理分页的区别？" class="headerlink" title="128. MyBatis 逻辑分页和物理分页的区别？"></a>128. MyBatis 逻辑分页和物理分页的区别？</h3><p>逻辑分页是一次性查询很多数据，然后再在结果中检索分页的数据，弊端就是需要消耗大量的内存，有内存溢出的风险、对数据塔里较大</p><p>物理分页是从数据库中查询指定条数的数据，弥补了一次性全部查出所有数据的种种缺点</p><h3 id="129-MyBatis-延迟加载的原理是什么？"><a href="#129-MyBatis-延迟加载的原理是什么？" class="headerlink" title="129. MyBatis 延迟加载的原理是什么？"></a>129. MyBatis 延迟加载的原理是什么？</h3><p>MyBatis 支持延迟加载，设置 lazyLoadingEnabled=true 即可</p><p>延迟加载的原理是调用的时候触发加载，而不是在初始化的时候就加载信息</p><h3 id="130-说一下-MyBatis-的一级缓存和二级缓存"><a href="#130-说一下-MyBatis-的一级缓存和二级缓存" class="headerlink" title="130. 说一下 MyBatis 的一级缓存和二级缓存"></a>130. 说一下 MyBatis 的一级缓存和二级缓存</h3><ul><li>一级缓存：基于 PerpetualCache 和 HashMap 本地缓存，它的声明周期是和 SQLSession 一致的，有多个 SQLSession 或者分布式的环境中数据库操作，可能会出现脏数据。当 Session flush 或 close 之后该 Session 中的所有 Cache 就将清空，默认一级缓存是开启的</li><li>二级缓存：也是基于 PerpetualCache 的 HashMap 本地缓存，不同在于其存储作用域为 Mapper 级别的，如果多个 SQLSession 之间需要共享缓存，则需要使用到二级缓存，并且二级缓存可自定义存储源，如 Ehcache。默认不打开二级缓存，使用二级缓存属性需要实现 Serializable 序列化接口</li></ul><p>开启二级缓存数据查询流程：二级缓存 -&gt; 一级缓存 -&gt; 数据库</p><p>缓存更新机制：当某一作用域（一级缓存 Session / 二级缓存 Mapper）进行了 C / U / D 操作之后，默认该作用域下所有 select 中的缓存将被 clear</p><h3 id="131-MyBatis-和-hibernate-区别有哪些？"><a href="#131-MyBatis-和-hibernate-区别有哪些？" class="headerlink" title="131. MyBatis 和 hibernate 区别有哪些？"></a>131. MyBatis 和 hibernate 区别有哪些？</h3><ul><li><strong>灵活性：</strong>MyBatis 灵活</li><li><strong>可移植性：</strong>MyBatis 需要自己手写 SQL，每个数据库不同，SQL也不同，移植性较差</li><li><strong>学习和使用门槛：</strong>MyBatis 简单，入门和使用快</li><li><strong>二级缓存：</strong>hibernate 有更好的二级缓存，且可以自行更换为第三方的二级缓存</li></ul><h3 id="132-MyBatis-有哪些执行器？"><a href="#132-MyBatis-有哪些执行器？" class="headerlink" title="132. MyBatis 有哪些执行器？"></a>132. MyBatis 有哪些执行器？</h3><ul><li><strong>SimpleExecutor：</strong>每执行一次 update 或 select 就开启一个 Statement 对象，用完立刻关闭 Statement 对象</li><li><strong>ReuseExecutor：</strong>执行 update 或 select，以 SQL 作为 key 查找 Statement，存在就使用，不存在就创建，用完后不关闭 Statement 对象，而是放置于 Map 内供下一次使用。简单地说就是重复使用 Statement 对象</li><li><strong>BatchExecutor：</strong>执行 update（没有 select，jdbc 批处理不支持 select），将所有 SQL 都添加到批处理（addBatch()）中，等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象，每个 Statement 对象都是 addBatch() 完毕后，等待逐一执行 executeBatch() 批处理，与 jdbc 批处理相同</li></ul><h3 id="133-MyBatis-分页插件的实现原理是什么？"><a href="#133-MyBatis-分页插件的实现原理是什么？" class="headerlink" title="133. MyBatis 分页插件的实现原理是什么？"></a>133. MyBatis 分页插件的实现原理是什么？</h3><p>分页插件的基本原理是使用 MyBatis 提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的 SQL，然后重写 SQL，添加对应的物理分页语句和物理分页参数</p><h2 id="Kafka-模块"><a href="#Kafka-模块" class="headerlink" title="Kafka 模块"></a>Kafka 模块</h2><h3 id="152-kafka-可以脱离-zookeeper-单独使用吗？为什么？"><a href="#152-kafka-可以脱离-zookeeper-单独使用吗？为什么？" class="headerlink" title="152. kafka 可以脱离 zookeeper 单独使用吗？为什么？"></a>152. kafka 可以脱离 zookeeper 单独使用吗？为什么？</h3><p>不能，因为 kafka 使用 zookeeper 管理和协调 kafka 的节点服务器</p><h3 id="153-kafka-有几种数据保留的策略？"><a href="#153-kafka-有几种数据保留的策略？" class="headerlink" title="153. kafka 有几种数据保留的策略？"></a>153. kafka 有几种数据保留的策略？</h3><ul><li><strong>按照过期时间保留</strong></li><li><strong>按照存储的消息大小保留</strong></li></ul><h3 id="154-kafka-同时设置了-7-天和-10G-清楚数据，到第五天的时候消息达到了-10G，kafka-将如何处理？"><a href="#154-kafka-同时设置了-7-天和-10G-清楚数据，到第五天的时候消息达到了-10G，kafka-将如何处理？" class="headerlink" title="154. kafka 同时设置了 7 天和 10G 清楚数据，到第五天的时候消息达到了 10G，kafka 将如何处理？"></a>154. kafka 同时设置了 7 天和 10G 清楚数据，到第五天的时候消息达到了 10G，kafka 将如何处理？</h3><p>kafka 会执行数据清楚工作，时间和大小不论条件是否满足，都会清空数据</p><h3 id="155-什么情况下会导致-kafka-运行变慢？"><a href="#155-什么情况下会导致-kafka-运行变慢？" class="headerlink" title="155. 什么情况下会导致 kafka 运行变慢？"></a>155. 什么情况下会导致 kafka 运行变慢？</h3><ul><li>CPU 性能瓶颈</li><li>磁盘读写瓶颈</li><li>网络瓶颈</li></ul><h3 id="156-使用-kafka-集群需要注意什么？"><a href="#156-使用-kafka-集群需要注意什么？" class="headerlink" title="156. 使用 kafka 集群需要注意什么？"></a>156. 使用 kafka 集群需要注意什么？</h3><p>集群的数量不是越多越好，最好不要超过 7 个，节点越多，消息复制需要的时间就越长，整个群组的吞吐量就会越低</p><p>集群数量最好是单数，因为超过一半故障，集群就不能用了，设置为单数容错率更高</p><h2 id="Zookeeper-模块"><a href="#Zookeeper-模块" class="headerlink" title="Zookeeper 模块"></a>Zookeeper 模块</h2><h3 id="157-Zookeeper-是什么？"><a href="#157-Zookeeper-是什么？" class="headerlink" title="157. Zookeeper 是什么？"></a>157. Zookeeper 是什么？</h3><p>zookeeper 是一个分布式的，开放源码的分布式应用程序协调服务器，是 google chubby 开源实现，是 hadoop 和 hbase 的重要组件。它是一个分布式应用提供一致性服务的软件，提供的功能包括：配置维护，域名服务，分布式同步，租服务等</p><h3 id="158-zookeeper-都有哪些功能？"><a href="#158-zookeeper-都有哪些功能？" class="headerlink" title="158. zookeeper 都有哪些功能？"></a>158. zookeeper 都有哪些功能？</h3><ul><li><strong>集群管理：</strong>监控节点存活状态</li><li><strong>主节点选举：</strong>主节点挂掉之后，可以从备用的节点开始新一轮选主，注解点选举说的就是这个选举过程，使用 zookeeper 可以协助完成这个过程</li><li><strong>分布式锁：</strong>zookeeper 提供两种锁，<strong>独占锁</strong>和<strong>共享锁</strong>。独占锁即一次只能有一个线程使用资源，共享锁是读锁共享，读写互斥，即可以有多线程同时读同一个资源，如果要使用写锁，也只能有一个线程使用。zookeeper 可以对分布式锁进行控制</li><li><strong>命名服务：</strong>在分布式系统中，通过使用命名服务，客户端应用程序能够根据指定名字来获取资源或服务的地址，提供者等信息</li></ul><h3 id="159-zookeeper-有几种部署模式？"><a href="#159-zookeeper-有几种部署模式？" class="headerlink" title="159. zookeeper 有几种部署模式？"></a>159. zookeeper 有几种部署模式？</h3><ul><li>单机部署</li><li>集群部署</li><li>伪集群部署</li></ul><h3 id="160-zookeeper-怎么保证主从节点的状态同步？"><a href="#160-zookeeper-怎么保证主从节点的状态同步？" class="headerlink" title="160. zookeeper 怎么保证主从节点的状态同步？"></a>160. zookeeper 怎么保证主从节点的状态同步？</h3><p>zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步，实现这个机制的协议叫做 zab 协议，zab 协议有两种：</p><ul><li>恢复模式（选主）</li><li>广播模式（同步）</li></ul><p>当服务服务启动或者在领导者崩溃后，zab 就进入了恢复模式，当领导者被选择出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了，状态同步保证了 leader 和 server 具有相同的系统状态</p><h3 id="161-集群中为什么要有主节点？"><a href="#161-集群中为什么要有主节点？" class="headerlink" title="161. 集群中为什么要有主节点？"></a>161. 集群中为什么要有主节点？</h3><p>在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他机器可以共享这个结果，这样可以大大减少重复计算，提高性能，所以就需要主节点</p><h3 id="162-集群中有-3-太服务器，其中一个节点宕机，zookeeper-还可以使用吗？"><a href="#162-集群中有-3-太服务器，其中一个节点宕机，zookeeper-还可以使用吗？" class="headerlink" title="162. 集群中有 3 太服务器，其中一个节点宕机，zookeeper 还可以使用吗？"></a>162. 集群中有 3 太服务器，其中一个节点宕机，zookeeper 还可以使用吗？</h3><p>可以，单数服务器只要没有超过一半的服务器宕机，就可以继续使用</p><h3 id="163-说一下-zookeeper-的通知机制"><a href="#163-说一下-zookeeper-的通知机制" class="headerlink" title="163. 说一下 zookeeper 的通知机制"></a>163. 说一下 zookeeper 的通知机制</h3><p>客户端会对某一个 znode 建立一个 watcher 时间，当该 znode 发生变化时，这些客户端就会收到 zookeeper 的通知，然后客户端可以根据 znode 变化来做出业务上的改变</p><h2 id="MySQL-模块"><a href="#MySQL-模块" class="headerlink" title="MySQL 模块"></a>MySQL 模块</h2><h3 id="164-数据库的三范式是什么？"><a href="#164-数据库的三范式是什么？" class="headerlink" title="164. 数据库的三范式是什么？"></a>164. 数据库的三范式是什么？</h3><ul><li>第一范式（1NF）：强调的是列的原子性，<strong>列不可再分</strong></li><li>第二范式（2NF）：<strong>属性完全依赖于主键</strong></li><li>第三范式（3NF）：<strong>属性不依赖于其它非主属性    属性直接依赖于主键</strong></li></ul><h3 id="165-一张自增表里面总共有-7-条数据，删除最后两条，重启-MySQL-数据库，又插入一条数据，id-是多少？"><a href="#165-一张自增表里面总共有-7-条数据，删除最后两条，重启-MySQL-数据库，又插入一条数据，id-是多少？" class="headerlink" title="165. 一张自增表里面总共有 7 条数据，删除最后两条，重启 MySQL 数据库，又插入一条数据，id 是多少？"></a>165. 一张自增表里面总共有 7 条数据，删除最后两条，重启 MySQL 数据库，又插入一条数据，id 是多少？</h3><p>表类型如果是 MyISAM，id 就是 8</p><p>表类型如果是 InnoDB，id 就是 6</p><p>InnoDB 表只会吧自增主键的最大 id 记录在内存中，所以重启之后会导致最大 id 丢失</p><h3 id="166-如何获取当前数据库版本？"><a href="#166-如何获取当前数据库版本？" class="headerlink" title="166. 如何获取当前数据库版本？"></a>166. 如何获取当前数据库版本？</h3><p>使用 select version() 获取当前 MySQL 数据库版本</p><h3 id="167-说下-ACID-是什么？"><a href="#167-说下-ACID-是什么？" class="headerlink" title="167. 说下 ACID 是什么？"></a>167. 说下 ACID 是什么？</h3><ul><li><strong>Atomicity（原子性）：</strong>一个事务中的所有操作，或者全部完成，获取全部不完成，不会结束在中间某个环节，事务在执行过程中发生错误，会被恢复（Rollback）到事务开始之前的状态，就像这个事务从来没有执行过一样。<strong>即事务不可分割，不可简约</strong></li><li><strong>Consistency（一致性）：</strong>在事务开始之前和事务结束之后，数据库的完整性没有被破坏，这表示<strong>写入的资料必须完全复合物所有的预设约束、触发器、级联回滚等</strong></li><li><strong>Isolation（隔离性）：</strong>数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括：<strong>读未提交</strong>，读提交，<strong>可重复读</strong>和<strong>串行化</strong>。</li><li><strong>Durability（持久性）：</strong>事务处理结束后对数据的修改是永久的，即便系统故障也不会丢失。</li></ul><h3 id="168-char-和-varchar-的区别是什么？"><a href="#168-char-和-varchar-的区别是什么？" class="headerlink" title="168. char 和 varchar 的区别是什么？"></a>168. char 和 varchar 的区别是什么？</h3><ul><li>char(n)：固定长度类型<ul><li>优点：效率高</li><li>缺点：占用空间</li><li>适用场景：存储密码的 md5 值，固定长度的使用 char 非常合适</li></ul></li><li>varchar(n)：可变长度，存储的值是每个值占用的字节，加上一个用来记录其长度的字节长度</li></ul><p>所以，从空间上考虑 varchar 比较合适；从效率上考虑 char 比较合适。二者使用需要权衡</p><h3 id="169-float-和-double-的区别是什么？"><a href="#169-float-和-double-的区别是什么？" class="headerlink" title="169. float 和 double 的区别是什么？"></a>169. float 和 double 的区别是什么？</h3><ul><li>float 最多可以存储 8 位的十进制数，并且在内存中占 4 字节。</li><li>double 最多可以存储 16 位的十进制数，在内存中占 8 字节</li></ul><h3 id="170-MySQL-的内连接、左连接、右连接有什么区别？"><a href="#170-MySQL-的内连接、左连接、右连接有什么区别？" class="headerlink" title="170. MySQL 的内连接、左连接、右连接有什么区别？"></a>170. MySQL 的内连接、左连接、右连接有什么区别？</h3><p>内连接关键字：inner join</p><p>左连接关键字：left join</p><p>右连接关键字：right join</p><p>内连接是把匹配的关联数据显示出来</p><p>左连接是左边的表全部显示出来，右边的表显示出符合条件的数据；右连接相反</p><h3 id="171-MySQL-的索引怎么实现的？"><a href="#171-MySQL-的索引怎么实现的？" class="headerlink" title="171. MySQL 的索引怎么实现的？"></a>171. MySQL 的索引怎么实现的？</h3><p>索引是满足某种特定查找算法的数据结构，而这些数据结构和以某种方式指向数据，从而实现高效查找数据</p><p>具体来说 MySQL 中的索引，不同的数据引擎实现有所不同。但目前主流的数据库引擎的索引都是 B+ 树实现的，B+ 树的搜索效率，可以达到二分法的性能，找到数据区域之后就找到了完整的数据结构了</p><h3 id="172-怎么验证买-MySQL-的索引是否满足需求？"><a href="#172-怎么验证买-MySQL-的索引是否满足需求？" class="headerlink" title="172. 怎么验证买 MySQL 的索引是否满足需求？"></a>172. 怎么验证买 MySQL 的索引是否满足需求？</h3><p>使用 explain 查看 SQL 是如何执行查询的，从而分析索引是否满足需求</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">explain</span> <span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">table</span> <span class="keyword">where</span> <span class="keyword">type</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h3 id="173-说一下数据库的事务隔离"><a href="#173-说一下数据库的事务隔离" class="headerlink" title="173. 说一下数据库的事务隔离?"></a>173. 说一下数据库的事务隔离?</h3><p>MySQL 的事务隔离是在 MySQL. ini 配置文件里添加的，在文件的最后添加：transaction-isolation = REPEATABLE-READ</p><p>可用的配置值：<strong>READ-UNCOMMITTED</strong>、<strong>READ-COMMITTED</strong>、<strong>REPEATABLE-READ</strong>、<strong>SERIALIZABLE</strong></p><ul><li>READ-UNCOMMITTED：未提交读，最低隔离级别、事务未提交前，就可被其他事务读取（会出现幻读、脏读、不可重复读）</li><li>READ-COMMITTED：提交读，一个事务提交后才能被其他事务读取到（会造成幻读、不可重复读）。</li><li>REPEATABLE-READ：可重复读，默认级别，保证多次读取同一个数据时，其值都和事务开始时候的内容是一致，禁止读取到别的事务未提交的数据（会造成幻读）</li><li>SERIALIZABLE：序列化，代价最高最可靠的隔离级别，该隔离级别能防止脏读、不可重复读、幻读</li><li>脏读 ：表示一个事务能够读取另一个事务中还未提交的数据。比如，某个事务尝试插入记录 A，此时该事务还未提交，然后另一个事务尝试读取到了记录 A</li><li>不可重复读 ：是指在一个事务内，多次读同一数据。</li><li>幻读 ：指同一个事务内多次查询返回的结果集不一样。比如同一个事务 A 第一次查询时候有 n 条记录，但是第二次同等条件下查询却有 n+1 条记录，这就好像产生了幻觉。发生幻读的原因也是另外一个事务新增或者删除或者修改了第一个事务结果集里面的数据，同一个记录的数据内容被修改了，所有数据行的记录就变多或者变少了。</li></ul><h3 id="174-说一下-MySQL-常用的引擎？"><a href="#174-说一下-MySQL-常用的引擎？" class="headerlink" title="174.说一下 MySQL 常用的引擎？"></a>174.说一下 MySQL 常用的引擎？</h3><ul><li>InnoDB 引擎：InnoDB 引擎提供了对数据库 acid 事务的支持，并且还提供了行级锁和外键的约束，它的设计的目标就是处理大数据容量的数据库系统。MySQL 运行的时候，InnoDB 会在内存中建立缓冲池，用于缓冲数据和索引。但是该引擎是不支持全文搜索，同时启动也比较的慢，它是不会保存表的行数的，所以当进行 select count(<em>) from table 指令的时候，需要进行扫描全表。由于锁的粒度小，写操作是不会锁定全表的,所以在并发度较高的场景下使用会提升效率的。</em></li><li>MyIASM 引擎：MySQL 的默认引擎，但不提供事务的支持，也不支持行级锁和外键。因此当执行插入和更新语句时，即执行写操作的时候需要锁定这个表，所以会导致效率会降低。不过和 InnoDB 不同的是，MyIASM 引擎是保存了表的行数，于是当进行 select count(*) from table 语句时，可以直接的读取已经保存的值而不需要进行扫描全表。所以，如果表的读操作远远多于写操作时，并且不需要事务的支持的，可以将 MyIASM 作为数据库引擎的首选。</li></ul><h3 id="175-说一下-MySQL-的行锁和表锁？"><a href="#175-说一下-MySQL-的行锁和表锁？" class="headerlink" title="175.说一下 MySQL 的行锁和表锁？"></a>175.说一下 MySQL 的行锁和表锁？</h3><p>MyISAM 只支持表锁，InnoDB 支持表锁和行锁，默认为行锁。</p><ul><li>表级锁：开销小，加锁快，不会出现死锁。锁定粒度大，发生锁冲突的概率最高，并发量最低。</li><li>行级锁：开销大，加锁慢，会出现死锁。锁力度小，发生锁冲突的概率小，并发度最高。</li></ul><h3 id="176-说一下乐观锁和悲观锁？"><a href="#176-说一下乐观锁和悲观锁？" class="headerlink" title="176.说一下乐观锁和悲观锁？"></a>176.说一下乐观锁和悲观锁？</h3><p>乐观锁：每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在提交更新的时候会判断一下在此期间别人有没有去更新这个数据</p><p>悲观锁：每次去拿数据的时候都认为别人会修改，所以每次拿数据的时候都会上锁，这样别人想拿这条数据就会阻止，这个锁被释放。</p><h3 id="177-MySQL-问题排查都有哪些手段？"><a href="#177-MySQL-问题排查都有哪些手段？" class="headerlink" title="177.MySQL 问题排查都有哪些手段？"></a>177.MySQL 问题排查都有哪些手段？</h3><ul><li>使用 show processlist 命令查看当前所有连接信息。</li><li>使用 explain 命令查询 SQL 语句执行计划。</li><li>开启慢查询日志，查看慢查询的 SQL。</li></ul><h3 id="178-如何做-MySQL-的性能优化？"><a href="#178-如何做-MySQL-的性能优化？" class="headerlink" title="178.如何做 MySQL 的性能优化？"></a>178.如何做 MySQL 的性能优化？</h3><ul><li>为搜索字段创建索引。</li><li>避免使用 select *，列出需要查询的字段。</li><li>垂直分割分表。</li><li>选择正确的存储引擎。</li></ul><h2 id="Redis-模块"><a href="#Redis-模块" class="headerlink" title="Redis 模块"></a>Redis 模块</h2><h3 id="179-Redis-是什么？都有哪些使用场景？"><a href="#179-Redis-是什么？都有哪些使用场景？" class="headerlink" title="179.Redis 是什么？都有哪些使用场景？"></a>179.Redis 是什么？都有哪些使用场景？</h3><p>Redis 是一个使用 C 语言开发的高速缓存数据库</p><p>Redis 使用场景：</p><ul><li>记录帖子点赞数、点击数、评论数</li><li>缓存近期热帖</li><li>缓存文章详情信息；</li><li>记录用户会话信息</li></ul><h3 id="180-Redis-有哪些功能？"><a href="#180-Redis-有哪些功能？" class="headerlink" title="180.Redis 有哪些功能？"></a>180.Redis 有哪些功能？</h3><ul><li>数据缓存功能</li><li>分布式锁的功能</li><li>支持数据持久化</li><li>支持事务</li><li>支持消息队列</li></ul><h3 id="182-Redis-为什么是单线程的？"><a href="#182-Redis-为什么是单线程的？" class="headerlink" title="182.Redis 为什么是单线程的？"></a>182.Redis 为什么是单线程的？</h3><p>因为 cpu 不是 Redis 的瓶颈，Redis 的瓶颈最有可能是机器内存或者网络带宽</p><h3 id="183-什么是缓存穿透？怎么解决？"><a href="#183-什么是缓存穿透？怎么解决？" class="headerlink" title="183.什么是缓存穿透？怎么解决？"></a>183.什么是缓存穿透？怎么解决？</h3><ul><li>缓存穿透：指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。</li><li>解决方案：最简单粗暴的方法如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们就把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。</li></ul><h3 id="184-Redis-支持的数据类型有哪些？"><a href="#184-Redis-支持的数据类型有哪些？" class="headerlink" title="184.Redis 支持的数据类型有哪些？"></a>184.Redis 支持的数据类型有哪些？</h3><p>Redis 支持的数据类型：string（字符串）、list（列表）、hash（字典）、set（集合）、zset（有序集合）</p><h3 id="185-Redis-支持的-Java-客户端都有哪些？"><a href="#185-Redis-支持的-Java-客户端都有哪些？" class="headerlink" title="185.Redis 支持的 Java 客户端都有哪些？"></a>185.Redis 支持的 Java 客户端都有哪些？</h3><p>支持的 Java 客户端有 Redisson、Jedis、lettuce 等</p><h3 id="187-怎么保证缓存和数据库数据的一致性？"><a href="#187-怎么保证缓存和数据库数据的一致性？" class="headerlink" title="187.怎么保证缓存和数据库数据的一致性？"></a>187.怎么保证缓存和数据库数据的一致性？</h3><ol><li>合理设置缓存的过期时间。</li><li>新增、更改、删除数据库操作时同步更新 Redis，可以使用事物机制来保证数据的一致性。</li></ol><h3 id="188-Redis-持久化有几种方式？"><a href="#188-Redis-持久化有几种方式？" class="headerlink" title="188.Redis 持久化有几种方式？"></a>188.Redis 持久化有几种方式？</h3><ol><li>RDB（Redis Database）：指定的时间间隔能对你的数据进行快照存储。</li><li>AOF（Append Only File）：每一个收到的写命令都通过 write 函数追加到文件中。</li></ol><h3 id="190-Redis-分布式锁有什么缺陷？"><a href="#190-Redis-分布式锁有什么缺陷？" class="headerlink" title="190.Redis 分布式锁有什么缺陷？"></a>190.Redis 分布式锁有什么缺陷？</h3><p>Redis 分布式锁不能解决超时的问题，分布式锁有一个超时时间，程序的执行如果超出了锁的超时时间就会出现问题</p><h3 id="191-Redis-如何做内存优化？"><a href="#191-Redis-如何做内存优化？" class="headerlink" title="191.Redis 如何做内存优化？"></a>191.Redis 如何做内存优化？</h3><p>尽量使用 Redis 的散列表，把相关的信息放到散列表里面存储，而不是把每个字段单独存储，这样可以有效的减少内存使用。比如将 Web 系统的用户对象，应该放到散列表里面再整体存储到 Redis，而不是把用户的姓名、年龄、密码、邮箱等字段分别设置 key 进行存储。</p><h3 id="193-Redis-常见的性能问题有哪些？该如何解决？"><a href="#193-Redis-常见的性能问题有哪些？该如何解决？" class="headerlink" title="193.Redis 常见的性能问题有哪些？该如何解决？"></a>193.Redis 常见的性能问题有哪些？该如何解决？</h3><p>主服务器写内存快照，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以主服务器最好不要写内存快照</p><p>Redis 主从复制的性能问题，为了主从复制的速度和连接的稳定性，主从库最好在同一个局域网内。</p><h2 id="JVM-模块"><a href="#JVM-模块" class="headerlink" title="JVM 模块"></a>JVM 模块</h2><h3 id="194-说一下-JVM-的主要组成部分？及其作用？"><a href="#194-说一下-JVM-的主要组成部分？及其作用？" class="headerlink" title="194.说一下 JVM 的主要组成部分？及其作用？"></a>194.说一下 JVM 的主要组成部分？及其作用？</h3><ul><li>类加载器（ClassLoader）</li><li>运行时数据区（Runtime Data Area）</li><li>执行引擎（Execution Engine）</li><li>本地库接口（Native Interface）</li><li>组件的作用：首先通过类加载器（ClassLoader）会把 Java 代码转换成字节码，运行时数据区（Runtime Data Area）再把字节码加载到内存中，而字节码文件只是 JVM 的一套指令集规范，并不能直接交个底层操作系统去执行，因此需要特定的命令解析器执行引擎（Execution Engine），将字节码翻译成底层系统指令，再交由 CPU 去执行，而这个过程中需要调用其他语言的本地库接口（Native Interface）来实现整个程序的功能。</li></ul><h3 id="195-说一下-JVM-运行时数据区？"><a href="#195-说一下-JVM-运行时数据区？" class="headerlink" title="195.说一下 JVM 运行时数据区？"></a>195.说一下 JVM 运行时数据区？</h3><ul><li><strong>程序计数器（Program Counter Register）：</strong>当前线程所执行的字节码的行号指示器，字节码解析器的工作是通过改变这个计数器的值，来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能，都需要依赖这个计数器来完成</li><li><strong>Java 虚拟机栈（Java Virtual Machine Stacks）：</strong>用于存储局部变量表、操作数栈、动态链接、方法出口等信息</li><li><strong>本地方法栈（Native Method Stack）：</strong>与虚拟机栈的作用是一样的，只不过虚拟机栈是服务 Java 方法的，而本地方法栈是为虚拟机调用 Native 方法服务的</li><li><strong>Java 堆（Java Heap）：</strong>Java 虚拟机中内存最大的一块，是被所有线程共享的，几乎所有的对象实例都在这里分配内存</li><li><strong>方法区（Methed Area）：</strong>用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据</li></ul><h3 id="196-说一下堆栈的区别？"><a href="#196-说一下堆栈的区别？" class="headerlink" title="196.说一下堆栈的区别？"></a>196.说一下堆栈的区别？</h3><ul><li>功能方面：堆是用来存放对象的，栈是用来执行程序的。</li><li>共享性：堆是线程共享的，栈是线程私有的。</li><li>空间大小：堆大小远远大于栈。</li></ul><h3 id="197-队列和栈是什么？有什么区别？"><a href="#197-队列和栈是什么？有什么区别？" class="headerlink" title="197.队列和栈是什么？有什么区别？"></a>197.队列和栈是什么？有什么区别？</h3><p>队列和栈都是被<strong>用来预存储数据的</strong></p><p>队列允许先进先出检索元素，但也有例外的情况，Deque 接口允许从两端检索元素</p><p>栈和队列很相似，但它运行对元素进行后进先出进行检索</p><h3 id="198-什么是双亲委派模型？"><a href="#198-什么是双亲委派模型？" class="headerlink" title="198.什么是双亲委派模型？"></a>198.什么是双亲委派模型？</h3><p>在介绍双亲委派模型之前先说下类加载器。对于任意一个类，都需要由加载它的类加载器和这个类本身统一确立在 JVM 中的唯一性，每一个类加载器，都有一个独立的类名称空间。类加载器就是根据指定全限定名称将 class 文件加载到 JVM 内存，然后再转化为 class 对象。</p><p>类加载器分类：</p><ul><li>启动类加载器（Bootstrap ClassLoader），是虚拟机自身的一部分，用来加载Java_HOME/lib/目录中的，或者被 -Xbootclasspath 参数所指定的路径中并且被虚拟机识别的类库；</li><li>其他类加载器：<ul><li>扩展类加载器（Extension ClassLoader）：负责加载libext目录或Java. ext. dirs系统变量指定的路径中的所有类库；</li><li>应用程序类加载器（Application ClassLoader）：负责加载用户类路径（classpath）上的指定类库，我们可以直接使用这个类加载器。一般情况，如果我们没有自定义类加载器默认就是用这个加载器</li></ul></li></ul><p><strong>双亲委派模型：</strong>如果一个类加载器收到了类加载的请求，它首先不会自己去加载这个类，而是把这个请求委派给父类加载器去完成，每一层的类加载器都是如此，这样所有的加载请求都会被传送到顶层的启动类加载器中，只有当父加载无法完成加载请求（它的搜索范围中没找到所需的类）时，子加载器才会尝试去加载类。</p><h3 id="199-说一下类装载的执行过程？"><a href="#199-说一下类装载的执行过程？" class="headerlink" title="199.说一下类装载的执行过程？"></a>199.说一下类装载的执行过程？</h3><ol><li>加载：根据查找路径找到相应的 class 文件然后导入；</li><li>检查：检查加载的 class 文件的正确性；</li><li>准备：给类中的静态变量分配内存空间；</li><li>解析：虚拟机将常量池中的符号引用替换成直接引用的过程。符号引用就理解为一个标示，而在直接引用直接指向内存中的地址；</li><li>初始化：对静态变量和静态代码块执行初始化工作；</li></ol><h3 id="200-怎么判断对象是否可以被回收？"><a href="#200-怎么判断对象是否可以被回收？" class="headerlink" title="200.怎么判断对象是否可以被回收？"></a>200.怎么判断对象是否可以被回收？</h3><ul><li>引用计数器：为每个对象创建一个引用计数，有对象引用时计数器 +1，引用被释放时计数 -1，当计数器为 0 时就可以被回收。<strong>它有一个缺点不能解决循环引用的问题</strong></li><li>可达性分析：从 GC Roots 开始向下搜索，搜索所走过的路径称为引用链。当一个对象到 GC Roots 没有任何引用链相连时，则证明此对象是可以被回收的</li></ul><h3 id="201-Java-中都有哪些引用类型？"><a href="#201-Java-中都有哪些引用类型？" class="headerlink" title="201.Java 中都有哪些引用类型？"></a>201.Java 中都有哪些引用类型？</h3><ul><li>强引用：发生 gc 的时候不会被回收。</li><li>软引用：有用但不是必须的对象，在发生内存溢出之前会被回收</li><li>弱引用：有用但不是必须的对象，在下一次GC时会被回收。</li><li>虚引用（幽灵引用/幻影引用）：无法通过虚引用获得对象，用 PhantomReference 现虚引用，虚引用的用途是在 gc 时返回一个通知</li></ul><h3 id="202-说一下-JVM-有哪些垃圾回收算法？"><a href="#202-说一下-JVM-有哪些垃圾回收算法？" class="headerlink" title="202.说一下 JVM 有哪些垃圾回收算法？"></a>202.说一下 JVM 有哪些垃圾回收算法？</h3><ul><li>标记-清除算法：标记无用对象，然后进行清除回收。缺点：效率不高，无法清除垃圾碎片</li><li>标记-整理算法：标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存</li><li>复制算法：按照容量划分二个大小相等的内存区域，当一块用完的时候将活着的对象复制到另一块上，然后再把已使用的内存空间一次清理掉。缺点：内存使用率不高，只有原来的一半</li><li>分代算法：根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代，新生代基本采用复制算法，老年代采用标记整理算法</li></ul><h3 id="203-说一下-JVM-有哪些垃圾回收器？"><a href="#203-说一下-JVM-有哪些垃圾回收器？" class="headerlink" title="203.说一下 JVM 有哪些垃圾回收器？"></a>203.说一下 JVM 有哪些垃圾回收器？</h3><ul><li>Serial：最早的单线程串行垃圾回收器。</li><li>Serial Old：Serial 垃圾回收器的老年版本，同样也是单线程的，可以作为 CMS 垃圾回收器的备选预案。</li><li>ParNew：是 Serial 的多线程版本。</li><li>Parallel 和 ParNew 收集器类似是多线程的，但 Parallel 是吞吐量优先的收集器，可以牺牲等待时间换取系统的吞吐量。</li><li>Parallel Old 是 Parallel 老生代版本，Parallel 使用的是复制的内存回收算法，Parallel Old 使用的是标记-整理的内存</li></ul><p>回收算法。</p><ul><li>CMS：一种以获得最短停顿时间为目标的收集器，非常适用 B/S 系统。</li><li>G1：一种兼顾吞吐量和停顿时间的 GC 实现，是 JDK 9 以后的默认 GC 选项。</li></ul><h3 id="205-新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？"><a href="#205-新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？" class="headerlink" title="205.新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？"></a>205.新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？</h3><p>新生代回收器：Serial、ParNew、Parallel Scavenge</p><p>老年代回收器：Serial Old、Parallel Old、CMS</p><p>整堆回收器：G1</p><p>新生代垃圾回收器一般采用的是复制算法，复制算法的优点是效率高，缺点是内存利用率低；老年代回收器一般采用的是标记-整理的算法进行垃圾回收。</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;SpringBoot / SpringCloud&lt;/li&gt;
&lt;li&gt;MyBatis 模块&lt;/li&gt;
&lt;li&gt;Kafka 模块&lt;/li&gt;
&lt;li&gt;Zookeeper 模块&lt;/li&gt;
&lt;li&gt;MySQL 模块&lt;/li&gt;
&lt;li&gt;Redis 模块&lt;/li&gt;
&lt;li&gt;JVM 模块&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;题库来源于 &lt;a href=&quot;https://www.javazhiyin.com/42272.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Java知音&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Java" scheme="https://www.chentyit.com/categories/Java/"/>
    
    
      <category term="面试" scheme="https://www.chentyit.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 整合 Kafka</title>
    <link href="https://www.chentyit.com/2019/10/02/Spark-Streaming-%E6%95%B4%E5%90%88-Kafka/"/>
    <id>https://www.chentyit.com/2019/10/02/Spark-Streaming-整合-Kafka/</id>
    <published>2019-10-02T15:25:41.000Z</published>
    <updated>2019-10-08T12:16:31.788Z</updated>
    
    <content type="html"><![CDATA[<ul><li>使用版本</li><li>Receiver-based</li><li>Direct Approach (No Receivers) 推荐</li><li>指定偏移量读取 Kafka 信息</li></ul><a id="more"></a><h2 id="使用版本"><a href="#使用版本" class="headerlink" title="使用版本"></a>使用版本</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 个人感觉 10 版本比较好用 但是下面的 API 就不一样了 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 官网上有详细文档，为了保证工程一致性，下面的 API使用 8 版本 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span></span></span><br></pre></td></tr></table></figure><h2 id="Receiver-based"><a href="#Receiver-based" class="headerlink" title="Receiver-based"></a>Receiver-based</h2><p>这种方法使用接收器来接收数据。接收器是使用 Kafka 高级消费者 API 实现的；与所有接收器一样，通过接收器从 Kafka 接收的数据存储在 Spark 执行器中，然后由 Spark Streaming 启动的作业处理数据</p><h3 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h3><ol><li>启动 ZK</li><li>启动 Kafka</li><li>创建 topic（kafka_streaming_topic_cty）</li></ol><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/10/2 18:21</span></span><br><span class="line"><span class="comment"> * @Description: SparkStreaming 对接 Kafka —— Receiver</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaReceiverWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">4</span>) &#123;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">"KafkaReceiverWordCount &lt;zkQuorum&gt; &lt;group&gt; &lt;topics&gt; &lt;numThreads&gt;"</span>)</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> <span class="type">Array</span>(zkQuorum, group, topics, numThreads) = args</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> topicMap = topics.split(<span class="string">","</span>).map((_, numThreads.toInt)).toMap</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"KafkaReceiverWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createStream(ssc, zkQuorum, group, topicMap)</span><br><span class="line">        messages map(_._2) flatMap(_.split(<span class="string">" "</span>)) map((_, <span class="number">1</span>)) reduceByKey(_ + _) print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="提交集群运行"><a href="#提交集群运行" class="headerlink" title="提交集群运行"></a>提交集群运行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.spark.KafkaReceiverWordCount \</span><br><span class="line">--master local[2] \</span><br><span class="line">--name KafkaReceiverWordCount \</span><br><span class="line"><span class="meta">#</span> 这个必须得加，在生产环境中需要用 maven 下载好 jar 包直接添加到 kafka 的 lib 里面</span><br><span class="line">--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 \</span><br><span class="line">/home/hadoop/lib/sparktrain-1.0.jar  hadoop000:2181 test kafka_streaming_topic 1</span><br></pre></td></tr></table></figure><h2 id="Direct-Approach-No-Receivers-推荐"><a href="#Direct-Approach-No-Receivers-推荐" class="headerlink" title="Direct Approach (No Receivers) 推荐"></a>Direct Approach (No Receivers) 推荐</h2><p>与基于接收器的方法（即方法1）相比，该方法具有以下优点：（官网翻译）</p><ul><li><strong>简化的并行性：</strong>无需创建多个输入Kafka流并将它们合并。使用 directStream，Spark Streaming 将创建与要使用的Kafka分区一样多的 RDD 分区，所有这些分区都将从 Kafka 并行读取数据。因此，Kafka 和 RDD 分区之间存在一对一的映射，这更易于理解和调整</li><li><strong>效率：在第一种方法中，要实现零数据丢失，需要将数据存储在预写日志中</strong>，从而进一步复制数据。这实际上是低效的，因为数据被有效地复制了两次-一次是通过 Kafka 复制，另一次是通过 “预写日志” 复制。第二种方法消除了该问题，因为没有接收器，因此不需要预写日志。<strong>只要您有足够的 Kafka 保留时间，就可以从 Kafka 中恢复信息</strong>。</li><li><strong>只执行一次精确的语义：</strong>第一种方法使用Kafka的高级 API 将偏移量存储在 Zookeeper 中。传统上，这是从 Kafka 消费数据的方式。尽管这种方法（与预写日志结合使用）可以确保零数据丢失（即至少一次语义），但在某些故障下某些记录可能会被消耗两次的可能性很小。发生这种情况是由于 Spark Streaming 可靠接收的数据与 Zookeeper 跟踪的偏移量之间存在不一致。因此，在第二种方法中，我们使用不使用 Zookeeper 的简单 Kafka API。Spark Streaming 在其检查点内跟踪偏移。这样可以消除 Spark Streaming 与 Zookeeper / Kafka 之间的不一致，因此即使出现故障，Spark Streaming 也会有效地一次接收每条记录。为了获得结果输出的一次语义，将数据保存到外部数据存储的输出操作必须是幂等的，或者是保存结果和偏移量的原子事务（请参见主程序中的输出操作的语义）有关更多信息的指南）</li></ul><h3 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/10/2 19:12</span></span><br><span class="line"><span class="comment"> * @Description: SparkStreaming 对接 Kafka —— Direct</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaDirectWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">"Usage: KafkaDirectWordCount &lt;brokers&gt; &lt;topics&gt;"</span>)</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> <span class="type">Array</span>(brokers, topics) = args</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaDirectWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> topicsSet = topics.split(<span class="string">","</span>).toSet</span><br><span class="line">        <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">"metadata.broker.list"</span> -&gt; brokers)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc, kafkaParams, topicsSet)</span><br><span class="line"></span><br><span class="line">        messages.map(_._2).flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _).print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="指定偏移量读取-Kafka-信息"><a href="#指定偏移量读取-Kafka-信息" class="headerlink" title="指定偏移量读取 Kafka 信息"></a>指定偏移量读取 Kafka 信息</h2><p><strong>代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Offset02App</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        <span class="string">"metadata.broker.list"</span> -&gt; <span class="string">"192.168.1.8:9092"</span>,</span><br><span class="line">        <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"smallest"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> topics = <span class="string">"imooc_cty_offset"</span>.split(<span class="string">", "</span>).toSet</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> checkpointDirectory = <span class="string">"E:\\test\\ck_point"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line">        <span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc, kafkaParams, topics)</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * set checkpoint directory</span></span><br><span class="line"><span class="comment">         * 将偏移量存储到外部介质中</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ssc.checkpoint(checkpointDirectory)</span><br><span class="line">        messages.checkpoint(<span class="type">Duration</span>(<span class="number">10</span> * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">        messages.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">                println(<span class="string">"慕课 CTY: "</span> + rdd.count())</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        ssc</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Offset01App"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(checkpointDirectory, functionToCreateContext _)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;使用版本&lt;/li&gt;
&lt;li&gt;Receiver-based&lt;/li&gt;
&lt;li&gt;Direct Approach (No Receivers) 推荐&lt;/li&gt;
&lt;li&gt;指定偏移量读取 Kafka 信息&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
      <category term="Kafka" scheme="https://www.chentyit.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 整合 Flume</title>
    <link href="https://www.chentyit.com/2019/10/02/Spark-Streaming-%E6%95%B4%E5%90%88-Flume/"/>
    <id>https://www.chentyit.com/2019/10/02/Spark-Streaming-整合-Flume/</id>
    <published>2019-10-02T04:18:41.000Z</published>
    <updated>2019-10-02T15:26:39.357Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Flume-style Push-based Approach</li><li>Pull-based Approach using a Custom Sink（推荐使用）</li></ul><a id="more"></a><h2 id="Flume-style-Push-based-Approach"><a href="#Flume-style-Push-based-Approach" class="headerlink" title="Flume-style Push-based Approach"></a>Flume-style Push-based Approach</h2><p><strong>基于Flume的推送的方法</strong></p><p>（管网翻译）</p><p>Flume 旨在在 Flume 代理之间推送数据。在这种方法中，Spark Streaming 本质上设置了一个接收器，该接收器充当 Flume 的 Avro 代理，Flume 可以将数据推送到该接收器。</p><ul><li>启动 Flume + Spark Streaming 应用程序时，其中一个 Spark 辅助程序必须在该计算机上运行</li><li>可以将 Flume 配置为将数据推送到该计算机上的端口</li></ul><h3 id="Flume-Agent-的编写："><a href="#Flume-Agent-的编写：" class="headerlink" title="Flume Agent 的编写："></a>Flume Agent 的编写：</h3><p><strong>flume_push_stream.conf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">simple-agent.sources = netcat-source</span><br><span class="line">simple-agent.sinks = avro-sink</span><br><span class="line">simple-agent.channels = memory-channel</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.type = netcat</span><br><span class="line">simple-agent.sources.netcat-source.bind = hadoop000</span><br><span class="line">simple-agent.sources.netcat-source.port = 44444</span><br><span class="line"></span><br><span class="line">simple-agent.sinks.avro-sink.type = avro</span><br><span class="line">simple-agent.sinks.avro-sink.hostname = hadoop000</span><br><span class="line">simple-agent.sinks.avro-sink.port = 41414</span><br><span class="line">simple-agent.sinks.avro-sink.connect-timeout = 30000</span><br><span class="line"></span><br><span class="line">simple-agent.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.channels = memory-channel</span><br><span class="line">simple-agent.sinks.avro-sink.channel = memory-channel</span><br></pre></td></tr></table></figure><p><strong>Spark Streaming 代码</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/10/2 10:18</span></span><br><span class="line"><span class="comment"> * @Description: Spark Streaming 整合 Flume —— Push</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FlumePushWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交到集群运行时需要判断参数</span></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">"Usage: FlumePushWordCount &lt;hostname&gt; &lt;port&gt;"</span>)</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> <span class="type">Array</span>(hostname, port) = args</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"FlumePushWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这里的 IP 地址是 Flume 机器的地址</span></span><br><span class="line">        <span class="comment">// 将参数传递到这来</span></span><br><span class="line">        <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createStream(ssc, hostname, port.toInt)</span><br><span class="line">        flumeStream.map(x =&gt; <span class="keyword">new</span> <span class="type">String</span>(x.event.getBody.array()).trim)</span><br><span class="line">        .flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _).print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>提交集群运行方式</strong>（也可以在本地运行调试再打包到集群中，注意下文中的<strong>踩坑</strong>）</p><ol><li><p>Spark 的 jar 包运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.spark.FlumePushWordCount \</span><br><span class="line">--master local[2] \</span><br><span class="line">--packages org.apache.spark:spark-streaming-flume_2.11:2.2.0 \</span><br><span class="line">/home/hadoop/lib/sparktrain-1.0.jar \</span><br><span class="line">hadoop000 41414</span><br></pre></td></tr></table></figure></li><li><p>启动 Flume</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name simple-agent  \</span><br><span class="line">--conf $FLUME_HOME/conf  \</span><br><span class="line">--conf-file $FLUME_HOME/conf/flume_push_stream.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li></ol><h3 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h3><ol><li>sink 的 IP 地址应该是 SparkStreaming 程序运行的那台机器的 IP 地址，如果这台机器在集群上，就要把程序打包上传集群运行（注：flume 所在的机器必须和程序运行的机器能 ping  通）</li><li>先启动 SparkStreaming 程序，再启动 flume</li></ol><h2 id="Pull-based-Approach-using-a-Custom-Sink（推荐使用）"><a href="#Pull-based-Approach-using-a-Custom-Sink（推荐使用）" class="headerlink" title="Pull-based Approach using a Custom Sink（推荐使用）"></a>Pull-based Approach using a Custom Sink（推荐使用）</h2><p><strong>基于 Pull 的方法自定义接收器</strong></p><p>这种方法不是运行 Flume 将数据直接推送到 Spark Streaming，而是运行自定义的 Flume 接收器</p><ul><li>Flume 将数据推入接收器，并且数据保持缓冲状态</li><li>Spark Streaming 使用可靠的 Flume 接收器和事务从接收器中提取数据。只有在 Spark Streaming 接收并复制了数据之后，事务才能成功</li></ul><p>与以前的方法相比，这确保了更强的可靠性和容错保证。但是，这需要将Flume配置为运行自定义接收器</p><h3 id="Flume-Agent-的编写：-1"><a href="#Flume-Agent-的编写：-1" class="headerlink" title="Flume Agent 的编写："></a>Flume Agent 的编写：</h3><p><strong>flume_pull_streaming.conf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">simple-agent.sources = netcat-source</span><br><span class="line">simple-agent.sinks = spark-sink</span><br><span class="line">simple-agent.channels = memory-channel</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.type = netcat</span><br><span class="line">simple-agent.sources.netcat-source.bind = hadoop000</span><br><span class="line">simple-agent.sources.netcat-source.port = 44444</span><br><span class="line"></span><br><span class="line">simple-agent.sinks.spark-sink.type = org.apache.spark.streaming.flume.sink.SparkSink</span><br><span class="line">simple-agent.sinks.spark-sink.hostname = hadoop000</span><br><span class="line">simple-agent.sinks.spark-sink.port = 41414</span><br><span class="line"></span><br><span class="line">simple-agent.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.channels = memory-channel</span><br><span class="line">simple-agent.sinks.spark-sink.channel = memory-channel</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent  \</span><br><span class="line">--name simple-agent   \</span><br><span class="line">--conf $FLUME_HOME/conf    \</span><br><span class="line">--conf-file $FLUME_HOME/conf/flume_pull_streaming.conf  \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p><strong>Spark Streaming 代码</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/10/2 12:10</span></span><br><span class="line"><span class="comment"> * @Description: Spark Streaming 整合 Flume - Pull</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FlumePullWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"FlumePushWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这里的 IP 地址是 Flume 机器的地址</span></span><br><span class="line">        <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createPollingStream(ssc, <span class="string">"192.168.10.120"</span>, <span class="number">41414</span>)</span><br><span class="line">        flumeStream.map(x =&gt; <span class="keyword">new</span> <span class="type">String</span>(x.event.getBody.array()).trim)</span><br><span class="line">        .flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _).print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="踩坑-1"><a href="#踩坑-1" class="headerlink" title="踩坑"></a>踩坑</h3><ol><li>先启动 FLume，再启动 SparkStreaming 程序</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Flume-style Push-based Approach&lt;/li&gt;
&lt;li&gt;Pull-based Approach using a Custom Sink（推荐使用）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flume" scheme="https://www.chentyit.com/tags/Flume/"/>
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkStreaming 进阶</title>
    <link href="https://www.chentyit.com/2019/09/28/SparkStreaming-%E8%BF%9B%E9%98%B6/"/>
    <id>https://www.chentyit.com/2019/09/28/SparkStreaming-进阶/</id>
    <published>2019-09-28T12:18:32.000Z</published>
    <updated>2019-09-28T12:20:12.737Z</updated>
    
    <content type="html"><![CDATA[<ul><li>带状态的算子（UpdateStateByKey）</li><li>基于 window 的统计</li><li>实例测试</li></ul><a id="more"></a><h2 id="带状态的算子（UpdateStateByKey）"><a href="#带状态的算子（UpdateStateByKey）" class="headerlink" title="带状态的算子（UpdateStateByKey）"></a>带状态的算子（UpdateStateByKey）</h2><p>updateStateByKey 操作使您可以保持任意状态，同时用新信息连续更新它。要使用此功能，将必须执行两个步骤：（官网内容）</p><ol><li><strong>定义状态：</strong>状态可以是任意数据类型</li><li><strong>定义状态更新功能：</strong>使用功能指定如何使用输入流中的先前状态和新值来更新状态</li></ol><h2 id="基于-window-的统计"><a href="#基于-window-的统计" class="headerlink" title="基于 window 的统计"></a>基于 window 的统计</h2><p>Spark Streaming 还提供窗口化计算，使您可以在数据的滑动窗口上应用转换</p><p><img src="http://spark.apache.org/docs/2.2.0/img/streaming-dstream-window.png" alt="window"></p><p>如图所示，每当窗口在源 DStream 上滑动时，落入窗口内的源 RDD 就会合并并进行操作，以生成窗口 DStream 的 RDD</p><p>在这种特定情况下，该操作将应用于数据的最后 3 个时间单位，并以 2 个时间单位滑动</p><p>这表明任何窗口操作都需要指定两个参数</p><ul><li><strong>窗口长度：</strong>窗口的持续时间</li><li><strong>滑动间隔：</strong>进行窗口操作的间隔</li></ul><h2 id="实例测试"><a href="#实例测试" class="headerlink" title="实例测试"></a>实例测试</h2><h3 id="1-统计到目前为止累计出现的单词的个数（需要保持住以前的状态）"><a href="#1-统计到目前为止累计出现的单词的个数（需要保持住以前的状态）" class="headerlink" title="1. 统计到目前为止累计出现的单词的个数（需要保持住以前的状态）"></a>1. 统计到目前为止累计出现的单词的个数（需要保持住以前的状态）</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/28 18:34</span></span><br><span class="line"><span class="comment"> * @Description: 使用 SparkStreaming 完成有状态统计</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StatefulWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 把当前的数据去更新已有的或者是老的数据</span></span><br><span class="line"><span class="comment">     * @param currentValues 当前的</span></span><br><span class="line"><span class="comment">     * @param preValues 老的</span></span><br><span class="line"><span class="comment">     * @return</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(currentValues: <span class="type">Seq</span>[<span class="type">Int</span>], preValues: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">        <span class="keyword">val</span> current = currentValues.sum</span><br><span class="line">        <span class="keyword">val</span> pre = preValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">        <span class="type">Some</span>(current + pre)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"StatefulWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果是用了 stateful 的算子，必须要设置 checkpoint</span></span><br><span class="line">        <span class="comment">// 在生产环境中，建议把 checkpoint 设置到 HDFS 的某个文件夹中</span></span><br><span class="line">        ssc.checkpoint(<span class="string">"."</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"192.168.1.10"</span>, <span class="number">6789</span>)</span><br><span class="line">        <span class="keyword">val</span> result = lines.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> state = result.updateStateByKey[<span class="type">Int</span>](updateFunction _)</span><br><span class="line">        state.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-计算到目前为止累计出现的单词个数写入到-MySQL"><a href="#2-计算到目前为止累计出现的单词个数写入到-MySQL" class="headerlink" title="2. 计算到目前为止累计出现的单词个数写入到 MySQL"></a>2. 计算到目前为止累计出现的单词个数写入到 MySQL</h3><ul><li>使用 Spark Streaming 进行统计分析</li><li>Spark Streaming 统计结果写入到 MySQL</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/28 18:34</span></span><br><span class="line"><span class="comment"> * @Description: 使用 SparkStreaming 完成有词频统计并将结果写入到 MySQL 数据库中</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ForeachRDDApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取 MySQL 的连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * @return</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createConnection</span></span>(): <span class="type">Connection</span> = &#123;</span><br><span class="line">        <span class="type">Class</span>.forName(<span class="string">"com.mysql.cj.jdbc.Driver"</span>)</span><br><span class="line">        <span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://localhost:3306/imoocbootscala?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai"</span>, <span class="string">"root"</span>, <span class="string">"Chentyit123456"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"StatefulWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果是用了 stateful 的算子，必须要设置 checkpoint</span></span><br><span class="line">        <span class="comment">// 在生产环境中，建议把 checkpoint 设置到 HDFS 的某个文件夹中</span></span><br><span class="line">        ssc.checkpoint(<span class="string">"."</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"192.168.1.10"</span>, <span class="number">6789</span>)</span><br><span class="line">        <span class="keyword">val</span> result = lines.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">        result.print()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 将结果写入到 MySQL</span></span><br><span class="line"><span class="comment">         * 这里的代码有两个问题：</span></span><br><span class="line"><span class="comment">         * 1. 对于已有的数据不会更新（改进方法：使用 Hbase 或者 Redis，再或者去数据库中查询，如果存在就更新，不存在就添加）</span></span><br><span class="line"><span class="comment">         * 2. 数据库连接没有使用连接池，会造成程序对资源的开销很大</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        result.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            rdd.foreachPartition(partitionOfRecords =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> connection = createConnection()</span><br><span class="line">                partitionOfRecords.foreach(recode =&gt; &#123;</span><br><span class="line">                    <span class="keyword">val</span> sql = <span class="string">"insert into wordcount(word, wordcount) values('"</span> + recode._1 + <span class="string">"',"</span> + recode._2 + <span class="string">")"</span></span><br><span class="line">                    connection.createStatement().execute(sql)</span><br><span class="line">                &#125;)</span><br><span class="line">                connection.close()</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-黑名单过滤"><a href="#3-黑名单过滤" class="headerlink" title="3. 黑名单过滤"></a>3. 黑名单过滤</h3><ul><li>transform 算子的使用</li><li>Spark Streaming 整合 RDD 进行操作</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/28 19:41</span></span><br><span class="line"><span class="comment"> * @Description: 黑名单过滤</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"TransformApp"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 构建黑名单</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">val</span> blacks = <span class="type">List</span>(<span class="string">"zs"</span>, <span class="string">"ls"</span>)</span><br><span class="line">        <span class="keyword">val</span> blacksRDD = ssc.sparkContext.parallelize(blacks).map(x =&gt; (x, <span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"192.168.1.10"</span>, <span class="number">6789</span>)</span><br><span class="line">        lines.map(x =&gt; (x.split(<span class="string">","</span>)(<span class="number">1</span>), x)).transform(rdd =&gt; &#123;</span><br><span class="line">            rdd.leftOuterJoin(blacksRDD).filter(!_._2._2.getOrElse(<span class="literal">false</span>)).map(x=&gt;x._2._1)</span><br><span class="line">        &#125;).print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-Spring-Streaming-整合-Spark-SQL"><a href="#4-Spring-Streaming-整合-Spark-SQL" class="headerlink" title="4. Spring Streaming 整合 Spark SQL"></a>4. Spring Streaming 整合 Spark SQL</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/28 20:09</span></span><br><span class="line"><span class="comment"> * @Description: Spark Streaming 整合 Spark SQL 完成词频统计操作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SqlNetworkWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"SqlNetworkWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"192.168.1.10"</span>, <span class="number">6789</span>)</span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Convert RDDs of the words DStream to DataFrame and run SQL query</span></span><br><span class="line">        words.foreachRDD &#123; (rdd: <span class="type">RDD</span>[<span class="type">String</span>], time: <span class="type">Time</span>) =&gt;</span><br><span class="line">            <span class="keyword">val</span> spark = <span class="type">SparkSessionSingleton</span>.getInstance(rdd.sparkContext.getConf)</span><br><span class="line">            <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Convert RDD[String] to RDD[case class] to DataFrame</span></span><br><span class="line">            <span class="keyword">val</span> wordsDataFrame = rdd.map(w =&gt; <span class="type">Record</span>(w)).toDF()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">            wordsDataFrame.createOrReplaceTempView(<span class="string">"words"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Do word count on table using SQL and print it</span></span><br><span class="line">            <span class="keyword">val</span> wordCountsDataFrame =</span><br><span class="line">            spark.sql(<span class="string">"select word, count(*) as total from words group by word"</span>)</span><br><span class="line">            println(<span class="string">s"========= <span class="subst">$time</span> ========="</span>)</span><br><span class="line">            wordCountsDataFrame.show()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Case class for converting RDD to DataFrame */</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">word: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">/**</span> <span class="title">Lazily</span> <span class="title">instantiated</span> <span class="title">singleton</span> <span class="title">instance</span> <span class="title">of</span> <span class="title">SparkSession</span> <span class="title">*/</span></span></span><br><span class="line"><span class="class">    <span class="title">object</span> <span class="title">SparkSessionSingleton</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 这个注解一般用于序列化的时候，标识某个字段不用被序列化</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">var</span> instance: <span class="type">SparkSession</span> = _</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">getInstance</span></span>(sparkConf: <span class="type">SparkConf</span>): <span class="type">SparkSession</span> = &#123;</span><br><span class="line">            <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">                instance = <span class="type">SparkSession</span></span><br><span class="line">                .builder</span><br><span class="line">                .config(sparkConf)</span><br><span class="line">                .getOrCreate()</span><br><span class="line">            &#125;</span><br><span class="line">            instance</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;带状态的算子（UpdateStateByKey）&lt;/li&gt;
&lt;li&gt;基于 window 的统计&lt;/li&gt;
&lt;li&gt;实例测试&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Anaconda四步安装多环境（鼠标点击安装）</title>
    <link href="https://www.chentyit.com/2019/09/28/Anaconda%E5%9B%9B%E6%AD%A5%E5%AE%89%E8%A3%85%E5%A4%9A%E7%8E%AF%E5%A2%83%EF%BC%88%E9%BC%A0%E6%A0%87%E7%82%B9%E5%87%BB%E5%AE%89%E8%A3%85%EF%BC%89/"/>
    <id>https://www.chentyit.com/2019/09/28/Anaconda四步安装多环境（鼠标点击安装）/</id>
    <published>2019-09-28T00:48:33.000Z</published>
    <updated>2019-09-28T00:56:45.347Z</updated>
    
    <content type="html"><![CDATA[<ol><li>打开 Anaconda 交互界面</li><li>点击 Environment</li><li>点击添加</li><li>点击完成</li></ol><a id="more"></a><p><img src="/2019/09/28/Anaconda四步安装多环境（鼠标点击安装）/1.png" alt="打开 Anaconda 交互界面"></p><p><img src="/2019/09/28/Anaconda四步安装多环境（鼠标点击安装）/2.png" alt="点击 Environment"></p><p><img src="/2019/09/28/Anaconda四步安装多环境（鼠标点击安装）/3.png" alt="点击添加"></p><p><img src="/2019/09/28/Anaconda四步安装多环境（鼠标点击安装）/4.png" alt="点击完成"></p>]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;打开 Anaconda 交互界面&lt;/li&gt;
&lt;li&gt;点击 Environment&lt;/li&gt;
&lt;li&gt;点击添加&lt;/li&gt;
&lt;li&gt;点击完成&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Python" scheme="https://www.chentyit.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>SparkStreaming 核心</title>
    <link href="https://www.chentyit.com/2019/09/27/SparkStreaming-%E6%A0%B8%E5%BF%83/"/>
    <id>https://www.chentyit.com/2019/09/27/SparkStreaming-核心/</id>
    <published>2019-09-27T12:05:38.000Z</published>
    <updated>2019-10-15T01:52:26.114Z</updated>
    
    <content type="html"><![CDATA[<ul><li>核心概念</li><li>代码实验</li></ul><a id="more"></a><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><p><a href="http://spark.apache.org/docs/2.2.0/streaming-programming-guide.html" target="_blank" rel="noopener">官网地址</a></p><h3 id="StreamingContext"><a href="#StreamingContext" class="headerlink" title="StreamingContext"></a>StreamingContext</h3><p>要初始化 Spark Streaming 程序，必须创建 StreamingContext 对象，该对象是所有 Spark Streaming 功能的主要入口点</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(appName).setMaster(master)</span><br><span class="line"><span class="comment">// Seconds：必须根据应用程序的延迟要求和可用的群集资源来设置批处理间隔</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p><strong>StreamingContext 源码</strong>构造方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a StreamingContext using an existing SparkContext.</span></span><br><span class="line"><span class="comment"> * @param sparkContext existing SparkContext</span></span><br><span class="line"><span class="comment"> * @param batchDuration the time interval at which streaming data will be divided into batches</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(sparkContext: <span class="type">SparkContext</span>, batchDuration: <span class="type">Duration</span>) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(sparkContext, <span class="literal">null</span>, batchDuration)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a StreamingContext by providing the configuration necessary for a new SparkContext.</span></span><br><span class="line"><span class="comment"> * @param conf a org.apache.spark.SparkConf object specifying Spark parameters</span></span><br><span class="line"><span class="comment"> * @param batchDuration the time interval at which streaming data will be divided into batches</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(conf: <span class="type">SparkConf</span>, batchDuration: <span class="type">Duration</span>) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="type">StreamingContext</span>.createNewSparkContext(conf), <span class="literal">null</span>, batchDuration)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一旦 StreamingContext 定义好后，就可以做一些事情：（官网翻译过来的）</p><ol><li>通过创建输入 DStream 定义输入源</li><li>通过将转换和输出操作应用于 DStream 来定义流计算</li><li>开始使用 streamingContext.start() 接收数据并对其进行处理</li><li>等待处理使用 streamingContext.awaitTermination() </li><li>可以使用 streamingContext.stop() 手动停止处理</li></ol><p>要记住的要点：（官网翻译过来的）</p><ul><li>一旦启动上下文，就无法设置新的流计算或将其添加到该流计算中</li><li>上下文停止后，将无法重新启动</li><li>JVM 中只能同时激活一个 StreamingContext</li><li>StreamingContext 上的 stop() 也会停止 SparkContext，如要仅停止 StreamingContext，请将名为 stopSparkContext 的 stop() 的可选参数设置为 false</li><li>只要在创建下一个 StreamingContext 之前停止了上一个 StreamingContext（不停止 SparkContext），就可以重复使用 SparkContext 创建多个 StreamingContext</li></ul><h3 id="DStream（Discretized-Streams）"><a href="#DStream（Discretized-Streams）" class="headerlink" title="DStream（Discretized Streams）"></a>DStream（Discretized Streams）</h3><p>在内部，DStream 由一系列连续的 RDD 表示，这是 Spark 对不可变的分布式数据集的抽象</p><p>DStream中的每个RDD都包含来自特定间隔的数据，如下图所示</p><p><img src="http://spark.apache.org/docs/2.2.0/img/streaming-dstream.png" alt="DStream"></p><p>对 DStream 操作算子，比如 map / flatMap，其实底层会被翻译为对 DStream 中的每个 RDD 都做相同的工作，因为一个 DStream 是由不同批次的 RDD 所构成的</p><p><img src="http://spark.apache.org/docs/2.2.0/img/streaming-dstream-ops.png" alt="操作算子"></p><h3 id="Input-DStream"><a href="#Input-DStream" class="headerlink" title="Input DStream"></a>Input DStream</h3><p>Input DStream 是表示从流源接收的输入数据流的 DStream</p><p>每个输入 DStream （文件流除外） 都与一个Receiver对象（Scala doc，Java doc）相关联，该对象从源接收数据并将其存储在 Spark 的内存中以进行处理</p><p>在本地运行Spark Streaming程序时，请勿使用 “local” 或 “local [1]” 作为主URL。这两种方式均意味着仅一个线程将用于本地运行任务。如果您使用基于接收方的输入DStream（例如套接字，Kafka，Flume等），则将使用单个线程来运行接收方，而不会留下任何线程来处理接收到的数据。因此，在本地运行时，请始终使用 “local [n]” 作为主 URL，其中 n &gt; 要运行的接收器数</p><h2 id="代码实验"><a href="#代码实验" class="headerlink" title="代码实验"></a>代码实验</h2><ul><li><p>Spark Streaming 处理 socket 数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/27 19:32</span></span><br><span class="line"><span class="comment"> * @Description: Spark Streaming 处理 Socket 数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建 StreamingContext 需要两个参数：SparkConf 和 batch interval</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"192.168.1.10"</span>, <span class="number">6789</span>)</span><br><span class="line">        <span class="keyword">val</span> result = lines.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        result.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Spark Streaming 处理 HDFS 文件数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/27 19:46</span></span><br><span class="line"><span class="comment"> * @Description: 使用 Spark Streaming 处理文件系统的数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FileWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"FileWordCount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> line = ssc.textFileStream(<span class="string">"E:\\test\\"</span>)</span><br><span class="line">    <span class="keyword">val</span> result = line.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;核心概念&lt;/li&gt;
&lt;li&gt;代码实验&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkStreaming入门</title>
    <link href="https://www.chentyit.com/2019/09/26/SparkStreaming%E5%85%A5%E9%97%A8/"/>
    <id>https://www.chentyit.com/2019/09/26/SparkStreaming入门/</id>
    <published>2019-09-26T12:01:16.000Z</published>
    <updated>2019-09-26T12:08:26.547Z</updated>
    
    <content type="html"><![CDATA[<ul><li>概述</li><li>应用场景</li><li>案例测试</li><li>工作原理</li></ul><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>将不同的数据源的数据经过 Spark Streaming 处理之后将结果输出到外部文件系统</p><p><strong>特点：</strong></p><ul><li>低延迟</li><li>能从错误中高效地恢复</li><li>能够运行在成百上千的节点</li><li>能够将批处理，机器学习，图计算等子框架和 Spark Streaming 综合起来使用</li></ul><p><img src="http://spark.apache.org/docs/2.2.0/img/streaming-flow.png" alt="streaming-flow"></p><p>输入进来的数据会被 Spark Streaming 处理成为 “批次”，然后由 Spark 引擎继续处理得到最终的数据流</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul><li>实时金融欺诈检测</li><li>实时访问电子传感器的检测</li><li>电商行业信息推荐</li></ul><h2 id="SparkStreaming-例子测试"><a href="#SparkStreaming-例子测试" class="headerlink" title="SparkStreaming 例子测试"></a>SparkStreaming 例子测试</h2><ol><li><p>spark-submit 提交（词频分析）</p><p>启动命令（监听 hadoop000 的 9999 端口）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master local[2] --class org.apache.spark.examples.streaming.NetworkWordCount --name NetworkWordCount /home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.2.0.jar hadoop000 9999</span><br></pre></td></tr></table></figure><p>官方测试代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">"Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;"</span>)</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamingExamples</span>.setStreamingLogLevels()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(args(<span class="number">0</span>), args(<span class="number">1</span>).toInt, <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        wordCounts.print()</span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>spark-shell 提交（词频分析）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop000"</span>, <span class="number">9999</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">wordCounts.print()</span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure></li></ol><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><h3 id="粗粒度"><a href="#粗粒度" class="headerlink" title="粗粒度"></a>粗粒度</h3><p>Spark Streaming 接收到实时数据流，把数据按照指定的时间段切成一片片小的数据块，然后把小的数据块传给 Spark Engine 处理</p><h3 id="细粒度"><a href="#细粒度" class="headerlink" title="细粒度"></a>细粒度</h3><p><img src="/2019/09/26/SparkStreaming入门/细粒度.png" alt="细粒度"></p><ol><li>Spark 应用程序运行在 Driver 端，应用程序中有 StreamingContext 和 SparkContext</li><li>Driver 命令在 Executor 上启动接收器</li><li>接收器启动后，将收到的数据拆分成 block 并存放到内存中，如果设置多副本就拷贝到其他机器中</li><li>Receiver 将 block 的信息返回给 StreamingContext，一定时间周期后，通知 SparkContext 启动 Jobs</li><li>SparkContext 将 Jobs 分发到 Executor 上执行</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;概述&lt;/li&gt;
&lt;li&gt;应用场景&lt;/li&gt;
&lt;li&gt;案例测试&lt;/li&gt;
&lt;li&gt;工作原理&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
  </entry>
  
</feed>
