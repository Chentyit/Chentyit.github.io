<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>翼叶知秋</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.chentyit.com/"/>
  <updated>2019-10-10T07:19:39.516Z</updated>
  <id>https://www.chentyit.com/</id>
  
  <author>
    <name>Chen Tianyi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Yarn 概念</title>
    <link href="https://www.chentyit.com/2019/10/10/Yarn-%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.chentyit.com/2019/10/10/Yarn-概念/</id>
    <published>2019-10-10T06:48:36.000Z</published>
    <updated>2019-10-10T07:19:39.516Z</updated>
    
    <content type="html"><![CDATA[<ul><li>简介</li><li>重要概念</li><li>执行过程</li><li>YARN的高可用</li></ul><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Yarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序</p><h2 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h2><ol><li>yarn 并不清楚用户提交的程序的运行机制</li><li>yarn 只提供运算资源的调度（用户向 yarn 申请资源，yarn 就负责分配资源）</li><li>yarn 中的主管角色叫 ResourceManager</li><li>yarn 中具体提供运算资源的角色叫 NodeManager</li><li>yarn 于运行用户程序完全解耦，意味着 yarn 上可以运行各种类型的分布式运算程序（MapReduce 只是其中一种）</li><li>spark 等运算框架都可以整合在 yarn 上运行，只要他们各自的框架中有符合 yarn 规范的资源请求机制即可</li></ol><h2 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h2><ol><li>客户端通过 YarnRunner 向 RM 提交 job 任务。申请运行一个 MR 程序，RM 返回一个 job id，资源提交路径</li><li>客户端提交 MR 相关的资源文件：job.xml，job.jar，job.split，job.splitmateinfo</li><li>客户端通知 RM 资源提交完毕，RM 初始化任务创建一个 Container，RM 随机在一台 NM 上启动一个 MRAppMaster，MRAppMaster 向 RM 申请资源分配容器（CPU，RAM，job 等资源）</li><li>在 NM 上启动 MapTask，Task 在执行的时候会向 MRAppMaster 汇报进度和状态，MRAppMaster 会向 RM 注册，用户可以通过 RM 查看当前作业的状态</li><li>MRAppMaster 会向 RM 为各个任务申请资源，并监控状态直到任务完成</li><li>MRAppMaster 等待所有 MapTask 执行完毕，再启动 ReduceTask</li><li>所有任务完成后，MRAppMaster 通知 RM 回收资源</li></ol><p><img src="/2019/10/10/Yarn-概念/yarn.png" alt="yarn"></p><h2 id="YARN的高可用"><a href="#YARN的高可用" class="headerlink" title="YARN的高可用"></a>YARN的高可用</h2><p><strong>ResourceManager：</strong>基于 Zookeeper 实现高可用机制，避免单节点故障</p><p><strong>NodeManager：</strong>执行失败后，ResourceManager 将失败任务告诉对应的 ApplicationMaster，由 ApplicationMaster 来决定如何处理失败的任务</p><p><strong>ApplicationMaster：</strong>执行失败后，由 ResourceManager 负责重启；ApplicationMaster 需处理内部的容错问题，并保存已经运行完成的 Task，重启后无需重新运行</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;简介&lt;/li&gt;
&lt;li&gt;重要概念&lt;/li&gt;
&lt;li&gt;执行过程&lt;/li&gt;
&lt;li&gt;YARN的高可用&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Yarn" scheme="https://www.chentyit.com/tags/Yarn/"/>
    
  </entry>
  
  <entry>
    <title>MR 概念</title>
    <link href="https://www.chentyit.com/2019/10/10/MR-%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.chentyit.com/2019/10/10/MR-概念/</id>
    <published>2019-10-10T03:53:45.000Z</published>
    <updated>2019-10-10T07:53:21.596Z</updated>
    
    <content type="html"><![CDATA[<ul><li>简介</li><li>MR 程序组成部分</li><li>MapTask 并行度</li><li>ReduceTask 的并行度</li><li>Shuffle 机制</li><li>文件太小如何处理</li><li>自定义分区</li><li>二次排序</li></ul><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>MapReduce 是一个分布式运算程序的<strong>编程框架</strong>，是用户开发<strong>基于 hadoop 的数据分析应用</strong>的核心框架</p><p>MapReduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 hadoop 集群上</p><h2 id="MR-程序组成部分"><a href="#MR-程序组成部分" class="headerlink" title="MR 程序组成部分"></a>MR 程序组成部分</h2><ul><li><p>Split</p><p>将程序输入的数据进行切分，每个 split 交给一个 MapTask 执行。split 的数量可以自己定义，默认情况下一个文件一个 split</p></li><li><p>Map</p><p>输入 为一个 split 中的数据，对 split 中的数据进行拆分，并以&lt;key, value&gt; 对的格式保存数据</p></li><li><p>Shuffle / Combine / sort</p><p>这几个过程在简单的 MR 程序中并不需要我们关注，因为源代码中已经给出了一些默认的 Shuffle / Combine / sort 处理器，作用分别是：</p><ul><li>Combine：对 MapTask 产生的结果在本地节点上进行合并、统计等，以减少后续整个集群间的 Shuffle 过程所需要传输的数据量</li><li>Shuffle / Sort：将集群中各个 MapTask 的处理结果在集群间进行传输，排序，数据经过这个阶段之后就作为 Reduce 端的输入</li></ul></li><li><p>Reduce</p><p>ReduceTask 的输入数据是经过排序之后的一系列 key 值相同的 &lt;key, value&gt; 对，ReduceTask 对其进行统计等处理，产生最终的输出。ReduceTask 的数量可以设置</p></li></ul><h2 id="MapTask-并行度"><a href="#MapTask-并行度" class="headerlink" title="MapTask 并行度"></a>MapTask 并行度</h2><p>选择并发数的影响因素：</p><ol><li>运算节点的硬件配置</li><li>运算任务的类型：CPU 密集型还是 IO 密集型</li><li>运算任务的数据量</li></ol><h3 id="Task-并行度的经验"><a href="#Task-并行度的经验" class="headerlink" title="Task 并行度的经验"></a>Task 并行度的经验</h3><ul><li>最好每个 task 执行的时间至少一分钟</li><li>如果 job 的每个 map 或者 reducetask 运行时间比较短，那就应该减少 job 的 map 或者 reduce 的数量，因为每个 task 的 setup 和加入到调度器需要消耗一定的时间，如果每个 task 都花不了太多时间，就没必要有太多的 task</li><li>默认情况下，每个 task 都是一个 jvm 实例，都需要开启和销毁，jvm 的开启和销毁所需要的时间比执行的时间要长，所以配置 jvm 的可重用性可以改善性能</li><li>mapred.job.reuse.jvm.num.tasks，默认是 1，表示一个 JVM 上最可以顺序执行的 task 数目是 1，也就是说一个 task 启动一个 JVM</li><li>如果 input 的文件非常大，可以考虑将 hdfs 上的每个 blocksize 设置大一些，比如 256MB 或者 512MB</li><li>JVM 重用技术不是指同一个 job 的多个 task 可以同时运行于同一个 JVM 上，而是排队按顺序执行</li></ul><h2 id="ReduceTask-的并行度"><a href="#ReduceTask-的并行度" class="headerlink" title="ReduceTask 的并行度"></a>ReduceTask 的并行度</h2><p>maptask 的并发数由切片数决定不同，reducetask 数量是可以手动设置的</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认为 1，手动设置为 4</span></span><br><span class="line">job.setNumReduceTask(<span class="number">4</span>);</span><br></pre></td></tr></table></figure><p>如果数据分布不均匀，就有可能在 reduce 阶段产生数据倾斜</p><p><strong>注意：</strong>ReduceTask 数量并不是任意设置的，需要考虑业务逻辑需求，有些情况需要计算全局汇总结果，就只能有一个 reducetask</p><p>尽量不要运行太多的 reducetask。最好 reduce 数量和集群中的 reduce 持平或者比集群中的 reduce slots 小</p><h2 id="Shuffle-机制"><a href="#Shuffle-机制" class="headerlink" title="Shuffle 机制"></a>Shuffle 机制</h2><p><img src="/2019/10/10/MR-概念/shuffer.png" alt="shuffle"></p><p><strong>wordcount 的 shuffle 详细过程：</strong></p><ol><li><strong>读取数据：</strong>MR 默认使用 TextInputFormat 来获取切片的数量，通过 createRecordReader 方法获取 RecordReader，缺省的 RecordReader 是 LineRecordReader，通过调用 LineRecordReader 的 nextKeyValue 方法获取每行的数据</li><li><strong>溢出：</strong>通过 OutputCollector 收集器手机读取的数据，缺省使用的是 Task.CombineOutputCollector，调用其 collect 进行溢出，收集器默认的空间是 100 M，当收集器达到 80% 的时候开始溢出</li><li><strong>分区排序：</strong>mapreduce 默认通过 HashPartitioner 进行分区且有序（在内存中结束）</li><li><strong>输出文件：</strong>maptask 的最终输出文件分区有序</li><li><strong>Reduce拉取文件：</strong>从各个分区中拉取相同的 key 到 reducetask 中，合并归并排序，相同的 key 看作一个 group</li><li><strong>写出数据：</strong>FileOutputFormat 调用 RecordWriter 写出文件，相同的 key 会写到一个分区</li></ol><h2 id="文件太小如何处理"><a href="#文件太小如何处理" class="headerlink" title="文件太小如何处理"></a>文件太小如何处理</h2><p><a href="https://blog.csdn.net/zgc625238677/article/details/51793259" target="_blank" rel="noopener">小文件处理方法</a></p><h2 id="自定义分区"><a href="#自定义分区" class="headerlink" title="自定义分区"></a>自定义分区</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomizeParitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;String, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        map.put(<span class="string">"139"</span>, <span class="number">0</span>);</span><br><span class="line">        map.put(<span class="string">"186"</span>, <span class="number">1</span>);</span><br><span class="line">        map.put(<span class="string">"187"</span>, <span class="number">2</span>);</span><br><span class="line">        map.put(<span class="string">"136"</span>, <span class="number">3</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text key, NullWritable value, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        String key_ = key.toString().subString(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">        <span class="keyword">return</span> map.get(key_) != <span class="keyword">null</span> ? map.get(key_) : <span class="number">4</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="二次排序"><a href="#二次排序" class="headerlink" title="二次排序"></a>二次排序</h2><p>在 hadoop 中一般都是按照 key 进行排序的，但有时候还需要按照 value 进行排序</p><p>有两种方法：buffer and int memory sort 和 value-to-key conversion</p><ul><li>buffer and int memory sort：主要是在 reduce() 函数中，将每个 key 对应的 value 值保存下来，进行排序。缺点是会造成 out of memory</li><li>value-to-key conversion：主要思想是将 key 和 value 拼接成一个组合 key，然后进行排序，这样 reduce() 函数获取结果就实现了按照 key 排序，然后按照 value 排序，但是需要用户自己实现 paritioner，以便只按照 key 进行数据划分</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;简介&lt;/li&gt;
&lt;li&gt;MR 程序组成部分&lt;/li&gt;
&lt;li&gt;MapTask 并行度&lt;/li&gt;
&lt;li&gt;ReduceTask 的并行度&lt;/li&gt;
&lt;li&gt;Shuffle 机制&lt;/li&gt;
&lt;li&gt;文件太小如何处理&lt;/li&gt;
&lt;li&gt;自定义分区&lt;/li&gt;
&lt;li&gt;二次排序&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="MapReduce" scheme="https://www.chentyit.com/tags/MapReduce/"/>
    
  </entry>
  
  <entry>
    <title>HDFS 概念</title>
    <link href="https://www.chentyit.com/2019/10/09/HDFS-%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.chentyit.com/2019/10/09/HDFS-概念/</id>
    <published>2019-10-09T10:26:21.000Z</published>
    <updated>2019-10-09T10:39:15.883Z</updated>
    
    <content type="html"><![CDATA[<ul><li>简介</li><li>重要特性</li><li>HDFS 角色说明</li><li>HDFS 高可用机制</li><li>读数据流程</li><li>写数据流程</li></ul><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h3><p>分而治之：将大文件大批量文件，分布式存放在大量服务器上，以便于采取分而治之的方式对海量数据进行运算分析</p><h3 id="重点概念"><a href="#重点概念" class="headerlink" title="重点概念"></a>重点概念</h3><p>文件切块、副本存放、元数据</p><h2 id="重要特性"><a href="#重要特性" class="headerlink" title="重要特性"></a>重要特性</h2><ol><li><p>HDFS 中的文件在物理上是<strong>分块存储</strong>，块的大小可以通过配置参数 dfs.blocksize 来规定，默认大小在 hadoop2.x 版本中是 128M</p></li><li><p>HDFS 文件系统会给客户端提供一个<strong>统一的抽象目录树</strong>，客户端通过路径来访问文件</p></li><li><p>目录结构及文件分块信息（元数据）的管理由 namenode 节点承担</p><p>namenode 是 HDFS 集群主节点，负责维护整个 HDFS 文件系统的目录树，以及每一个路径（文件）所对应的 block 块信息（block 的 id，及所在的 datanode 服务器）</p></li><li><p>文件的各个 block 的存储管理由 datanode 节点承担</p><p>datanode 是 HDFS 集群从节点，每个 block 都可以在多个 datanode 上存储多个副本（可以通过 dfs.replication 设置）</p></li><li><p>HDFS 是设计成适应一次写入，多次读出的场景，且不支持文件的修改</p></li></ol><h2 id="HDFS-角色说明"><a href="#HDFS-角色说明" class="headerlink" title="HDFS 角色说明"></a>HDFS 角色说明</h2><table><thead><tr><th>名称</th><th>作用</th></tr></thead><tbody><tr><td>namenode</td><td>接受客户端的读写请求<br>存储元数据信息<br>接收 datanode 心跳报告<br>负载均衡<br>分配数据块的存储节点</td></tr><tr><td>datanode</td><td>真正处理客户端的读写请求<br>向 namenode 发送心跳<br>向 namenode 发送块报告<br>真正存储数据<br>副本之间的相互复制</td></tr><tr><td>journalnode</td><td>两个 namenode 为了数据同步，会通过一组称作 journalnode 的独立进程相互通信<br>当 active 状态的 namenode 的命名空间有任何修改时，会告知大部分的 journalnode 进程</td></tr><tr><td>客户端</td><td>进行数据块的物理切分<br>向 namenode 发送读写请求<br>向 namenode发送读写响应</td></tr></tbody></table><h2 id="HDFS-高可用机制"><a href="#HDFS-高可用机制" class="headerlink" title="HDFS 高可用机制"></a>HDFS 高可用机制</h2><p><a href="https://blog.csdn.net/u012736748/article/details/79534019" target="_blank" rel="noopener">HDFS的高可用机制详解（journalnode 及 editlog）</a></p><p><a href="https://blog.csdn.net/u012736748/article/details/79541311" target="_blank" rel="noopener">HDFS高可用（HA）之ZKFC详解</a></p><h2 id="读数据流程"><a href="#读数据流程" class="headerlink" title="读数据流程"></a>读数据流程</h2><ol><li><p>客户端通过调用 FileSyste 对象 DistributedFileSystem（以下简称 DFS） 的 open() 方法带打开希望读取的文件</p></li><li><p>DFS 对象通过远程调用（RPC）来调用 namenode，以确定文件起始块的位置</p><p>namenode 返回存储该数据块副本的 datanod 的地址，datanode 也会根据与客户端的距离来排序（就近原则读取信息）</p></li><li><p>DFS 类返回一个 FSDataInputStream 对象给客户端用于读取数据，FSDataInputStream 封装 DFSInputStream 对象，该对象管理 datanode 和 namenode 的 I/O</p></li><li><p>客户端调用 read() 方法</p></li><li><p>DFSInputStream 连接地址最近的一个 datanode，然后反复调用 read() 方法，将数据从 datanode 中传输到客户端。</p></li><li><p>读到块末端后，DFSInputStream 关闭连接，并开始寻找下一个最佳的 datanode，执行 4-5-6 步直至读取完成</p></li><li><p>读取完成后，就调用 close() 方法关闭 FSDataInputStream</p></li></ol><p><img src="/2019/10/09/HDFS-概念/读数据.png" alt="读数据"></p><h2 id="写数据流程"><a href="#写数据流程" class="headerlink" title="写数据流程"></a>写数据流程</h2><ol><li><p>客户端对 DistributedFileSystem（以下简称 DFS） 对象调用 create() 方法</p></li><li><p>DFS 对 namenode 创建一个 RPC 调用，在文件系统命名空间中创建一个文件，但没有对应的数据块</p><p>namenode 检查文件系统中是否存在这个文件，若不存在且客户端有权限创建，则创建一条记录，反之抛出异常</p><p>DFS 向客户端返回一个 FSDataOutputStream 对象，FSDataOutputStream 封装了 DFSOutputStream 对象</p></li><li><p>客户端调用 write 写数据</p></li><li><p>DFSOutputStream 将客户端的数据分包写入内部队列，称为<strong>“数据队列”</strong></p><p>队列的作用是选择一组合适的 datanode，要求 namenode 分配新的数据块，按照顺序发送数据到 datanode 中</p></li><li><p>DFSOutputStream 同时维护着一个<strong>“确认队列”</strong>，所有 datanode 确认信息后，数据包才会从确认队列中删除</p></li><li><p>客户端完成数据写入后调用 close() 方法</p></li><li><p>联系 namenode 写入完成，等待确认</p></li></ol><p><img src="/2019/10/09/HDFS-概念/写数据.png" alt="写数据"></p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;简介&lt;/li&gt;
&lt;li&gt;重要特性&lt;/li&gt;
&lt;li&gt;HDFS 角色说明&lt;/li&gt;
&lt;li&gt;HDFS 高可用机制&lt;/li&gt;
&lt;li&gt;读数据流程&lt;/li&gt;
&lt;li&gt;写数据流程&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="HDFS" scheme="https://www.chentyit.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 整合 Flume &amp; Kafka</title>
    <link href="https://www.chentyit.com/2019/10/05/Spark-Streaming-%E6%95%B4%E5%90%88-Flume-Kafka/"/>
    <id>https://www.chentyit.com/2019/10/05/Spark-Streaming-整合-Flume-Kafka/</id>
    <published>2019-10-05T08:58:21.000Z</published>
    <updated>2019-10-05T09:01:27.383Z</updated>
    
    <content type="html"><![CDATA[<ul><li>大致流程</li><li>使用代码生成 Log4j 日志文件</li><li>Flume 配置文件</li></ul><a id="more"></a><h2 id="大致流程"><a href="#大致流程" class="headerlink" title="大致流程"></a>大致流程</h2><p><img src="/2019/10/05/Spark-Streaming-整合-Flume-Kafka/Spark Streaming 整合 Flume &amp; Kafka.png" alt="大致流程"></p><h2 id="使用代码生成-Log4j-日志文件"><a href="#使用代码生成-Log4j-日志文件" class="headerlink" title="使用代码生成 Log4j 日志文件"></a>使用代码生成 Log4j 日志文件</h2><p>在 log4j 的配置文件中指定将日志文件发送到 flume</p><p><strong>Java 代码</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LoggerGenerator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Logger logger = Logger.getLogger(LoggerGenerator.class.getName());</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            Thread.sleep(<span class="number">1000</span>);</span><br><span class="line">            logger.info(<span class="string">"value : "</span> + index++);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>log4j.properties</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">log4j.rootLogger=INFO,stdout,flume</span><br><span class="line"></span><br><span class="line">log4j.appender.stdout = org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.target = System.out</span><br><span class="line">log4j.appender.stdout.layout=org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%t] [%c] [%p] - %m%n</span><br><span class="line"></span><br><span class="line">log4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender</span><br><span class="line">log4j.appender.flume.Hostname = 192.168.10.114</span><br><span class="line">log4j.appender.flume.Port = 41414</span><br><span class="line">log4j.appender.flume.UnsafeMode = true</span><br></pre></td></tr></table></figure><h2 id="Flume-配置文件"><a href="#Flume-配置文件" class="headerlink" title="Flume 配置文件"></a>Flume 配置文件</h2><p><strong>streaming.conf</strong>（用来测试 log4j -&gt; flume）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">agent1.sources=avro-source</span><br><span class="line">agent1.channels=logger-channel</span><br><span class="line">agent1.sinks=log-sink</span><br><span class="line"></span><br><span class="line">#define source</span><br><span class="line">agent1.sources.avro-source.type=avro</span><br><span class="line">agent1.sources.avro-source.bind=hadoop000</span><br><span class="line">agent1.sources.avro-source.port=41414</span><br><span class="line"></span><br><span class="line">#define channel</span><br><span class="line">agent1.channels.logger-channel.type=memory</span><br><span class="line"></span><br><span class="line">#define sink</span><br><span class="line">agent1.sinks.log-sink.type=logger</span><br><span class="line"></span><br><span class="line">agent1.sources.avro-source.channels=logger-channel</span><br><span class="line">agent1.sinks.log-sink.channel=logger-channel</span><br></pre></td></tr></table></figure><p><strong>streaming2.conf</strong>（log4j -&gt; flume -&gt; kafka)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">agent1.sources=avro-source</span><br><span class="line">agent1.channels=logger-channel</span><br><span class="line">agent1.sinks=kafka-sink</span><br><span class="line"></span><br><span class="line">#define source</span><br><span class="line">agent1.sources.avro-source.type=avro</span><br><span class="line">agent1.sources.avro-source.bind=hadoop000</span><br><span class="line">agent1.sources.avro-source.port=41414</span><br><span class="line"></span><br><span class="line">#define channel</span><br><span class="line">agent1.channels.logger-channel.type=memory</span><br><span class="line"></span><br><span class="line">#define sink</span><br><span class="line">agent1.sinks.kafka-sink.type=org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">agent1.sinks.kafka-sink.topic = streamingtopic_cty</span><br><span class="line">agent1.sinks.kafka-sink.brokerList = hadoop000:9092</span><br><span class="line">agent1.sinks.kafka-sink.requiredAcks = 1</span><br><span class="line"># 到达 20 条数据才 sink</span><br><span class="line">agent1.sinks.kafka-sink.batchSize = 20</span><br><span class="line"></span><br><span class="line">agent1.sources.avro-source.channels=logger-channel</span><br><span class="line">agent1.sinks.kafka-sink.channel=logger-channel</span><br></pre></td></tr></table></figure><h2 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h2><p><strong>KafkaStreamingApp.scala</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** 输入参数 192.168.10.114:9092 streamingtopic_cty */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaStreamingApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: KafkaStreamingApp &lt;brokers&gt; &lt;topics&gt;"</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> <span class="type">Array</span>(brokers, topics) = args</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaStreamingApp"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topicsSet = topics.split(<span class="string">","</span>).toSet</span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">"metadata.broker.list"</span> -&gt; brokers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc, kafkaParams, topicsSet)</span><br><span class="line"></span><br><span class="line">    messages.map(_._2).count().print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;大致流程&lt;/li&gt;
&lt;li&gt;使用代码生成 Log4j 日志文件&lt;/li&gt;
&lt;li&gt;Flume 配置文件&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flume" scheme="https://www.chentyit.com/tags/Flume/"/>
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
      <category term="Kafka" scheme="https://www.chentyit.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Java面试题03</title>
    <link href="https://www.chentyit.com/2019/10/03/Java%E9%9D%A2%E8%AF%95%E9%A2%9803/"/>
    <id>https://www.chentyit.com/2019/10/03/Java面试题03/</id>
    <published>2019-10-03T02:06:39.000Z</published>
    <updated>2019-10-03T10:41:01.557Z</updated>
    
    <content type="html"><![CDATA[<ul><li>SpringBoot / SpringCloud</li><li>MyBatis 模块</li><li>Kafka 模块</li><li>Zookeeper 模块</li><li>MySQL 模块</li><li>Redis 模块</li><li>JVM 模块</li></ul><p>题库来源于 <a href="https://www.javazhiyin.com/42272.html" target="_blank" rel="noopener">Java知音</a></p><a id="more"></a><h2 id="SpringBoot-SpringCloud"><a href="#SpringBoot-SpringCloud" class="headerlink" title="SpringBoot / SpringCloud"></a>SpringBoot / SpringCloud</h2><h3 id="104-什么是-springboot？"><a href="#104-什么是-springboot？" class="headerlink" title="104. 什么是 springboot？"></a>104. 什么是 springboot？</h3><p>springboot 是为 spring 服务的，是用来简化新 spring 应用的初始搭建一斤开发过程的</p><h3 id="105-为什么要用-springboot？"><a href="#105-为什么要用-springboot？" class="headerlink" title="105. 为什么要用 springboot？"></a>105. 为什么要用 springboot？</h3><ul><li>配置简单</li><li>独立运行</li><li>自动装配</li><li>无代码生成和 xml 配置</li><li>提供应用监控</li><li>易上手</li><li>提升开发效率</li></ul><h3 id="106-springboot-核心配置文件是什么？"><a href="#106-springboot-核心配置文件是什么？" class="headerlink" title="106. springboot 核心配置文件是什么？"></a>106. springboot 核心配置文件是什么？</h3><ul><li>bootstrap（.yml 或者 .properties）：bootstrap 有父 ApplicationContext 加载的，比 application 优先加载，且 bootstrap 里面的属性不能被覆盖</li><li>application（.yml 获取 .properties）：用于 springboot 项目的自动化配置</li></ul><h3 id="107-springboot-配置文件有哪几种类型？他们有什么区别？"><a href="#107-springboot-配置文件有哪几种类型？他们有什么区别？" class="headerlink" title="107. springboot 配置文件有哪几种类型？他们有什么区别？"></a>107. springboot 配置文件有哪几种类型？他们有什么区别？</h3><p>配置文件有 .properties 和 .yml 格式，主要区别是熟悉风格不同</p><p>yml 格式不支持 @PropertySource 注解导入</p><h3 id="108-springboot-有哪些方式可以实现热部署？"><a href="#108-springboot-有哪些方式可以实现热部署？" class="headerlink" title="108. springboot 有哪些方式可以实现热部署？"></a>108. springboot 有哪些方式可以实现热部署？</h3><p>使用 devtools 启动热部署，添加 devtools 库，在配置文件中把 spring.devtools.restart.enable 设置为 true</p><p>使用 Intellij Idea 编译器，勾选上自动编译或手动重新编译</p><h3 id="109-JPA-全称-Java-Persistence-API，是-Java-持久化接口规范，hibernate-属于-jpa-的具体实现"><a href="#109-JPA-全称-Java-Persistence-API，是-Java-持久化接口规范，hibernate-属于-jpa-的具体实现" class="headerlink" title="109. JPA 全称 Java Persistence API，是 Java 持久化接口规范，hibernate 属于 jpa 的具体实现"></a>109. JPA 全称 Java Persistence API，是 Java 持久化接口规范，hibernate 属于 jpa 的具体实现</h3><h3 id="110-什么是-springcloud？"><a href="#110-什么是-springcloud？" class="headerlink" title="110. 什么是 springcloud？"></a>110. 什么是 springcloud？</h3><p>springcloud 是一系列框架的有序集合，它利用 springboot 的开发便利性，简化了分布式系统基础设施的开发，如服务发现注册，配置中心，消息总线，负载均衡、断路器、数据监控等，都可以用 springboot 的开发风格做到意见启动和部署</p><h3 id="111-springcloud-阻断器的作用是什么？"><a href="#111-springcloud-阻断器的作用是什么？" class="headerlink" title="111. springcloud 阻断器的作用是什么？"></a>111. springcloud 阻断器的作用是什么？</h3><p>在分布式架构中，住短期模式的作用也是类似的，当某个服务单元发生故障之后，通过断路器的故障监控，向调用方返回一个错误响应，而不是长时间的等待，这样就不会使得线程因调用故障服务长时间占用不释放，避免了故障在分布式系统中的蔓延</p><h2 id="MyBatis-模块"><a href="#MyBatis-模块" class="headerlink" title="MyBatis 模块"></a>MyBatis 模块</h2><h3 id="125-MyBatis-中-和-的区别是什么？"><a href="#125-MyBatis-中-和-的区别是什么？" class="headerlink" title="125. MyBatis 中 #{} 和 ${} 的区别是什么？"></a>125. MyBatis 中 #{} 和 ${} 的区别是什么？</h3><p>#{} 是预编译处理，${} 是字符串替换</p><p>再使用 #{} 时，MyBatis 会将 SQL 中的 #{} 替换成 “?”，配合 PreparedStatement 的 set 方法赋值，有效防止 SQL 注入，保证程序的运行安全</p><h3 id="126-MyBatis-有几种分页模式？"><a href="#126-MyBatis-有几种分页模式？" class="headerlink" title="126. MyBatis 有几种分页模式？"></a>126. MyBatis 有几种分页模式？</h3><ul><li><strong>逻辑分页：</strong>使用 MyBatis 自带的 RowBounds 进行分页，他是一次性查询很多数据，然后在数据中再进行检索</li><li><strong>物理分页：</strong>自己手写 SQL 分页或使用分页插件 PageHelper，去数据库查询指定条数的分页数据的形式</li></ul><h3 id="127-RowBounds-是一次性查询全部结果吗？"><a href="#127-RowBounds-是一次性查询全部结果吗？" class="headerlink" title="127. RowBounds 是一次性查询全部结果吗？"></a>127. RowBounds 是一次性查询全部结果吗？</h3><p>RowBounds 表面是在所有数据中检索数据，其实并非是一次性查询出所有数据，因为 MyBatis 是对 jdbc 的封装，在 jdbc 驱动中有一个 Fetch Size 的配置，它规定了每次最多从数据库中查询多少条数据，会在执行 next() 的时候，去查询更多的数据</p><p>对于 jdbc 来说，当调用 next() 的时候回自动完成查询工作，有效防止内存溢出</p><h3 id="128-MyBatis-逻辑分页和物理分页的区别？"><a href="#128-MyBatis-逻辑分页和物理分页的区别？" class="headerlink" title="128. MyBatis 逻辑分页和物理分页的区别？"></a>128. MyBatis 逻辑分页和物理分页的区别？</h3><p>逻辑分页是一次性查询很多数据，然后再在结果中检索分页的数据，弊端就是需要消耗大量的内存，有内存溢出的风险、对数据塔里较大</p><p>物理分页是从数据库中查询指定条数的数据，弥补了一次性全部查出所有数据的种种缺点</p><h3 id="129-MyBatis-延迟加载的原理是什么？"><a href="#129-MyBatis-延迟加载的原理是什么？" class="headerlink" title="129. MyBatis 延迟加载的原理是什么？"></a>129. MyBatis 延迟加载的原理是什么？</h3><p>MyBatis 支持延迟加载，设置 lazyLoadingEnabled=true 即可</p><p>延迟加载的原理是调用的时候触发加载，而不是在初始化的时候就加载信息</p><h3 id="130-说一下-MyBatis-的一级缓存和二级缓存"><a href="#130-说一下-MyBatis-的一级缓存和二级缓存" class="headerlink" title="130. 说一下 MyBatis 的一级缓存和二级缓存"></a>130. 说一下 MyBatis 的一级缓存和二级缓存</h3><ul><li>一级缓存：基于 PerpetualCache 和 HashMap 本地缓存，它的声明周期是和 SQLSession 一致的，有多个 SQLSession 或者分布式的环境中数据库操作，可能会出现脏数据。当 Session flush 或 close 之后该 Session 中的所有 Cache 就将清空，默认一级缓存是开启的</li><li>二级缓存：也是基于 PerpetualCache 的 HashMap 本地缓存，不同在于其存储作用域为 Mapper 级别的，如果多个 SQLSession 之间需要共享缓存，则需要使用到二级缓存，并且二级缓存可自定义存储源，如 Ehcache。默认不打开二级缓存，使用二级缓存属性需要实现 Serializable 序列化接口</li></ul><p>开启二级缓存数据查询流程：二级缓存 -&gt; 一级缓存 -&gt; 数据库</p><p>缓存更新机制：当某一作用域（一级缓存 Session / 二级缓存 Mapper）进行了 C / U / D 操作之后，默认该作用域下所有 select 中的缓存将被 clear</p><h3 id="131-MyBatis-和-hibernate-区别有哪些？"><a href="#131-MyBatis-和-hibernate-区别有哪些？" class="headerlink" title="131. MyBatis 和 hibernate 区别有哪些？"></a>131. MyBatis 和 hibernate 区别有哪些？</h3><ul><li><strong>灵活性：</strong>MyBatis 灵活</li><li><strong>可移植性：</strong>MyBatis 需要自己手写 SQL，每个数据库不同，SQL也不同，移植性较差</li><li><strong>学习和使用门槛：</strong>MyBatis 简单，入门和使用快</li><li><strong>二级缓存：</strong>hibernate 有更好的二级缓存，且可以自行更换为第三方的二级缓存</li></ul><h3 id="132-MyBatis-有哪些执行器？"><a href="#132-MyBatis-有哪些执行器？" class="headerlink" title="132. MyBatis 有哪些执行器？"></a>132. MyBatis 有哪些执行器？</h3><ul><li><strong>SimpleExecutor：</strong>每执行一次 update 或 select 就开启一个 Statement 对象，用完立刻关闭 Statement 对象</li><li><strong>ReuseExecutor：</strong>执行 update 或 select，以 SQL 作为 key 查找 Statement，存在就使用，不存在就创建，用完后不关闭 Statement 对象，而是放置于 Map 内供下一次使用。简单地说就是重复使用 Statement 对象</li><li><strong>BatchExecutor：</strong>执行 update（没有 select，jdbc 批处理不支持 select），将所有 SQL 都添加到批处理（addBatch()）中，等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象，每个 Statement 对象都是 addBatch() 完毕后，等待逐一执行 executeBatch() 批处理，与 jdbc 批处理相同</li></ul><h3 id="133-MyBatis-分页插件的实现原理是什么？"><a href="#133-MyBatis-分页插件的实现原理是什么？" class="headerlink" title="133. MyBatis 分页插件的实现原理是什么？"></a>133. MyBatis 分页插件的实现原理是什么？</h3><p>分页插件的基本原理是使用 MyBatis 提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的 SQL，然后重写 SQL，添加对应的物理分页语句和物理分页参数</p><h2 id="Kafka-模块"><a href="#Kafka-模块" class="headerlink" title="Kafka 模块"></a>Kafka 模块</h2><h3 id="152-kafka-可以脱离-zookeeper-单独使用吗？为什么？"><a href="#152-kafka-可以脱离-zookeeper-单独使用吗？为什么？" class="headerlink" title="152. kafka 可以脱离 zookeeper 单独使用吗？为什么？"></a>152. kafka 可以脱离 zookeeper 单独使用吗？为什么？</h3><p>不能，因为 kafka 使用 zookeeper 管理和协调 kafka 的节点服务器</p><h3 id="153-kafka-有几种数据保留的策略？"><a href="#153-kafka-有几种数据保留的策略？" class="headerlink" title="153. kafka 有几种数据保留的策略？"></a>153. kafka 有几种数据保留的策略？</h3><ul><li><strong>按照过期时间保留</strong></li><li><strong>按照存储的消息大小保留</strong></li></ul><h3 id="154-kafka-同时设置了-7-天和-10G-清楚数据，到第五天的时候消息达到了-10G，kafka-将如何处理？"><a href="#154-kafka-同时设置了-7-天和-10G-清楚数据，到第五天的时候消息达到了-10G，kafka-将如何处理？" class="headerlink" title="154. kafka 同时设置了 7 天和 10G 清楚数据，到第五天的时候消息达到了 10G，kafka 将如何处理？"></a>154. kafka 同时设置了 7 天和 10G 清楚数据，到第五天的时候消息达到了 10G，kafka 将如何处理？</h3><p>kafka 会执行数据清楚工作，时间和大小不论条件是否满足，都会清空数据</p><h3 id="155-什么情况下会导致-kafka-运行变慢？"><a href="#155-什么情况下会导致-kafka-运行变慢？" class="headerlink" title="155. 什么情况下会导致 kafka 运行变慢？"></a>155. 什么情况下会导致 kafka 运行变慢？</h3><ul><li>CPU 性能瓶颈</li><li>磁盘读写瓶颈</li><li>网络瓶颈</li></ul><h3 id="156-使用-kafka-集群需要注意什么？"><a href="#156-使用-kafka-集群需要注意什么？" class="headerlink" title="156. 使用 kafka 集群需要注意什么？"></a>156. 使用 kafka 集群需要注意什么？</h3><p>集群的数量不是越多越好，最好不要超过 7 个，节点越多，消息复制需要的时间就越长，整个群组的吞吐量就会越低</p><p>集群数量最好是单数，因为超过一半故障，集群就不能用了，设置为单数容错率更高</p><h2 id="Zookeeper-模块"><a href="#Zookeeper-模块" class="headerlink" title="Zookeeper 模块"></a>Zookeeper 模块</h2><h3 id="157-Zookeeper-是什么？"><a href="#157-Zookeeper-是什么？" class="headerlink" title="157. Zookeeper 是什么？"></a>157. Zookeeper 是什么？</h3><p>zookeeper 是一个分布式的，开放源码的分布式应用程序协调服务器，是 google chubby 开源实现，是 hadoop 和 hbase 的重要组件。它是一个分布式应用提供一致性服务的软件，提供的功能包括：配置维护，域名服务，分布式同步，租服务等</p><h3 id="158-zookeeper-都有哪些功能？"><a href="#158-zookeeper-都有哪些功能？" class="headerlink" title="158. zookeeper 都有哪些功能？"></a>158. zookeeper 都有哪些功能？</h3><ul><li><strong>集群管理：</strong>监控节点存活状态</li><li><strong>主节点选举：</strong>主节点挂掉之后，可以从备用的节点开始新一轮选主，注解点选举说的就是这个选举过程，使用 zookeeper 可以协助完成这个过程</li><li><strong>分布式锁：</strong>zookeeper 提供两种锁，<strong>独占锁</strong>和<strong>共享锁</strong>。独占锁即一次只能有一个线程使用资源，共享锁是读锁共享，读写互斥，即可以有多线程同时读同一个资源，如果要使用写锁，也只能有一个线程使用。zookeeper 可以对分布式锁进行控制</li><li><strong>命名服务：</strong>在分布式系统中，通过使用命名服务，客户端应用程序能够根据指定名字来获取资源或服务的地址，提供者等信息</li></ul><h3 id="159-zookeeper-有几种部署模式？"><a href="#159-zookeeper-有几种部署模式？" class="headerlink" title="159. zookeeper 有几种部署模式？"></a>159. zookeeper 有几种部署模式？</h3><ul><li>单机部署</li><li>集群部署</li><li>伪集群部署</li></ul><h3 id="160-zookeeper-怎么保证主从节点的状态同步？"><a href="#160-zookeeper-怎么保证主从节点的状态同步？" class="headerlink" title="160. zookeeper 怎么保证主从节点的状态同步？"></a>160. zookeeper 怎么保证主从节点的状态同步？</h3><p>zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步，实现这个机制的协议叫做 zab 协议，zab 协议有两种：</p><ul><li>恢复模式（选主）</li><li>广播模式（同步）</li></ul><p>当服务服务启动或者在领导者崩溃后，zab 就进入了恢复模式，当领导者被选择出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了，状态同步保证了 leader 和 server 具有相同的系统状态</p><h3 id="161-集群中为什么要有主节点？"><a href="#161-集群中为什么要有主节点？" class="headerlink" title="161. 集群中为什么要有主节点？"></a>161. 集群中为什么要有主节点？</h3><p>在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他机器可以共享这个结果，这样可以大大减少重复计算，提高性能，所以就需要主节点</p><h3 id="162-集群中有-3-太服务器，其中一个节点宕机，zookeeper-还可以使用吗？"><a href="#162-集群中有-3-太服务器，其中一个节点宕机，zookeeper-还可以使用吗？" class="headerlink" title="162. 集群中有 3 太服务器，其中一个节点宕机，zookeeper 还可以使用吗？"></a>162. 集群中有 3 太服务器，其中一个节点宕机，zookeeper 还可以使用吗？</h3><p>可以，单数服务器只要没有超过一半的服务器宕机，就可以继续使用</p><h3 id="163-说一下-zookeeper-的通知机制"><a href="#163-说一下-zookeeper-的通知机制" class="headerlink" title="163. 说一下 zookeeper 的通知机制"></a>163. 说一下 zookeeper 的通知机制</h3><p>客户端会对某一个 znode 建立一个 watcher 时间，当该 znode 发生变化时，这些客户端就会收到 zookeeper 的通知，然后客户端可以根据 znode 变化来做出业务上的改变</p><h2 id="MySQL-模块"><a href="#MySQL-模块" class="headerlink" title="MySQL 模块"></a>MySQL 模块</h2><h3 id="164-数据库的三范式是什么？"><a href="#164-数据库的三范式是什么？" class="headerlink" title="164. 数据库的三范式是什么？"></a>164. 数据库的三范式是什么？</h3><ul><li>第一范式（1NF）：强调的是列的原子性，<strong>列不可再分</strong></li><li>第二范式（2NF）：<strong>属性完全依赖于主键</strong></li><li>第三范式（3NF）：<strong>属性不依赖于其它非主属性    属性直接依赖于主键</strong></li></ul><h3 id="165-一张自增表里面总共有-7-条数据，删除最后两条，重启-MySQL-数据库，又插入一条数据，id-是多少？"><a href="#165-一张自增表里面总共有-7-条数据，删除最后两条，重启-MySQL-数据库，又插入一条数据，id-是多少？" class="headerlink" title="165. 一张自增表里面总共有 7 条数据，删除最后两条，重启 MySQL 数据库，又插入一条数据，id 是多少？"></a>165. 一张自增表里面总共有 7 条数据，删除最后两条，重启 MySQL 数据库，又插入一条数据，id 是多少？</h3><p>表类型如果是 MyISAM，id 就是 8</p><p>表类型如果是 InnoDB，id 就是 6</p><p>InnoDB 表只会吧自增主键的最大 id 记录在内存中，所以重启之后会导致最大 id 丢失</p><h3 id="166-如何获取当前数据库版本？"><a href="#166-如何获取当前数据库版本？" class="headerlink" title="166. 如何获取当前数据库版本？"></a>166. 如何获取当前数据库版本？</h3><p>使用 select version() 获取当前 MySQL 数据库版本</p><h3 id="167-说下-ACID-是什么？"><a href="#167-说下-ACID-是什么？" class="headerlink" title="167. 说下 ACID 是什么？"></a>167. 说下 ACID 是什么？</h3><ul><li><strong>Atomicity（原子性）：</strong>一个事务中的所有操作，或者全部完成，获取全部不完成，不会结束在中间某个环节，事务在执行过程中发生错误，会被恢复（Rollback）到事务开始之前的状态，就像这个事务从来没有执行过一样。<strong>即事务不可分割，不可简约</strong></li><li><strong>Consistency（一致性）：</strong>在事务开始之前和事务结束之后，数据库的完整性没有被破坏，这表示<strong>写入的资料必须完全复合物所有的预设约束、触发器、级联回滚等</strong></li><li><strong>Isolation（隔离性）：</strong>数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括：<strong>读未提交</strong>，读提交，<strong>可重复读</strong>和<strong>串行化</strong>。</li><li><strong>Durability（持久性）：</strong>事务处理结束后对数据的修改是永久的，即便系统故障也不会丢失。</li></ul><h3 id="168-char-和-varchar-的区别是什么？"><a href="#168-char-和-varchar-的区别是什么？" class="headerlink" title="168. char 和 varchar 的区别是什么？"></a>168. char 和 varchar 的区别是什么？</h3><ul><li>char(n)：固定长度类型<ul><li>优点：效率高</li><li>缺点：占用空间</li><li>适用场景：存储密码的 md5 值，固定长度的使用 char 非常合适</li></ul></li><li>varchar(n)：可变长度，存储的值是每个值占用的字节，加上一个用来记录其长度的字节长度</li></ul><p>所以，从空间上考虑 varchar 比较合适；从效率上考虑 char 比较合适。二者使用需要权衡</p><h3 id="169-float-和-double-的区别是什么？"><a href="#169-float-和-double-的区别是什么？" class="headerlink" title="169. float 和 double 的区别是什么？"></a>169. float 和 double 的区别是什么？</h3><ul><li>float 最多可以存储 8 位的十进制数，并且在内存中占 4 字节。</li><li>double 最多可以存储 16 位的十进制数，在内存中占 8 字节</li></ul><h3 id="170-MySQL-的内连接、左连接、右连接有什么区别？"><a href="#170-MySQL-的内连接、左连接、右连接有什么区别？" class="headerlink" title="170. MySQL 的内连接、左连接、右连接有什么区别？"></a>170. MySQL 的内连接、左连接、右连接有什么区别？</h3><p>内连接关键字：inner join</p><p>左连接关键字：left join</p><p>右连接关键字：right join</p><p>内连接是把匹配的关联数据显示出来</p><p>左连接是左边的表全部显示出来，右边的表显示出符合条件的数据；右连接相反</p><h3 id="171-MySQL-的索引怎么实现的？"><a href="#171-MySQL-的索引怎么实现的？" class="headerlink" title="171. MySQL 的索引怎么实现的？"></a>171. MySQL 的索引怎么实现的？</h3><p>索引是满足某种特定查找算法的数据结构，而这些数据结构和以某种方式指向数据，从而实现高效查找数据</p><p>具体来说 MySQL 中的索引，不同的数据引擎实现有所不同。但目前主流的数据库引擎的索引都是 B+ 树实现的，B+ 树的搜索效率，可以达到二分法的性能，找到数据区域之后就找到了完整的数据结构了</p><h3 id="172-怎么验证买-MySQL-的索引是否满足需求？"><a href="#172-怎么验证买-MySQL-的索引是否满足需求？" class="headerlink" title="172. 怎么验证买 MySQL 的索引是否满足需求？"></a>172. 怎么验证买 MySQL 的索引是否满足需求？</h3><p>使用 explain 查看 SQL 是如何执行查询的，从而分析索引是否满足需求</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">explain</span> <span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">table</span> <span class="keyword">where</span> <span class="keyword">type</span> = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><h3 id="173-说一下数据库的事务隔离"><a href="#173-说一下数据库的事务隔离" class="headerlink" title="173. 说一下数据库的事务隔离?"></a>173. 说一下数据库的事务隔离?</h3><p>MySQL 的事务隔离是在 MySQL. ini 配置文件里添加的，在文件的最后添加：transaction-isolation = REPEATABLE-READ</p><p>可用的配置值：<strong>READ-UNCOMMITTED</strong>、<strong>READ-COMMITTED</strong>、<strong>REPEATABLE-READ</strong>、<strong>SERIALIZABLE</strong></p><ul><li>READ-UNCOMMITTED：未提交读，最低隔离级别、事务未提交前，就可被其他事务读取（会出现幻读、脏读、不可重复读）</li><li>READ-COMMITTED：提交读，一个事务提交后才能被其他事务读取到（会造成幻读、不可重复读）。</li><li>REPEATABLE-READ：可重复读，默认级别，保证多次读取同一个数据时，其值都和事务开始时候的内容是一致，禁止读取到别的事务未提交的数据（会造成幻读）</li><li>SERIALIZABLE：序列化，代价最高最可靠的隔离级别，该隔离级别能防止脏读、不可重复读、幻读</li><li>脏读 ：表示一个事务能够读取另一个事务中还未提交的数据。比如，某个事务尝试插入记录 A，此时该事务还未提交，然后另一个事务尝试读取到了记录 A</li><li>不可重复读 ：是指在一个事务内，多次读同一数据。</li><li>幻读 ：指同一个事务内多次查询返回的结果集不一样。比如同一个事务 A 第一次查询时候有 n 条记录，但是第二次同等条件下查询却有 n+1 条记录，这就好像产生了幻觉。发生幻读的原因也是另外一个事务新增或者删除或者修改了第一个事务结果集里面的数据，同一个记录的数据内容被修改了，所有数据行的记录就变多或者变少了。</li></ul><h3 id="174-说一下-MySQL-常用的引擎？"><a href="#174-说一下-MySQL-常用的引擎？" class="headerlink" title="174.说一下 MySQL 常用的引擎？"></a>174.说一下 MySQL 常用的引擎？</h3><ul><li>InnoDB 引擎：InnoDB 引擎提供了对数据库 acid 事务的支持，并且还提供了行级锁和外键的约束，它的设计的目标就是处理大数据容量的数据库系统。MySQL 运行的时候，InnoDB 会在内存中建立缓冲池，用于缓冲数据和索引。但是该引擎是不支持全文搜索，同时启动也比较的慢，它是不会保存表的行数的，所以当进行 select count(<em>) from table 指令的时候，需要进行扫描全表。由于锁的粒度小，写操作是不会锁定全表的,所以在并发度较高的场景下使用会提升效率的。</em></li><li>MyIASM 引擎：MySQL 的默认引擎，但不提供事务的支持，也不支持行级锁和外键。因此当执行插入和更新语句时，即执行写操作的时候需要锁定这个表，所以会导致效率会降低。不过和 InnoDB 不同的是，MyIASM 引擎是保存了表的行数，于是当进行 select count(*) from table 语句时，可以直接的读取已经保存的值而不需要进行扫描全表。所以，如果表的读操作远远多于写操作时，并且不需要事务的支持的，可以将 MyIASM 作为数据库引擎的首选。</li></ul><h3 id="175-说一下-MySQL-的行锁和表锁？"><a href="#175-说一下-MySQL-的行锁和表锁？" class="headerlink" title="175.说一下 MySQL 的行锁和表锁？"></a>175.说一下 MySQL 的行锁和表锁？</h3><p>MyISAM 只支持表锁，InnoDB 支持表锁和行锁，默认为行锁。</p><ul><li>表级锁：开销小，加锁快，不会出现死锁。锁定粒度大，发生锁冲突的概率最高，并发量最低。</li><li>行级锁：开销大，加锁慢，会出现死锁。锁力度小，发生锁冲突的概率小，并发度最高。</li></ul><h3 id="176-说一下乐观锁和悲观锁？"><a href="#176-说一下乐观锁和悲观锁？" class="headerlink" title="176.说一下乐观锁和悲观锁？"></a>176.说一下乐观锁和悲观锁？</h3><p>乐观锁：每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在提交更新的时候会判断一下在此期间别人有没有去更新这个数据</p><p>悲观锁：每次去拿数据的时候都认为别人会修改，所以每次拿数据的时候都会上锁，这样别人想拿这条数据就会阻止，这个锁被释放。</p><h3 id="177-MySQL-问题排查都有哪些手段？"><a href="#177-MySQL-问题排查都有哪些手段？" class="headerlink" title="177.MySQL 问题排查都有哪些手段？"></a>177.MySQL 问题排查都有哪些手段？</h3><ul><li>使用 show processlist 命令查看当前所有连接信息。</li><li>使用 explain 命令查询 SQL 语句执行计划。</li><li>开启慢查询日志，查看慢查询的 SQL。</li></ul><h3 id="178-如何做-MySQL-的性能优化？"><a href="#178-如何做-MySQL-的性能优化？" class="headerlink" title="178.如何做 MySQL 的性能优化？"></a>178.如何做 MySQL 的性能优化？</h3><ul><li>为搜索字段创建索引。</li><li>避免使用 select *，列出需要查询的字段。</li><li>垂直分割分表。</li><li>选择正确的存储引擎。</li></ul><h2 id="Redis-模块"><a href="#Redis-模块" class="headerlink" title="Redis 模块"></a>Redis 模块</h2><h3 id="179-Redis-是什么？都有哪些使用场景？"><a href="#179-Redis-是什么？都有哪些使用场景？" class="headerlink" title="179.Redis 是什么？都有哪些使用场景？"></a>179.Redis 是什么？都有哪些使用场景？</h3><p>Redis 是一个使用 C 语言开发的高速缓存数据库</p><p>Redis 使用场景：</p><ul><li>记录帖子点赞数、点击数、评论数</li><li>缓存近期热帖</li><li>缓存文章详情信息；</li><li>记录用户会话信息</li></ul><h3 id="180-Redis-有哪些功能？"><a href="#180-Redis-有哪些功能？" class="headerlink" title="180.Redis 有哪些功能？"></a>180.Redis 有哪些功能？</h3><ul><li>数据缓存功能</li><li>分布式锁的功能</li><li>支持数据持久化</li><li>支持事务</li><li>支持消息队列</li></ul><h3 id="182-Redis-为什么是单线程的？"><a href="#182-Redis-为什么是单线程的？" class="headerlink" title="182.Redis 为什么是单线程的？"></a>182.Redis 为什么是单线程的？</h3><p>因为 cpu 不是 Redis 的瓶颈，Redis 的瓶颈最有可能是机器内存或者网络带宽</p><h3 id="183-什么是缓存穿透？怎么解决？"><a href="#183-什么是缓存穿透？怎么解决？" class="headerlink" title="183.什么是缓存穿透？怎么解决？"></a>183.什么是缓存穿透？怎么解决？</h3><ul><li>缓存穿透：指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。</li><li>解决方案：最简单粗暴的方法如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们就把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。</li></ul><h3 id="184-Redis-支持的数据类型有哪些？"><a href="#184-Redis-支持的数据类型有哪些？" class="headerlink" title="184.Redis 支持的数据类型有哪些？"></a>184.Redis 支持的数据类型有哪些？</h3><p>Redis 支持的数据类型：string（字符串）、list（列表）、hash（字典）、set（集合）、zset（有序集合）</p><h3 id="185-Redis-支持的-Java-客户端都有哪些？"><a href="#185-Redis-支持的-Java-客户端都有哪些？" class="headerlink" title="185.Redis 支持的 Java 客户端都有哪些？"></a>185.Redis 支持的 Java 客户端都有哪些？</h3><p>支持的 Java 客户端有 Redisson、Jedis、lettuce 等</p><h3 id="187-怎么保证缓存和数据库数据的一致性？"><a href="#187-怎么保证缓存和数据库数据的一致性？" class="headerlink" title="187.怎么保证缓存和数据库数据的一致性？"></a>187.怎么保证缓存和数据库数据的一致性？</h3><ol><li>合理设置缓存的过期时间。</li><li>新增、更改、删除数据库操作时同步更新 Redis，可以使用事物机制来保证数据的一致性。</li></ol><h3 id="188-Redis-持久化有几种方式？"><a href="#188-Redis-持久化有几种方式？" class="headerlink" title="188.Redis 持久化有几种方式？"></a>188.Redis 持久化有几种方式？</h3><ol><li>RDB（Redis Database）：指定的时间间隔能对你的数据进行快照存储。</li><li>AOF（Append Only File）：每一个收到的写命令都通过 write 函数追加到文件中。</li></ol><h3 id="190-Redis-分布式锁有什么缺陷？"><a href="#190-Redis-分布式锁有什么缺陷？" class="headerlink" title="190.Redis 分布式锁有什么缺陷？"></a>190.Redis 分布式锁有什么缺陷？</h3><p>Redis 分布式锁不能解决超时的问题，分布式锁有一个超时时间，程序的执行如果超出了锁的超时时间就会出现问题</p><h3 id="191-Redis-如何做内存优化？"><a href="#191-Redis-如何做内存优化？" class="headerlink" title="191.Redis 如何做内存优化？"></a>191.Redis 如何做内存优化？</h3><p>尽量使用 Redis 的散列表，把相关的信息放到散列表里面存储，而不是把每个字段单独存储，这样可以有效的减少内存使用。比如将 Web 系统的用户对象，应该放到散列表里面再整体存储到 Redis，而不是把用户的姓名、年龄、密码、邮箱等字段分别设置 key 进行存储。</p><h3 id="193-Redis-常见的性能问题有哪些？该如何解决？"><a href="#193-Redis-常见的性能问题有哪些？该如何解决？" class="headerlink" title="193.Redis 常见的性能问题有哪些？该如何解决？"></a>193.Redis 常见的性能问题有哪些？该如何解决？</h3><p>主服务器写内存快照，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务，所以主服务器最好不要写内存快照</p><p>Redis 主从复制的性能问题，为了主从复制的速度和连接的稳定性，主从库最好在同一个局域网内。</p><h2 id="JVM-模块"><a href="#JVM-模块" class="headerlink" title="JVM 模块"></a>JVM 模块</h2><h3 id="194-说一下-JVM-的主要组成部分？及其作用？"><a href="#194-说一下-JVM-的主要组成部分？及其作用？" class="headerlink" title="194.说一下 JVM 的主要组成部分？及其作用？"></a>194.说一下 JVM 的主要组成部分？及其作用？</h3><ul><li>类加载器（ClassLoader）</li><li>运行时数据区（Runtime Data Area）</li><li>执行引擎（Execution Engine）</li><li>本地库接口（Native Interface）</li><li>组件的作用：首先通过类加载器（ClassLoader）会把 Java 代码转换成字节码，运行时数据区（Runtime Data Area）再把字节码加载到内存中，而字节码文件只是 JVM 的一套指令集规范，并不能直接交个底层操作系统去执行，因此需要特定的命令解析器执行引擎（Execution Engine），将字节码翻译成底层系统指令，再交由 CPU 去执行，而这个过程中需要调用其他语言的本地库接口（Native Interface）来实现整个程序的功能。</li></ul><h3 id="195-说一下-JVM-运行时数据区？"><a href="#195-说一下-JVM-运行时数据区？" class="headerlink" title="195.说一下 JVM 运行时数据区？"></a>195.说一下 JVM 运行时数据区？</h3><ul><li><strong>程序计数器（Program Counter Register）：</strong>当前线程所执行的字节码的行号指示器，字节码解析器的工作是通过改变这个计数器的值，来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能，都需要依赖这个计数器来完成</li><li><strong>Java 虚拟机栈（Java Virtual Machine Stacks）：</strong>用于存储局部变量表、操作数栈、动态链接、方法出口等信息</li><li><strong>本地方法栈（Native Method Stack）：</strong>与虚拟机栈的作用是一样的，只不过虚拟机栈是服务 Java 方法的，而本地方法栈是为虚拟机调用 Native 方法服务的</li><li><strong>Java 堆（Java Heap）：</strong>Java 虚拟机中内存最大的一块，是被所有线程共享的，几乎所有的对象实例都在这里分配内存</li><li><strong>方法区（Methed Area）：</strong>用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据</li></ul><h3 id="196-说一下堆栈的区别？"><a href="#196-说一下堆栈的区别？" class="headerlink" title="196.说一下堆栈的区别？"></a>196.说一下堆栈的区别？</h3><ul><li>功能方面：堆是用来存放对象的，栈是用来执行程序的。</li><li>共享性：堆是线程共享的，栈是线程私有的。</li><li>空间大小：堆大小远远大于栈。</li></ul><h3 id="197-队列和栈是什么？有什么区别？"><a href="#197-队列和栈是什么？有什么区别？" class="headerlink" title="197.队列和栈是什么？有什么区别？"></a>197.队列和栈是什么？有什么区别？</h3><p>队列和栈都是被<strong>用来预存储数据的</strong></p><p>队列允许先进先出检索元素，但也有例外的情况，Deque 接口允许从两端检索元素</p><p>栈和队列很相似，但它运行对元素进行后进先出进行检索</p><h3 id="198-什么是双亲委派模型？"><a href="#198-什么是双亲委派模型？" class="headerlink" title="198.什么是双亲委派模型？"></a>198.什么是双亲委派模型？</h3><p>在介绍双亲委派模型之前先说下类加载器。对于任意一个类，都需要由加载它的类加载器和这个类本身统一确立在 JVM 中的唯一性，每一个类加载器，都有一个独立的类名称空间。类加载器就是根据指定全限定名称将 class 文件加载到 JVM 内存，然后再转化为 class 对象。</p><p>类加载器分类：</p><ul><li>启动类加载器（Bootstrap ClassLoader），是虚拟机自身的一部分，用来加载Java_HOME/lib/目录中的，或者被 -Xbootclasspath 参数所指定的路径中并且被虚拟机识别的类库；</li><li>其他类加载器：<ul><li>扩展类加载器（Extension ClassLoader）：负责加载libext目录或Java. ext. dirs系统变量指定的路径中的所有类库；</li><li>应用程序类加载器（Application ClassLoader）：负责加载用户类路径（classpath）上的指定类库，我们可以直接使用这个类加载器。一般情况，如果我们没有自定义类加载器默认就是用这个加载器</li></ul></li></ul><p><strong>双亲委派模型：</strong>如果一个类加载器收到了类加载的请求，它首先不会自己去加载这个类，而是把这个请求委派给父类加载器去完成，每一层的类加载器都是如此，这样所有的加载请求都会被传送到顶层的启动类加载器中，只有当父加载无法完成加载请求（它的搜索范围中没找到所需的类）时，子加载器才会尝试去加载类。</p><h3 id="199-说一下类装载的执行过程？"><a href="#199-说一下类装载的执行过程？" class="headerlink" title="199.说一下类装载的执行过程？"></a>199.说一下类装载的执行过程？</h3><ol><li>加载：根据查找路径找到相应的 class 文件然后导入；</li><li>检查：检查加载的 class 文件的正确性；</li><li>准备：给类中的静态变量分配内存空间；</li><li>解析：虚拟机将常量池中的符号引用替换成直接引用的过程。符号引用就理解为一个标示，而在直接引用直接指向内存中的地址；</li><li>初始化：对静态变量和静态代码块执行初始化工作；</li></ol><h3 id="200-怎么判断对象是否可以被回收？"><a href="#200-怎么判断对象是否可以被回收？" class="headerlink" title="200.怎么判断对象是否可以被回收？"></a>200.怎么判断对象是否可以被回收？</h3><ul><li>引用计数器：为每个对象创建一个引用计数，有对象引用时计数器 +1，引用被释放时计数 -1，当计数器为 0 时就可以被回收。<strong>它有一个缺点不能解决循环引用的问题</strong></li><li>可达性分析：从 GC Roots 开始向下搜索，搜索所走过的路径称为引用链。当一个对象到 GC Roots 没有任何引用链相连时，则证明此对象是可以被回收的</li></ul><h3 id="201-Java-中都有哪些引用类型？"><a href="#201-Java-中都有哪些引用类型？" class="headerlink" title="201.Java 中都有哪些引用类型？"></a>201.Java 中都有哪些引用类型？</h3><ul><li>强引用：发生 gc 的时候不会被回收。</li><li>软引用：有用但不是必须的对象，在发生内存溢出之前会被回收</li><li>弱引用：有用但不是必须的对象，在下一次GC时会被回收。</li><li>虚引用（幽灵引用/幻影引用）：无法通过虚引用获得对象，用 PhantomReference 现虚引用，虚引用的用途是在 gc 时返回一个通知</li></ul><h3 id="202-说一下-JVM-有哪些垃圾回收算法？"><a href="#202-说一下-JVM-有哪些垃圾回收算法？" class="headerlink" title="202.说一下 JVM 有哪些垃圾回收算法？"></a>202.说一下 JVM 有哪些垃圾回收算法？</h3><ul><li>标记-清除算法：标记无用对象，然后进行清除回收。缺点：效率不高，无法清除垃圾碎片</li><li>标记-整理算法：标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存</li><li>复制算法：按照容量划分二个大小相等的内存区域，当一块用完的时候将活着的对象复制到另一块上，然后再把已使用的内存空间一次清理掉。缺点：内存使用率不高，只有原来的一半</li><li>分代算法：根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代，新生代基本采用复制算法，老年代采用标记整理算法</li></ul><h3 id="203-说一下-JVM-有哪些垃圾回收器？"><a href="#203-说一下-JVM-有哪些垃圾回收器？" class="headerlink" title="203.说一下 JVM 有哪些垃圾回收器？"></a>203.说一下 JVM 有哪些垃圾回收器？</h3><ul><li>Serial：最早的单线程串行垃圾回收器。</li><li>Serial Old：Serial 垃圾回收器的老年版本，同样也是单线程的，可以作为 CMS 垃圾回收器的备选预案。</li><li>ParNew：是 Serial 的多线程版本。</li><li>Parallel 和 ParNew 收集器类似是多线程的，但 Parallel 是吞吐量优先的收集器，可以牺牲等待时间换取系统的吞吐量。</li><li>Parallel Old 是 Parallel 老生代版本，Parallel 使用的是复制的内存回收算法，Parallel Old 使用的是标记-整理的内存</li></ul><p>回收算法。</p><ul><li>CMS：一种以获得最短停顿时间为目标的收集器，非常适用 B/S 系统。</li><li>G1：一种兼顾吞吐量和停顿时间的 GC 实现，是 JDK 9 以后的默认 GC 选项。</li></ul><h3 id="205-新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？"><a href="#205-新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？" class="headerlink" title="205.新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？"></a>205.新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？</h3><p>新生代回收器：Serial、ParNew、Parallel Scavenge</p><p>老年代回收器：Serial Old、Parallel Old、CMS</p><p>整堆回收器：G1</p><p>新生代垃圾回收器一般采用的是复制算法，复制算法的优点是效率高，缺点是内存利用率低；老年代回收器一般采用的是标记-整理的算法进行垃圾回收。</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;SpringBoot / SpringCloud&lt;/li&gt;
&lt;li&gt;MyBatis 模块&lt;/li&gt;
&lt;li&gt;Kafka 模块&lt;/li&gt;
&lt;li&gt;Zookeeper 模块&lt;/li&gt;
&lt;li&gt;MySQL 模块&lt;/li&gt;
&lt;li&gt;Redis 模块&lt;/li&gt;
&lt;li&gt;JVM 模块&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;题库来源于 &lt;a href=&quot;https://www.javazhiyin.com/42272.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Java知音&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Java" scheme="https://www.chentyit.com/categories/Java/"/>
    
    
      <category term="面试" scheme="https://www.chentyit.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 整合 Kafka</title>
    <link href="https://www.chentyit.com/2019/10/02/Spark-Streaming-%E6%95%B4%E5%90%88-Kafka/"/>
    <id>https://www.chentyit.com/2019/10/02/Spark-Streaming-整合-Kafka/</id>
    <published>2019-10-02T15:25:41.000Z</published>
    <updated>2019-10-08T12:16:31.788Z</updated>
    
    <content type="html"><![CDATA[<ul><li>使用版本</li><li>Receiver-based</li><li>Direct Approach (No Receivers) 推荐</li><li>指定偏移量读取 Kafka 信息</li></ul><a id="more"></a><h2 id="使用版本"><a href="#使用版本" class="headerlink" title="使用版本"></a>使用版本</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-8_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 个人感觉 10 版本比较好用 但是下面的 API 就不一样了 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 官网上有详细文档，为了保证工程一致性，下面的 API使用 8 版本 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-kafka-0-10_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span></span></span><br></pre></td></tr></table></figure><h2 id="Receiver-based"><a href="#Receiver-based" class="headerlink" title="Receiver-based"></a>Receiver-based</h2><p>这种方法使用接收器来接收数据。接收器是使用 Kafka 高级消费者 API 实现的；与所有接收器一样，通过接收器从 Kafka 接收的数据存储在 Spark 执行器中，然后由 Spark Streaming 启动的作业处理数据</p><h3 id="操作步骤"><a href="#操作步骤" class="headerlink" title="操作步骤"></a>操作步骤</h3><ol><li>启动 ZK</li><li>启动 Kafka</li><li>创建 topic（kafka_streaming_topic_cty）</li></ol><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/10/2 18:21</span></span><br><span class="line"><span class="comment"> * @Description: SparkStreaming 对接 Kafka —— Receiver</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaReceiverWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">4</span>) &#123;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">"KafkaReceiverWordCount &lt;zkQuorum&gt; &lt;group&gt; &lt;topics&gt; &lt;numThreads&gt;"</span>)</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> <span class="type">Array</span>(zkQuorum, group, topics, numThreads) = args</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> topicMap = topics.split(<span class="string">","</span>).map((_, numThreads.toInt)).toMap</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"KafkaReceiverWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createStream(ssc, zkQuorum, group, topicMap)</span><br><span class="line">        messages map(_._2) flatMap(_.split(<span class="string">" "</span>)) map((_, <span class="number">1</span>)) reduceByKey(_ + _) print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="提交集群运行"><a href="#提交集群运行" class="headerlink" title="提交集群运行"></a>提交集群运行</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.spark.KafkaReceiverWordCount \</span><br><span class="line">--master local[2] \</span><br><span class="line">--name KafkaReceiverWordCount \</span><br><span class="line"><span class="meta">#</span> 这个必须得加，在生产环境中需要用 maven 下载好 jar 包直接添加到 kafka 的 lib 里面</span><br><span class="line">--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0 \</span><br><span class="line">/home/hadoop/lib/sparktrain-1.0.jar  hadoop000:2181 test kafka_streaming_topic 1</span><br></pre></td></tr></table></figure><h2 id="Direct-Approach-No-Receivers-推荐"><a href="#Direct-Approach-No-Receivers-推荐" class="headerlink" title="Direct Approach (No Receivers) 推荐"></a>Direct Approach (No Receivers) 推荐</h2><p>与基于接收器的方法（即方法1）相比，该方法具有以下优点：（官网翻译）</p><ul><li><strong>简化的并行性：</strong>无需创建多个输入Kafka流并将它们合并。使用 directStream，Spark Streaming 将创建与要使用的Kafka分区一样多的 RDD 分区，所有这些分区都将从 Kafka 并行读取数据。因此，Kafka 和 RDD 分区之间存在一对一的映射，这更易于理解和调整</li><li><strong>效率：在第一种方法中，要实现零数据丢失，需要将数据存储在预写日志中</strong>，从而进一步复制数据。这实际上是低效的，因为数据被有效地复制了两次-一次是通过 Kafka 复制，另一次是通过 “预写日志” 复制。第二种方法消除了该问题，因为没有接收器，因此不需要预写日志。<strong>只要您有足够的 Kafka 保留时间，就可以从 Kafka 中恢复信息</strong>。</li><li><strong>只执行一次精确的语义：</strong>第一种方法使用Kafka的高级 API 将偏移量存储在 Zookeeper 中。传统上，这是从 Kafka 消费数据的方式。尽管这种方法（与预写日志结合使用）可以确保零数据丢失（即至少一次语义），但在某些故障下某些记录可能会被消耗两次的可能性很小。发生这种情况是由于 Spark Streaming 可靠接收的数据与 Zookeeper 跟踪的偏移量之间存在不一致。因此，在第二种方法中，我们使用不使用 Zookeeper 的简单 Kafka API。Spark Streaming 在其检查点内跟踪偏移。这样可以消除 Spark Streaming 与 Zookeeper / Kafka 之间的不一致，因此即使出现故障，Spark Streaming 也会有效地一次接收每条记录。为了获得结果输出的一次语义，将数据保存到外部数据存储的输出操作必须是幂等的，或者是保存结果和偏移量的原子事务（请参见主程序中的输出操作的语义）有关更多信息的指南）</li></ul><h3 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/10/2 19:12</span></span><br><span class="line"><span class="comment"> * @Description: SparkStreaming 对接 Kafka —— Direct</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaDirectWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">"Usage: KafkaDirectWordCount &lt;brokers&gt; &lt;topics&gt;"</span>)</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> <span class="type">Array</span>(brokers, topics) = args</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaDirectWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> topicsSet = topics.split(<span class="string">","</span>).toSet</span><br><span class="line">        <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="string">"metadata.broker.list"</span> -&gt; brokers)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc, kafkaParams, topicsSet)</span><br><span class="line"></span><br><span class="line">        messages.map(_._2).flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _).print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="指定偏移量读取-Kafka-信息"><a href="#指定偏移量读取-Kafka-信息" class="headerlink" title="指定偏移量读取 Kafka 信息"></a>指定偏移量读取 Kafka 信息</h2><p><strong>代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Offset02App</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">        <span class="string">"metadata.broker.list"</span> -&gt; <span class="string">"192.168.1.8:9092"</span>,</span><br><span class="line">        <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"smallest"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">val</span> topics = <span class="string">"imooc_cty_offset"</span>.split(<span class="string">", "</span>).toSet</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">val</span> checkpointDirectory = <span class="string">"E:\\test\\ck_point"</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line">        <span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>, <span class="type">StringDecoder</span>, <span class="type">StringDecoder</span>](ssc, kafkaParams, topics)</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * set checkpoint directory</span></span><br><span class="line"><span class="comment">         * 将偏移量存储到外部介质中</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ssc.checkpoint(checkpointDirectory)</span><br><span class="line">        messages.checkpoint(<span class="type">Duration</span>(<span class="number">10</span> * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">        messages.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">                println(<span class="string">"慕课 CTY: "</span> + rdd.count())</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        ssc</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Offset01App"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="type">StreamingContext</span>.getOrCreate(checkpointDirectory, functionToCreateContext _)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;使用版本&lt;/li&gt;
&lt;li&gt;Receiver-based&lt;/li&gt;
&lt;li&gt;Direct Approach (No Receivers) 推荐&lt;/li&gt;
&lt;li&gt;指定偏移量读取 Kafka 信息&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
      <category term="Kafka" scheme="https://www.chentyit.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 整合 Flume</title>
    <link href="https://www.chentyit.com/2019/10/02/Spark-Streaming-%E6%95%B4%E5%90%88-Flume/"/>
    <id>https://www.chentyit.com/2019/10/02/Spark-Streaming-整合-Flume/</id>
    <published>2019-10-02T04:18:41.000Z</published>
    <updated>2019-10-02T15:26:39.357Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Flume-style Push-based Approach</li><li>Pull-based Approach using a Custom Sink（推荐使用）</li></ul><a id="more"></a><h2 id="Flume-style-Push-based-Approach"><a href="#Flume-style-Push-based-Approach" class="headerlink" title="Flume-style Push-based Approach"></a>Flume-style Push-based Approach</h2><p><strong>基于Flume的推送的方法</strong></p><p>（管网翻译）</p><p>Flume 旨在在 Flume 代理之间推送数据。在这种方法中，Spark Streaming 本质上设置了一个接收器，该接收器充当 Flume 的 Avro 代理，Flume 可以将数据推送到该接收器。</p><ul><li>启动 Flume + Spark Streaming 应用程序时，其中一个 Spark 辅助程序必须在该计算机上运行</li><li>可以将 Flume 配置为将数据推送到该计算机上的端口</li></ul><h3 id="Flume-Agent-的编写："><a href="#Flume-Agent-的编写：" class="headerlink" title="Flume Agent 的编写："></a>Flume Agent 的编写：</h3><p><strong>flume_push_stream.conf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">simple-agent.sources = netcat-source</span><br><span class="line">simple-agent.sinks = avro-sink</span><br><span class="line">simple-agent.channels = memory-channel</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.type = netcat</span><br><span class="line">simple-agent.sources.netcat-source.bind = hadoop000</span><br><span class="line">simple-agent.sources.netcat-source.port = 44444</span><br><span class="line"></span><br><span class="line">simple-agent.sinks.avro-sink.type = avro</span><br><span class="line">simple-agent.sinks.avro-sink.hostname = hadoop000</span><br><span class="line">simple-agent.sinks.avro-sink.port = 41414</span><br><span class="line">simple-agent.sinks.avro-sink.connect-timeout = 30000</span><br><span class="line"></span><br><span class="line">simple-agent.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.channels = memory-channel</span><br><span class="line">simple-agent.sinks.avro-sink.channel = memory-channel</span><br></pre></td></tr></table></figure><p><strong>Spark Streaming 代码</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/10/2 10:18</span></span><br><span class="line"><span class="comment"> * @Description: Spark Streaming 整合 Flume —— Push</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FlumePushWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交到集群运行时需要判断参数</span></span><br><span class="line">        <span class="keyword">if</span> (args.length != <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">"Usage: FlumePushWordCount &lt;hostname&gt; &lt;port&gt;"</span>)</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> <span class="type">Array</span>(hostname, port) = args</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"FlumePushWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这里的 IP 地址是 Flume 机器的地址</span></span><br><span class="line">        <span class="comment">// 将参数传递到这来</span></span><br><span class="line">        <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createStream(ssc, hostname, port.toInt)</span><br><span class="line">        flumeStream.map(x =&gt; <span class="keyword">new</span> <span class="type">String</span>(x.event.getBody.array()).trim)</span><br><span class="line">        .flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _).print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>提交集群运行方式</strong>（也可以在本地运行调试再打包到集群中，注意下文中的<strong>踩坑</strong>）</p><ol><li><p>Spark 的 jar 包运行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class com.imooc.spark.FlumePushWordCount \</span><br><span class="line">--master local[2] \</span><br><span class="line">--packages org.apache.spark:spark-streaming-flume_2.11:2.2.0 \</span><br><span class="line">/home/hadoop/lib/sparktrain-1.0.jar \</span><br><span class="line">hadoop000 41414</span><br></pre></td></tr></table></figure></li><li><p>启动 Flume</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name simple-agent  \</span><br><span class="line">--conf $FLUME_HOME/conf  \</span><br><span class="line">--conf-file $FLUME_HOME/conf/flume_push_stream.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure></li></ol><h3 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h3><ol><li>sink 的 IP 地址应该是 SparkStreaming 程序运行的那台机器的 IP 地址，如果这台机器在集群上，就要把程序打包上传集群运行（注：flume 所在的机器必须和程序运行的机器能 ping  通）</li><li>先启动 SparkStreaming 程序，再启动 flume</li></ol><h2 id="Pull-based-Approach-using-a-Custom-Sink（推荐使用）"><a href="#Pull-based-Approach-using-a-Custom-Sink（推荐使用）" class="headerlink" title="Pull-based Approach using a Custom Sink（推荐使用）"></a>Pull-based Approach using a Custom Sink（推荐使用）</h2><p><strong>基于 Pull 的方法自定义接收器</strong></p><p>这种方法不是运行 Flume 将数据直接推送到 Spark Streaming，而是运行自定义的 Flume 接收器</p><ul><li>Flume 将数据推入接收器，并且数据保持缓冲状态</li><li>Spark Streaming 使用可靠的 Flume 接收器和事务从接收器中提取数据。只有在 Spark Streaming 接收并复制了数据之后，事务才能成功</li></ul><p>与以前的方法相比，这确保了更强的可靠性和容错保证。但是，这需要将Flume配置为运行自定义接收器</p><h3 id="Flume-Agent-的编写：-1"><a href="#Flume-Agent-的编写：-1" class="headerlink" title="Flume Agent 的编写："></a>Flume Agent 的编写：</h3><p><strong>flume_pull_streaming.conf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">simple-agent.sources = netcat-source</span><br><span class="line">simple-agent.sinks = spark-sink</span><br><span class="line">simple-agent.channels = memory-channel</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.type = netcat</span><br><span class="line">simple-agent.sources.netcat-source.bind = hadoop000</span><br><span class="line">simple-agent.sources.netcat-source.port = 44444</span><br><span class="line"></span><br><span class="line">simple-agent.sinks.spark-sink.type = org.apache.spark.streaming.flume.sink.SparkSink</span><br><span class="line">simple-agent.sinks.spark-sink.hostname = hadoop000</span><br><span class="line">simple-agent.sinks.spark-sink.port = 41414</span><br><span class="line"></span><br><span class="line">simple-agent.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">simple-agent.sources.netcat-source.channels = memory-channel</span><br><span class="line">simple-agent.sinks.spark-sink.channel = memory-channel</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent  \</span><br><span class="line">--name simple-agent   \</span><br><span class="line">--conf $FLUME_HOME/conf    \</span><br><span class="line">--conf-file $FLUME_HOME/conf/flume_pull_streaming.conf  \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p><strong>Spark Streaming 代码</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/10/2 12:10</span></span><br><span class="line"><span class="comment"> * @Description: Spark Streaming 整合 Flume - Pull</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FlumePullWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"FlumePushWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 这里的 IP 地址是 Flume 机器的地址</span></span><br><span class="line">        <span class="keyword">val</span> flumeStream = <span class="type">FlumeUtils</span>.createPollingStream(ssc, <span class="string">"192.168.10.120"</span>, <span class="number">41414</span>)</span><br><span class="line">        flumeStream.map(x =&gt; <span class="keyword">new</span> <span class="type">String</span>(x.event.getBody.array()).trim)</span><br><span class="line">        .flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _).print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="踩坑-1"><a href="#踩坑-1" class="headerlink" title="踩坑"></a>踩坑</h3><ol><li>先启动 FLume，再启动 SparkStreaming 程序</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Flume-style Push-based Approach&lt;/li&gt;
&lt;li&gt;Pull-based Approach using a Custom Sink（推荐使用）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flume" scheme="https://www.chentyit.com/tags/Flume/"/>
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkStreaming 进阶</title>
    <link href="https://www.chentyit.com/2019/09/28/SparkStreaming-%E8%BF%9B%E9%98%B6/"/>
    <id>https://www.chentyit.com/2019/09/28/SparkStreaming-进阶/</id>
    <published>2019-09-28T12:18:32.000Z</published>
    <updated>2019-09-28T12:20:12.737Z</updated>
    
    <content type="html"><![CDATA[<ul><li>带状态的算子（UpdateStateByKey）</li><li>基于 window 的统计</li><li>实例测试</li></ul><a id="more"></a><h2 id="带状态的算子（UpdateStateByKey）"><a href="#带状态的算子（UpdateStateByKey）" class="headerlink" title="带状态的算子（UpdateStateByKey）"></a>带状态的算子（UpdateStateByKey）</h2><p>updateStateByKey 操作使您可以保持任意状态，同时用新信息连续更新它。要使用此功能，将必须执行两个步骤：（官网内容）</p><ol><li><strong>定义状态：</strong>状态可以是任意数据类型</li><li><strong>定义状态更新功能：</strong>使用功能指定如何使用输入流中的先前状态和新值来更新状态</li></ol><h2 id="基于-window-的统计"><a href="#基于-window-的统计" class="headerlink" title="基于 window 的统计"></a>基于 window 的统计</h2><p>Spark Streaming 还提供窗口化计算，使您可以在数据的滑动窗口上应用转换</p><p><img src="http://spark.apache.org/docs/2.2.0/img/streaming-dstream-window.png" alt="window"></p><p>如图所示，每当窗口在源 DStream 上滑动时，落入窗口内的源 RDD 就会合并并进行操作，以生成窗口 DStream 的 RDD</p><p>在这种特定情况下，该操作将应用于数据的最后 3 个时间单位，并以 2 个时间单位滑动</p><p>这表明任何窗口操作都需要指定两个参数</p><ul><li><strong>窗口长度：</strong>窗口的持续时间</li><li><strong>滑动间隔：</strong>进行窗口操作的间隔</li></ul><h2 id="实例测试"><a href="#实例测试" class="headerlink" title="实例测试"></a>实例测试</h2><h3 id="1-统计到目前为止累计出现的单词的个数（需要保持住以前的状态）"><a href="#1-统计到目前为止累计出现的单词的个数（需要保持住以前的状态）" class="headerlink" title="1. 统计到目前为止累计出现的单词的个数（需要保持住以前的状态）"></a>1. 统计到目前为止累计出现的单词的个数（需要保持住以前的状态）</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/28 18:34</span></span><br><span class="line"><span class="comment"> * @Description: 使用 SparkStreaming 完成有状态统计</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StatefulWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 把当前的数据去更新已有的或者是老的数据</span></span><br><span class="line"><span class="comment">     * @param currentValues 当前的</span></span><br><span class="line"><span class="comment">     * @param preValues 老的</span></span><br><span class="line"><span class="comment">     * @return</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">updateFunction</span></span>(currentValues: <span class="type">Seq</span>[<span class="type">Int</span>], preValues: <span class="type">Option</span>[<span class="type">Int</span>]): <span class="type">Option</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">        <span class="keyword">val</span> current = currentValues.sum</span><br><span class="line">        <span class="keyword">val</span> pre = preValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">        <span class="type">Some</span>(current + pre)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"StatefulWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果是用了 stateful 的算子，必须要设置 checkpoint</span></span><br><span class="line">        <span class="comment">// 在生产环境中，建议把 checkpoint 设置到 HDFS 的某个文件夹中</span></span><br><span class="line">        ssc.checkpoint(<span class="string">"."</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"192.168.1.10"</span>, <span class="number">6789</span>)</span><br><span class="line">        <span class="keyword">val</span> result = lines.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">val</span> state = result.updateStateByKey[<span class="type">Int</span>](updateFunction _)</span><br><span class="line">        state.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-计算到目前为止累计出现的单词个数写入到-MySQL"><a href="#2-计算到目前为止累计出现的单词个数写入到-MySQL" class="headerlink" title="2. 计算到目前为止累计出现的单词个数写入到 MySQL"></a>2. 计算到目前为止累计出现的单词个数写入到 MySQL</h3><ul><li>使用 Spark Streaming 进行统计分析</li><li>Spark Streaming 统计结果写入到 MySQL</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/28 18:34</span></span><br><span class="line"><span class="comment"> * @Description: 使用 SparkStreaming 完成有词频统计并将结果写入到 MySQL 数据库中</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ForeachRDDApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取 MySQL 的连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * @return</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createConnection</span></span>(): <span class="type">Connection</span> = &#123;</span><br><span class="line">        <span class="type">Class</span>.forName(<span class="string">"com.mysql.cj.jdbc.Driver"</span>)</span><br><span class="line">        <span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://localhost:3306/imoocbootscala?characterEncoding=utf8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai"</span>, <span class="string">"root"</span>, <span class="string">"Chentyit123456"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"StatefulWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果是用了 stateful 的算子，必须要设置 checkpoint</span></span><br><span class="line">        <span class="comment">// 在生产环境中，建议把 checkpoint 设置到 HDFS 的某个文件夹中</span></span><br><span class="line">        ssc.checkpoint(<span class="string">"."</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"192.168.1.10"</span>, <span class="number">6789</span>)</span><br><span class="line">        <span class="keyword">val</span> result = lines.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">        result.print()</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 将结果写入到 MySQL</span></span><br><span class="line"><span class="comment">         * 这里的代码有两个问题：</span></span><br><span class="line"><span class="comment">         * 1. 对于已有的数据不会更新（改进方法：使用 Hbase 或者 Redis，再或者去数据库中查询，如果存在就更新，不存在就添加）</span></span><br><span class="line"><span class="comment">         * 2. 数据库连接没有使用连接池，会造成程序对资源的开销很大</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        result.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            rdd.foreachPartition(partitionOfRecords =&gt; &#123;</span><br><span class="line">                <span class="keyword">val</span> connection = createConnection()</span><br><span class="line">                partitionOfRecords.foreach(recode =&gt; &#123;</span><br><span class="line">                    <span class="keyword">val</span> sql = <span class="string">"insert into wordcount(word, wordcount) values('"</span> + recode._1 + <span class="string">"',"</span> + recode._2 + <span class="string">")"</span></span><br><span class="line">                    connection.createStatement().execute(sql)</span><br><span class="line">                &#125;)</span><br><span class="line">                connection.close()</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125;)</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-黑名单过滤"><a href="#3-黑名单过滤" class="headerlink" title="3. 黑名单过滤"></a>3. 黑名单过滤</h3><ul><li>transform 算子的使用</li><li>Spark Streaming 整合 RDD 进行操作</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/28 19:41</span></span><br><span class="line"><span class="comment"> * @Description: 黑名单过滤</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TransformApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"TransformApp"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 构建黑名单</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">val</span> blacks = <span class="type">List</span>(<span class="string">"zs"</span>, <span class="string">"ls"</span>)</span><br><span class="line">        <span class="keyword">val</span> blacksRDD = ssc.sparkContext.parallelize(blacks).map(x =&gt; (x, <span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"192.168.1.10"</span>, <span class="number">6789</span>)</span><br><span class="line">        lines.map(x =&gt; (x.split(<span class="string">","</span>)(<span class="number">1</span>), x)).transform(rdd =&gt; &#123;</span><br><span class="line">            rdd.leftOuterJoin(blacksRDD).filter(!_._2._2.getOrElse(<span class="literal">false</span>)).map(x=&gt;x._2._1)</span><br><span class="line">        &#125;).print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-Spring-Streaming-整合-Spark-SQL"><a href="#4-Spring-Streaming-整合-Spark-SQL" class="headerlink" title="4. Spring Streaming 整合 Spark SQL"></a>4. Spring Streaming 整合 Spark SQL</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/28 20:09</span></span><br><span class="line"><span class="comment"> * @Description: Spark Streaming 整合 Spark SQL 完成词频统计操作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SqlNetworkWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"SqlNetworkWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"192.168.1.10"</span>, <span class="number">6789</span>)</span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Convert RDDs of the words DStream to DataFrame and run SQL query</span></span><br><span class="line">        words.foreachRDD &#123; (rdd: <span class="type">RDD</span>[<span class="type">String</span>], time: <span class="type">Time</span>) =&gt;</span><br><span class="line">            <span class="keyword">val</span> spark = <span class="type">SparkSessionSingleton</span>.getInstance(rdd.sparkContext.getConf)</span><br><span class="line">            <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Convert RDD[String] to RDD[case class] to DataFrame</span></span><br><span class="line">            <span class="keyword">val</span> wordsDataFrame = rdd.map(w =&gt; <span class="type">Record</span>(w)).toDF()</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Creates a temporary view using the DataFrame</span></span><br><span class="line">            wordsDataFrame.createOrReplaceTempView(<span class="string">"words"</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Do word count on table using SQL and print it</span></span><br><span class="line">            <span class="keyword">val</span> wordCountsDataFrame =</span><br><span class="line">            spark.sql(<span class="string">"select word, count(*) as total from words group by word"</span>)</span><br><span class="line">            println(<span class="string">s"========= <span class="subst">$time</span> ========="</span>)</span><br><span class="line">            wordCountsDataFrame.show()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/** Case class for converting RDD to DataFrame */</span></span><br><span class="line">    <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">word: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="title">/**</span> <span class="title">Lazily</span> <span class="title">instantiated</span> <span class="title">singleton</span> <span class="title">instance</span> <span class="title">of</span> <span class="title">SparkSession</span> <span class="title">*/</span></span></span><br><span class="line"><span class="class">    <span class="title">object</span> <span class="title">SparkSessionSingleton</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 这个注解一般用于序列化的时候，标识某个字段不用被序列化</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">var</span> instance: <span class="type">SparkSession</span> = _</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">getInstance</span></span>(sparkConf: <span class="type">SparkConf</span>): <span class="type">SparkSession</span> = &#123;</span><br><span class="line">            <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">                instance = <span class="type">SparkSession</span></span><br><span class="line">                .builder</span><br><span class="line">                .config(sparkConf)</span><br><span class="line">                .getOrCreate()</span><br><span class="line">            &#125;</span><br><span class="line">            instance</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;带状态的算子（UpdateStateByKey）&lt;/li&gt;
&lt;li&gt;基于 window 的统计&lt;/li&gt;
&lt;li&gt;实例测试&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Anaconda四步安装多环境（鼠标点击安装）</title>
    <link href="https://www.chentyit.com/2019/09/28/Anaconda%E5%9B%9B%E6%AD%A5%E5%AE%89%E8%A3%85%E5%A4%9A%E7%8E%AF%E5%A2%83%EF%BC%88%E9%BC%A0%E6%A0%87%E7%82%B9%E5%87%BB%E5%AE%89%E8%A3%85%EF%BC%89/"/>
    <id>https://www.chentyit.com/2019/09/28/Anaconda四步安装多环境（鼠标点击安装）/</id>
    <published>2019-09-28T00:48:33.000Z</published>
    <updated>2019-09-28T00:56:45.347Z</updated>
    
    <content type="html"><![CDATA[<ol><li>打开 Anaconda 交互界面</li><li>点击 Environment</li><li>点击添加</li><li>点击完成</li></ol><a id="more"></a><p><img src="/2019/09/28/Anaconda四步安装多环境（鼠标点击安装）/1.png" alt="打开 Anaconda 交互界面"></p><p><img src="/2019/09/28/Anaconda四步安装多环境（鼠标点击安装）/2.png" alt="点击 Environment"></p><p><img src="/2019/09/28/Anaconda四步安装多环境（鼠标点击安装）/3.png" alt="点击添加"></p><p><img src="/2019/09/28/Anaconda四步安装多环境（鼠标点击安装）/4.png" alt="点击完成"></p>]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;打开 Anaconda 交互界面&lt;/li&gt;
&lt;li&gt;点击 Environment&lt;/li&gt;
&lt;li&gt;点击添加&lt;/li&gt;
&lt;li&gt;点击完成&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Python" scheme="https://www.chentyit.com/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>SparkStreaming 核心</title>
    <link href="https://www.chentyit.com/2019/09/27/SparkStreaming-%E6%A0%B8%E5%BF%83/"/>
    <id>https://www.chentyit.com/2019/09/27/SparkStreaming-核心/</id>
    <published>2019-09-27T12:05:38.000Z</published>
    <updated>2019-09-28T12:19:46.445Z</updated>
    
    <content type="html"><![CDATA[<ul><li>核心概念</li><li>代码实验</li></ul><a id="more"></a><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><p><a href="http://spark.apache.org/docs/2.2.0/streaming-programming-guide.html" target="_blank" rel="noopener">官网地址</a></p><h3 id="StreamingContext"><a href="#StreamingContext" class="headerlink" title="StreamingContext"></a>StreamingContext</h3><p>要初始化 Spark Streaming 程序，必须创建 StreamingContext 对象，该对象是所有 Spark Streaming 功能的主要入口点</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(appName).setMaster(master)</span><br><span class="line"><span class="comment">// Seconds：必须根据应用程序的延迟要求和可用的群集资源来设置批处理间隔</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p><strong>StreamingContext 源码</strong>构造方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a StreamingContext using an existing SparkContext.</span></span><br><span class="line"><span class="comment"> * @param sparkContext existing SparkContext</span></span><br><span class="line"><span class="comment"> * @param batchDuration the time interval at which streaming data will be divided into batches</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(sparkContext: <span class="type">SparkContext</span>, batchDuration: <span class="type">Duration</span>) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(sparkContext, <span class="literal">null</span>, batchDuration)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a StreamingContext by providing the configuration necessary for a new SparkContext.</span></span><br><span class="line"><span class="comment"> * @param conf a org.apache.spark.SparkConf object specifying Spark parameters</span></span><br><span class="line"><span class="comment"> * @param batchDuration the time interval at which streaming data will be divided into batches</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(conf: <span class="type">SparkConf</span>, batchDuration: <span class="type">Duration</span>) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(<span class="type">StreamingContext</span>.createNewSparkContext(conf), <span class="literal">null</span>, batchDuration)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>一旦 StreamingContext 定义好后，就可以做一些事情：（官网翻译过来的）</p><ol><li>通过创建输入 DStream 定义输入源</li><li>通过将转换和输出操作应用于 DStream 来定义流计算</li><li>开始使用 streamingContext.start() 接收数据并对其进行处理</li><li>等待使用 streamingContext.awaitTermination() 停止处理（手动或由于任何错误）</li><li>可以使用 streamingContext.stop() 手动停止处理</li></ol><p>要记住的要点：（官网翻译过来的）</p><ul><li>一旦启动上下文，就无法设置新的流计算或将其添加到该流计算中</li><li>上下文停止后，将无法重新启动</li><li>JVM 中只能同时激活一个 StreamingContext</li><li>StreamingContext 上的 stop() 也会停止 SparkContext，如要仅停止 StreamingContext，请将名为 stopSparkContext 的 stop() 的可选参数设置为 false</li><li>只要在创建下一个 StreamingContext 之前停止了上一个 StreamingContext（不停止 SparkContext），就可以重复使用 SparkContext 创建多个 StreamingContext</li></ul><h3 id="DStream（Discretized-Streams）"><a href="#DStream（Discretized-Streams）" class="headerlink" title="DStream（Discretized Streams）"></a>DStream（Discretized Streams）</h3><p>在内部，DStream 由一系列连续的 RDD 表示，这是 Spark 对不可变的分布式数据集的抽象</p><p>DStream中的每个RDD都包含来自特定间隔的数据，如下图所示</p><p><img src="http://spark.apache.org/docs/2.2.0/img/streaming-dstream.png" alt="DStream"></p><p>对 DStream 操作算子，比如 map / flatMap，其实底层会被翻译为对 DStream 中的每个 RDD 都做相同的工作，因为一个 DStream 是由不同批次的 RDD 所构成的</p><p><img src="http://spark.apache.org/docs/2.2.0/img/streaming-dstream-ops.png" alt="操作算子"></p><h3 id="Input-DStream"><a href="#Input-DStream" class="headerlink" title="Input DStream"></a>Input DStream</h3><p>Input DStream 是表示从流源接收的输入数据流的 DStream</p><p>每个输入 DStream （文件流除外） 都与一个Receiver对象（Scala doc，Java doc）相关联，该对象从源接收数据并将其存储在 Spark 的内存中以进行处理</p><p>在本地运行Spark Streaming程序时，请勿使用 “local” 或 “local [1]” 作为主URL。这两种方式均意味着仅一个线程将用于本地运行任务。如果您使用基于接收方的输入DStream（例如套接字，Kafka，Flume等），则将使用单个线程来运行接收方，而不会留下任何线程来处理接收到的数据。因此，在本地运行时，请始终使用 “local [n]” 作为主 URL，其中 n &gt; 要运行的接收器数</p><h2 id="代码实验"><a href="#代码实验" class="headerlink" title="代码实验"></a>代码实验</h2><ul><li><p>Spark Streaming 处理 socket 数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/27 19:32</span></span><br><span class="line"><span class="comment"> * @Description: Spark Streaming 处理 Socket 数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建 StreamingContext 需要两个参数：SparkConf 和 batch interval</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">        ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"192.168.1.10"</span>, <span class="number">6789</span>)</span><br><span class="line">        <span class="keyword">val</span> result = lines.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        result.print()</span><br><span class="line"></span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>Spark Streaming 处理 HDFS 文件数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @Author Chentyit</span></span><br><span class="line"><span class="comment"> * @Date 2019/9/27 19:46</span></span><br><span class="line"><span class="comment"> * @Description: 使用 Spark Streaming 处理文件系统的数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FileWordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"FileWordCount"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.sparkContext.setLogLevel(<span class="string">"OFF"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> line = ssc.textFileStream(<span class="string">"E:\\test\\"</span>)</span><br><span class="line">    <span class="keyword">val</span> result = line.flatMap(_.split(<span class="string">" "</span>)).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;核心概念&lt;/li&gt;
&lt;li&gt;代码实验&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>SparkStreaming入门</title>
    <link href="https://www.chentyit.com/2019/09/26/SparkStreaming%E5%85%A5%E9%97%A8/"/>
    <id>https://www.chentyit.com/2019/09/26/SparkStreaming入门/</id>
    <published>2019-09-26T12:01:16.000Z</published>
    <updated>2019-09-26T12:08:26.547Z</updated>
    
    <content type="html"><![CDATA[<ul><li>概述</li><li>应用场景</li><li>案例测试</li><li>工作原理</li></ul><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>将不同的数据源的数据经过 Spark Streaming 处理之后将结果输出到外部文件系统</p><p><strong>特点：</strong></p><ul><li>低延迟</li><li>能从错误中高效地恢复</li><li>能够运行在成百上千的节点</li><li>能够将批处理，机器学习，图计算等子框架和 Spark Streaming 综合起来使用</li></ul><p><img src="http://spark.apache.org/docs/2.2.0/img/streaming-flow.png" alt="streaming-flow"></p><p>输入进来的数据会被 Spark Streaming 处理成为 “批次”，然后由 Spark 引擎继续处理得到最终的数据流</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><ul><li>实时金融欺诈检测</li><li>实时访问电子传感器的检测</li><li>电商行业信息推荐</li></ul><h2 id="SparkStreaming-例子测试"><a href="#SparkStreaming-例子测试" class="headerlink" title="SparkStreaming 例子测试"></a>SparkStreaming 例子测试</h2><ol><li><p>spark-submit 提交（词频分析）</p><p>启动命令（监听 hadoop000 的 9999 端口）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./spark-submit --master local[2] --class org.apache.spark.examples.streaming.NetworkWordCount --name NetworkWordCount /home/hadoop/app/spark-2.2.0-bin-2.6.0-cdh5.7.0/examples/jars/spark-examples_2.11-2.2.0.jar hadoop000 9999</span><br></pre></td></tr></table></figure><p>官方测试代码：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">"Usage: NetworkWordCount &lt;hostname&gt; &lt;port&gt;"</span>)</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">StreamingExamples</span>.setStreamingLogLevels()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(args(<span class="number">0</span>), args(<span class="number">1</span>).toInt, <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">        wordCounts.print()</span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>spark-shell 提交（词频分析）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"hadoop000"</span>, <span class="number">9999</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = words.map(x =&gt; (x, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">wordCounts.print()</span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure></li></ol><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><h3 id="粗粒度"><a href="#粗粒度" class="headerlink" title="粗粒度"></a>粗粒度</h3><p>Spark Streaming 接收到实时数据流，把数据按照指定的时间段切成一片片小的数据块，然后把小的数据块传给 Spark Engine 处理</p><h3 id="细粒度"><a href="#细粒度" class="headerlink" title="细粒度"></a>细粒度</h3><p><img src="/2019/09/26/SparkStreaming入门/细粒度.png" alt="细粒度"></p><ol><li>Spark 应用程序运行在 Driver 端，应用程序中有 StreamingContext 和 SparkContext</li><li>Driver 命令在 Executor 上启动接收器</li><li>接收器启动后，将收到的数据拆分成 block 并存放到内存中，如果设置多副本就拷贝到其他机器中</li><li>Receiver 将 block 的信息返回给 StreamingContext，一定时间周期后，通知 SparkContext 启动 Jobs</li><li>SparkContext 将 Jobs 分发到 Executor 上执行</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;概述&lt;/li&gt;
&lt;li&gt;应用场景&lt;/li&gt;
&lt;li&gt;案例测试&lt;/li&gt;
&lt;li&gt;工作原理&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.chentyit.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>分布式消息队列 Kafka</title>
    <link href="https://www.chentyit.com/2019/09/25/%E5%88%86%E5%B8%83%E5%BC%8F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97-Kafka/"/>
    <id>https://www.chentyit.com/2019/09/25/分布式消息队列-Kafka/</id>
    <published>2019-09-25T08:45:01.000Z</published>
    <updated>2019-09-25T10:39:27.866Z</updated>
    
    <content type="html"><![CDATA[<ul><li>概述</li><li>下载</li><li>架构及核心概念</li><li>部署及使用</li><li>容错性测试</li><li>API 编程</li><li>Flume &amp; Kafka 整合</li><li>踩坑</li></ul><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><a href="http://kafka.apache.org/" target="_blank" rel="noopener">官网</a></p><ul><li>将数据流变成一个消息系统</li><li>高效处理数据流（近乎实时处理）</li><li>安全，多副本存储于分布式系统中</li></ul><p><em>消息中间件：生产者和消费者</em></p><h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p><a href="http://kafka.apache.org/" target="_blank" rel="noopener">官网</a></p><p>建议 0.8.0 到 0.10.1</p><p>0.8.0 —— 看老师使用过</p><p>0.9.0 —— 实验环境运行成功</p><p>0.10.1 —— 学习环境运行成功</p><ul><li>配置环境变量就可以开始运行</li></ul><h2 id="架构及核心概念"><a href="#架构及核心概念" class="headerlink" title="架构及核心概念"></a>架构及核心概念</h2><h3 id="基础组件"><a href="#基础组件" class="headerlink" title="基础组件"></a>基础组件</h3><ul><li>producer：生产者 —— 生产馒头</li><li>consumer：消费者 —— 吃馒头</li><li>broker：篮子</li><li>topic：主题，相当于馒头的标签，标志消费者吃那个标签下的馒头</li></ul><h3 id="基本架构"><a href="#基本架构" class="headerlink" title="基本架构"></a>基本架构</h3><ul><li><p>Kafka is run as a cluster on one or more servers that can span multiple datacenters.</p><p>（Kafka在一个或多个可以跨越多个数据中心的服务器上作为集群运行）</p></li><li><p>The Kafka cluster stores streams of <em>records</em> in categories called <em>topics</em>.</p><p>（Kafka群集将记录流存储在称为主题的类别中）</p></li><li><p>Each record consists of a key, a value, and a timestamp.</p><p>（每个记录由一个键，一个值和一个时间戳组成）</p></li></ul><h3 id="四个核心-API"><a href="#四个核心-API" class="headerlink" title="四个核心 API"></a>四个核心 API</h3><ul><li><strong>Producer API</strong> 发布消息到 1 和或多个 topic</li><li><strong>Consumer API</strong> 订阅一个或多个 topic，并处理产生的消息</li><li><strong>Stream API</strong> 充当一个流处理器，从 1 个或多个 topic 消费输出流，产生一个输出流到 1 个或多个输出 topic，有效将输入流转换到输出流</li><li><strong>Connector API</strong> 允许侯建或运行可重复使用的生产者或消费者，将 topic 连接到现有的应用程序或数据系统。</li></ul><h3 id="主题和日志"><a href="#主题和日志" class="headerlink" title="主题和日志"></a>主题和日志</h3><p><strong>Topic</strong> 是发布的消息或者种子的名字。对于每个 Topic，Kafka 集群维护这一个分区的 log</p><p>每个分区都是一个有序不可变的队列，且可以持续添加<strong>（只是局部有序，全局无序，如果要全局有序就只能有一个分区）</strong></p><p>分区中以唯一的偏移量标记每个消息</p><p>消费者持有和操作的都是偏移值，好处是不会影响到其他消费者，也更加自由灵活读取消息</p><p><strong>分区设计目的</strong></p><ul><li>处理更多消息，不受单台服务器的限制</li><li>分区可以作为并行处理单元</li></ul><h3 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h3><p>Log 的分区被分布式到集群中的多个服务器上，每个服务器处理它分到的分区。根据配置每个分区，还可以复制到其他服务器作为<strong>备份容错</strong>（下面有测试）</p><p> 每个分区有一个 leader，0 个或多个 follower，Leader 处理此分区的读写请求，follower 被动复制数据，如果 leader 宕机，follower 被推举为新 leader（下面测试有体现）</p><p>一个 leader 也有可能是其他分区的 follower，目的是为了负载均衡</p><h3 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h3><p>向某个 Topic 发布消息，也负责选择发布到 Topic 上的哪个分区</p><p>选择算法：轮流选择，权重选择等（由开发者决定）</p><h3 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h3><p>消费模型有两种：<strong>队列</strong>和<strong>发布-订阅</strong></p><ul><li>队列：一组消费者从服务器读取消息，一条消息只有其中一个消费者处理</li><li>发布-订阅：消息被广播给所有消费者，接收到消息的消费者都可以处理此消息</li></ul><h3 id="消费者组"><a href="#消费者组" class="headerlink" title="消费者组"></a>消费者组</h3><p>Kafka 为消费者模型提供的单一消费者凑相关模型</p><p>所有消费者在一个组中就是队列模型</p><p>不在一个组中就是发布-订阅模型</p><h2 id="部署及使用"><a href="#部署及使用" class="headerlink" title="部署及使用"></a>部署及使用</h2><ul><li><p>单节点单 Broker 部署及使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># 配置 $KAFKA_HOME/config/server.properties 下的文件</span><br><span class="line">broker.id=0</span><br><span class="line">listeners</span><br><span class="line">log.dirs（不能是tmp，重启之后会消失）</span><br><span class="line">zookeeper.connect</span><br><span class="line"></span><br><span class="line"># 启动 Kafka</span><br><span class="line">kafka-server-start.sh config/server.properties</span><br><span class="line"></span><br><span class="line"># 创建 topic</span><br><span class="line">kafka-topics.sh --create --zookeeper hadoop000:2181 --replication-factor 1 --partitions 1 --topic cty_topic</span><br><span class="line"></span><br><span class="line"># 查看所有 topic</span><br><span class="line">kafka-topics.sh --list --zookeeper hadoop000:2181</span><br><span class="line"></span><br><span class="line"># 发送信息（生产馒头）</span><br><span class="line">kafka-console-producer.sh --broker-list hadoop000:9092 --topic cty_topic</span><br><span class="line"></span><br><span class="line"># 消费消息（吃馒头）</span><br><span class="line"># 写 --from-beginning（从第一个馒头开始吃）</span><br><span class="line"># 不写就是从新蒸好的馒头开始吃</span><br><span class="line">kafka-console-consumer.sh --zookeeper hadoop000:2181 --topic cty_topic --from-beginning</span><br><span class="line"></span><br><span class="line"># 查看所有 topic 的详细信息</span><br><span class="line">kafka-topics.sh --describe --zookeeper hadoop000:2181</span><br><span class="line"># 查看指定 topic 的详细信息</span><br><span class="line">kafka-topics.sh --describe --zookeeper hadoop000:2181 --topic cty_topic</span><br></pre></td></tr></table></figure></li><li><p>单节点多 Broker 部署及使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"># 这是官网复制的</span><br><span class="line"># 在同一台主机上部署多 Broker（按需求改）</span><br><span class="line">config/server-1.properties:</span><br><span class="line">    broker.id=1</span><br><span class="line">    port=9093</span><br><span class="line">    log.dir=/tmp/kafka-logs-1</span><br><span class="line"></span><br><span class="line">config/server-2.properties:</span><br><span class="line">    broker.id=2</span><br><span class="line">    port=9094</span><br><span class="line">    log.dir=/tmp/kafka-logs-2</span><br><span class="line"></span><br><span class="line"># 启动多个 Broker</span><br><span class="line"># daemon（守护进程）</span><br><span class="line">kafka-server-start.sh -daemon config/server-1.properties &amp;</span><br><span class="line">kafka-server-start.sh -daemon config/server-2.properties &amp;</span><br><span class="line"></span><br><span class="line"># jps（这里是启动了三个）</span><br><span class="line">22915 Kafka</span><br><span class="line">23109 Jps</span><br><span class="line">22981 Kafka</span><br><span class="line">21829 QuorumPeerMain</span><br><span class="line">23048 Kafka</span><br><span class="line"></span><br><span class="line"># 创建 topic</span><br><span class="line">kafka-topics.sh --create --zookeeper hadoop000:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic-cty</span><br><span class="line"></span><br><span class="line"># 查看 topic 详细信息</span><br><span class="line">kafka-topics.sh --describe --zookeeper hadoop000:2181 --topic my-replicated-topic-cty</span><br><span class="line"># PartitionCount 分区数</span><br><span class="line"># ReplicationFactor 副本数</span><br><span class="line"># Leader 领导者</span><br><span class="line"># Replicas 副本顺序</span><br><span class="line"># Isr 存活节点</span><br><span class="line">Topic:my-replicated-topic-ctyPartitionCount:1ReplicationFactor:3Configs:</span><br><span class="line">Topic: my-replicated-topic-ctyPartition: 0Leader: 1Replicas: 1,3,2Isr: 1,3,2</span><br></pre></td></tr></table></figure></li><li><p>多节点多 Broker 部署及使用（使用方法和<strong>单节点多 Broker</strong> 一样只不过分发到其他机器上了）</p></li></ul><h2 id="容错性测试"><a href="#容错性测试" class="headerlink" title="容错性测试"></a>容错性测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 先删除一个 Broker 查看是否还能继续生产馒头和吃馒头</span><br><span class="line"># 使用 kill -9 pid 强制关闭一个 Broker</span><br><span class="line">[hadoop@hadoop000 kafka_2.11-0.9.0.0]$ jps -m</span><br><span class="line">22915 Kafka config/server-1.properties</span><br><span class="line">22981 Kafka config/server-2.properties</span><br><span class="line">23048 Kafka config/server-3.properties</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 kafka_2.11-0.9.0.0]$ kull -9 22981</span><br><span class="line"></span><br><span class="line">[hadoop@hadoop000 kafka_2.11-0.9.0.0]$ jps -m</span><br><span class="line">22915 Kafka config/server-1.properties</span><br><span class="line">23048 Kafka config/server-3.properties</span><br><span class="line"></span><br><span class="line"># 查看 topic 的信息（还剩 1 和 3 存活，且能继续生产馒头和吃馒头）</span><br><span class="line">Topic:my-replicated-topic-ctyPartitionCount:1ReplicationFactor:3Configs:</span><br><span class="line">Topic: my-replicated-topic-ctyPartition: 0Leader: 1Replicas: 1,3,2Isr: 1,3</span><br><span class="line"></span><br><span class="line"># 让 Leader 嗝屁（还是能继续生产馒头和吃馒头）</span><br><span class="line"># 注意：主节点嗝屁后，消费者会去找新的主节点，找的过程中会有报错</span><br><span class="line"># 就是找不到蒸馒头的人了，等新人一来就又继续吃了</span><br><span class="line">Topic:my-replicated-topic-ctyPartitionCount:1ReplicationFactor:3Configs:</span><br><span class="line">Topic: my-replicated-topic-ctyPartition: 0Leader: 3Replicas: 1,3,2Isr: 3</span><br></pre></td></tr></table></figure><h2 id="API-编程"><a href="#API-编程" class="headerlink" title="API 编程"></a>API 编程</h2><p><strong>KafkaProperties.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/25 14:54</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: 设置一些参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProperties</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String ZK = <span class="string">"192.168.43.169:2181"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String TOPIC = <span class="string">"cty_topic"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String BROKER_LIST = <span class="string">"192.168.43.169:9092"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String GROUP_ID = <span class="string">"test_group1"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>KafkaProducer.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/25 14:57</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: Kafka 生产者</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducer</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String topic;</span><br><span class="line">    <span class="keyword">private</span> Producer&lt;Integer, String&gt; producer;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">KafkaProducer</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topic = topic;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 用于保存参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置参数</span></span><br><span class="line">        properties.put(<span class="string">"metadata.broker.list"</span>,KafkaProperties.BROKER_LIST);</span><br><span class="line">        properties.put(<span class="string">"serializer.class"</span>,<span class="string">"kafka.serializer.StringEncoder"</span>);</span><br><span class="line">        properties.put(<span class="string">"request.required.acks"</span>,<span class="string">"1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 实例化一个生产者</span></span><br><span class="line">        producer = <span class="keyword">new</span> Producer&lt;Integer, String&gt;(<span class="keyword">new</span> ProducerConfig(properties));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 发送的数据</span></span><br><span class="line">        <span class="keyword">int</span> messageNo = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="comment">// 构建发送的数据</span></span><br><span class="line">            String message = <span class="string">"message_"</span> + messageNo;</span><br><span class="line">            <span class="comment">// 发送消息</span></span><br><span class="line">            producer.send(<span class="keyword">new</span> KeyedMessage&lt;Integer, String&gt;(topic, message));</span><br><span class="line">            <span class="comment">// 打印消息</span></span><br><span class="line">            System.out.println(<span class="string">"Sent: "</span> + message);</span><br><span class="line">            messageNo ++ ;</span><br><span class="line">            <span class="keyword">try</span>&#123;</span><br><span class="line">                Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>KafkaConsumer.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/25 15:45</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: 消费者（吃馒头）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumer</span> <span class="keyword">extends</span> <span class="title">Thread</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String topic;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">KafkaConsumer</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.topic = topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建连接器</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> ConsumerConnector <span class="title">createConnector</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 用于保存参数</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置参数</span></span><br><span class="line">        <span class="comment">// 不设置 group.id 会报错</span></span><br><span class="line">        properties.put(<span class="string">"zookeeper.connect"</span>, KafkaProperties.ZK);</span><br><span class="line">        properties.put(<span class="string">"group.id"</span>, KafkaProperties.GROUP_ID);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回一个消费者</span></span><br><span class="line">        <span class="keyword">return</span> Consumer.createJavaConsumerConnector(<span class="keyword">new</span> ConsumerConfig(properties));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 获取一个消费者实例</span></span><br><span class="line">        ConsumerConnector consumer = createConnector();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 保存 topic</span></span><br><span class="line">        Map&lt;String, Integer&gt; topicCountMap = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">16</span>);</span><br><span class="line">        topicCountMap.put(topic, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 第一个 String：topic</span></span><br><span class="line">        <span class="comment">// List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; 对应的数据流</span></span><br><span class="line">        Map&lt;String, List&lt;KafkaStream&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt;&gt;&gt; messageStream = consumer.createMessageStreams(topicCountMap);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取我们每次接受到的数据</span></span><br><span class="line">        KafkaStream&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; stream = messageStream.get(topic).get(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 使用迭代器获取数据</span></span><br><span class="line">        <span class="keyword">for</span> (kafka.message.MessageAndMetadata&lt;<span class="keyword">byte</span>[], <span class="keyword">byte</span>[]&gt; messageAndMetadata : stream) &#123;</span><br><span class="line">            String message = <span class="keyword">new</span> String(messageAndMetadata.message());</span><br><span class="line">            System.out.println(<span class="string">"rec:"</span> + message);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>KafkaClientApp.java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/25 15:12</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaClientApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 启动两个线程</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">new</span> KafkaProducer(KafkaProperties.TOPIC).start();</span><br><span class="line">        <span class="keyword">new</span> KafkaConsumer(KafkaProperties.TOPIC).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Flume-amp-Kafka-整合"><a href="#Flume-amp-Kafka-整合" class="headerlink" title="Flume &amp; Kafka 整合"></a>Flume &amp; Kafka 整合</h2><p><img src="/2019/09/25/分布式消息队列-Kafka/Flume&amp;Kafka.png" alt="Flume&amp;Kafka"></p><p><strong>avro-memory-kafka.conf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">avro-memory-kafka.sources = avro-source</span><br><span class="line">avro-memory-kafka.sinks = kafka-sink</span><br><span class="line">avro-memory-kafka.channels = memory-channel</span><br><span class="line"></span><br><span class="line">avro-memory-kafka.sources.avro-source.type = avro</span><br><span class="line">avro-memory-kafka.sources.avro-source.bind = hadoop000</span><br><span class="line">avro-memory-kafka.sources.avro-source.port = 44444</span><br><span class="line"></span><br><span class="line">avro-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">avro-memory-kafka.sinks.kafka-sink.brokerList = hadoop000:9092</span><br><span class="line">avro-memory-kafka.sinks.kafka-sink.topic = cty_topic</span><br><span class="line">avro-memory-kafka.sinks.kafka-sink.batchSize = 5</span><br><span class="line">avro-memory-kafka.sinks.kafka-sink.requiredAcks = 1</span><br><span class="line"></span><br><span class="line">avro-memory-kafka.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">avro-memory-kafka.sources.avro-source.channels = memory-channel</span><br><span class="line">avro-memory-kafka.sinks.kafka-sink.channel = memory-channel</span><br></pre></td></tr></table></figure><p><strong>avro-memory-logger.conf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">avro-memory-logger.sources = avro-source</span><br><span class="line">avro-memory-logger.sinks = logger-sink</span><br><span class="line">avro-memory-logger.channels = memory-channel</span><br><span class="line"></span><br><span class="line">avro-memory-logger.sources.avro-source.type = avro</span><br><span class="line">avro-memory-logger.sources.avro-source.bind = hadoop000</span><br><span class="line">avro-memory-logger.sources.avro-source.port = 44444</span><br><span class="line"></span><br><span class="line">avro-memory-logger.sinks.logger-sink.type = logger</span><br><span class="line"></span><br><span class="line">avro-memory-logger.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">avro-memory-logger.sources.avro-source.channels = memory-channel</span><br><span class="line">avro-memory-logger.sinks.logger-sink.channel = memory-channel</span><br></pre></td></tr></table></figure><p><strong>先启动 avro-memory-kafka.conf</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name avro-memory-kafka  \</span><br><span class="line">--conf $FLUME_HOME/conf  \</span><br><span class="line">--conf-file $FLUME_HOME/conf/avro-memory-kafka.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p><strong>再启动 exec-memory-avro</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name exec-memory-avro  \</span><br><span class="line">--conf $FLUME_HOME/conf  \</span><br><span class="line">--conf-file $FLUME_HOME/conf/exec-memory-avro.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p><strong>最后启动 Kafka</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure><p><strong>在终端查看消费者信息</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh --zookeeper hadoop000:2181 --topic cty_topic</span><br></pre></td></tr></table></figure><h2 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h2><ol><li><p>在 server.properties 中的 listeners 用于外网访问 Kafka 集群，如果需要在本地调试传消息到 Kafka 集群，就需要这个参数，而且 listeners 的参数必须规范，后面的 IPv4 地址必须要按照规范来，不能用主机名代替，不然外部无法连接，设置规范</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listeners=PLAINTEXT://192.168.43.169:9092</span><br></pre></td></tr></table></figure></li><li><p>Flume 下沉到 Kafka 需要看官网，下沉的版本号有要求，比如 flume1.6 只支持 kafka 0.9 以上版本</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;概述&lt;/li&gt;
&lt;li&gt;下载&lt;/li&gt;
&lt;li&gt;架构及核心概念&lt;/li&gt;
&lt;li&gt;部署及使用&lt;/li&gt;
&lt;li&gt;容错性测试&lt;/li&gt;
&lt;li&gt;API 编程&lt;/li&gt;
&lt;li&gt;Flume &amp;amp; Kafka 整合&lt;/li&gt;
&lt;li&gt;踩坑&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Kafka" scheme="https://www.chentyit.com/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>分布式日志收集框架 Flume</title>
    <link href="https://www.chentyit.com/2019/09/25/%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E6%A1%86%E6%9E%B6-Flume/"/>
    <id>https://www.chentyit.com/2019/09/25/分布式日志收集框架-Flume/</id>
    <published>2019-09-25T03:20:35.000Z</published>
    <updated>2019-09-25T03:22:25.189Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Flume 概述</li><li>安装</li><li>Flume 架构及核心组件</li><li>Flume 配置文件描述</li><li>监控一个文件实时采集新增的数据输出到控制台</li><li>跨服务采集日志</li></ul><a id="more"></a><h2 id="Flume-概述"><a href="#Flume-概述" class="headerlink" title="Flume 概述"></a>Flume 概述</h2><p><a href="http://flume.apache.org/" target="_blank" rel="noopener">官网</a></p><p>FLume 是有 Cloudera 提供的一个<strong>分布式，高可靠，高可用</strong>的服务，用于分布式的海量日志的高效收集、聚合、移动系统</p><p>设计目标：可靠性，扩展性，管理性</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul><li><p>安装 JDK</p></li><li><p>下载 flume 安装包</p></li><li><p>解压安装包</p></li><li><p>更改配置文件</p><p>修改 conf 下的 flume-env.sh.template 复制一份到 flume-env.sh 并修改文件中的 JAVA_HOME 为 jdk 的真实路径</p></li></ul><h2 id="Flume-架构及核心组件"><a href="#Flume-架构及核心组件" class="headerlink" title="Flume 架构及核心组件"></a>Flume 架构及核心组件</h2><ul><li><p>Source：收集</p><p>指定数据的来源</p></li><li><p>Channel：聚集</p><p>为数据提供一个临时缓存的地方</p></li><li><p>Sink：输出</p><p>从 Channel 中将数据读取出来，输出到指定位置</p></li></ul><h2 id="Flume-配置文件描述"><a href="#Flume-配置文件描述" class="headerlink" title="Flume 配置文件描述"></a>Flume 配置文件描述</h2><ul><li>a1：agent 名称</li><li>r1：source 的名称</li><li>k1：sink 的名称</li><li>c1：channle 的名称</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line"># 组件类型为 netcat</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line"># 要绑定的主机名或IP地址</span><br><span class="line">a1.sources.r1.bind = hadoop000</span><br><span class="line"># 要绑定的端口号</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line"># 需要记录组件类型名称</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line"># 组件类型名称，必须是内存</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"># 通道中存储的最大事件数</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line"># 每次通道从源或汇给接收器的最大事件数</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><p><strong>启动命令</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent --name a1 --conf conf --conf-file conf/example.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p><strong>接收到的消息：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Event: &#123; headers:&#123;&#125; body: 68 65 6C 6C 6F 0D     hello. &#125;</span><br></pre></td></tr></table></figure><p>Event 是 Flume 数据传输的基本单元</p><p>Event = 可选的 header + byte array</p><h2 id="监控一个文件实时采集新增的数据输出到控制台"><a href="#监控一个文件实时采集新增的数据输出到控制台" class="headerlink" title="监控一个文件实时采集新增的数据输出到控制台"></a>监控一个文件实时采集新增的数据输出到控制台</h2><p><strong>Agent 选型：</strong>exec source + Memory Channel + Logger Sink</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line"># 组件类型名称为 exec </span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /home/hadoop/data/data.log</span><br><span class="line">a1.sources.r1.shell = /bin/sh -c</span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line"># 需要记录组件类型名称</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line"># 组件类型名称，必须是内存</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line"># 通道中存储的最大事件数</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line"># 每次通道从源或汇给接收器的最大事件数</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><h2 id="跨服务采集日志"><a href="#跨服务采集日志" class="headerlink" title="跨服务采集日志"></a>跨服务采集日志</h2><p><strong>技术选型：</strong></p><ul><li>exec source + memory channel + avro sink</li><li>avro source + memory channel + logger sink</li></ul><p><strong>exec-memory-avro.conf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">exec-memory-avro.sources = exec-source</span><br><span class="line">exec-memory-avro.sinks = avro-sink</span><br><span class="line">exec-memory-avro.channels = memory-channel</span><br><span class="line"></span><br><span class="line">exec-memory-avro.sources.exec-source.type = exec</span><br><span class="line">exec-memory-avro.sources.exec-source.command = tail -F /home/hadoop/data/data.log</span><br><span class="line">exec-memory-avro.sources.exec-source.shell = /bin/sh -c</span><br><span class="line"></span><br><span class="line">exec-memory-avro.sinks.avro-sink.type = avro</span><br><span class="line">exec-memory-avro.sinks.avro-sink.hostname = hadoop000</span><br><span class="line">exec-memory-avro.sinks.avro-sink.port = 44444</span><br><span class="line"></span><br><span class="line">exec-memory-avro.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">exec-memory-avro.sources.exec-source.channels = memory-channel</span><br><span class="line">exec-memory-avro.sinks.avro-sink.channel = memory-channel</span><br></pre></td></tr></table></figure><p><strong>avro-memory-logger.conf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">avro-memory-logger.sources = avro-source</span><br><span class="line">avro-memory-logger.sinks = logger-sink</span><br><span class="line">avro-memory-logger.channels = memory-channel</span><br><span class="line"></span><br><span class="line">avro-memory-logger.sources.avro-source.type = avro</span><br><span class="line">avro-memory-logger.sources.avro-source.bind = hadoop000</span><br><span class="line">avro-memory-logger.sources.avro-source.port = 44444</span><br><span class="line"></span><br><span class="line">avro-memory-logger.sinks.logger-sink.type = logger</span><br><span class="line"></span><br><span class="line">avro-memory-logger.channels.memory-channel.type = memory</span><br><span class="line"></span><br><span class="line">avro-memory-logger.sources.avro-source.channels = memory-channel</span><br><span class="line">avro-memory-logger.sinks.logger-sink.channel = memory-channel</span><br></pre></td></tr></table></figure><p><strong>先启动 avro-memory-logger</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name avro-memory-logger  \</span><br><span class="line">--conf $FLUME_HOME/conf  \</span><br><span class="line">--conf-file $FLUME_HOME/conf/avro-memory-logger.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p><strong>再启动 exec-memory-avro</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent \</span><br><span class="line">--name exec-memory-avro  \</span><br><span class="line">--conf $FLUME_HOME/conf  \</span><br><span class="line">--conf-file $FLUME_HOME/conf/exec-memory-avro.conf \</span><br><span class="line">-Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Flume 概述&lt;/li&gt;
&lt;li&gt;安装&lt;/li&gt;
&lt;li&gt;Flume 架构及核心组件&lt;/li&gt;
&lt;li&gt;Flume 配置文件描述&lt;/li&gt;
&lt;li&gt;监控一个文件实时采集新增的数据输出到控制台&lt;/li&gt;
&lt;li&gt;跨服务采集日志&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flume" scheme="https://www.chentyit.com/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Java面试题02</title>
    <link href="https://www.chentyit.com/2019/09/24/Java%E9%9D%A2%E8%AF%95%E9%A2%9802/"/>
    <id>https://www.chentyit.com/2019/09/24/Java面试题02/</id>
    <published>2019-09-24T00:18:18.000Z</published>
    <updated>2019-09-29T02:43:47.300Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Java 多线程模块</li><li>Java 反射模块</li><li>Java 对象拷贝模块</li><li>Java 异常模块</li><li>网络模块</li><li>设计模式模块</li><li>Spring / Spring MVC 模块</li></ul><p>题库来源于 <a href="https://www.javazhiyin.com/42272.html" target="_blank" rel="noopener">Java知音</a></p><a id="more"></a><h2 id="Java多线程模块"><a href="#Java多线程模块" class="headerlink" title="Java多线程模块"></a>Java多线程模块</h2><h3 id="51-ThreadLocal-是什么？有哪些使用场景"><a href="#51-ThreadLocal-是什么？有哪些使用场景" class="headerlink" title="51. ThreadLocal 是什么？有哪些使用场景"></a>51. ThreadLocal 是什么？有哪些使用场景</h3><p>ThreadLocal 为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本而不会影响其它线程所对应的的副本</p><p>ThreadLocal 使用场景：数据库连接和 session 管理等</p><h3 id="52-synchronized-底层实现原理"><a href="#52-synchronized-底层实现原理" class="headerlink" title="52. synchronized 底层实现原理"></a>52. synchronized 底层实现原理</h3><p>synchronized 是由一对 monitorenter / monitorexit 指令实现的，monitor 对象时同步的基本实现单元，在 Java 6 之前，monitor 的实现完全是依靠操作系统内部的互斥锁，因为需要进行用户态到内核态的切换，所以同步操作是一个无差别的重量级操作，性能也很低。</p><p>Java 6 的时候，JVM 虚拟机提供了三种不同的 monitor 实现： 偏向锁，轻量级锁和重量级锁，用于改进其性能</p><h3 id="53-synchronized-和-volatile-的区别是什么？"><a href="#53-synchronized-和-volatile-的区别是什么？" class="headerlink" title="53.synchronized 和 volatile 的区别是什么？"></a>53.synchronized 和 volatile 的区别是什么？</h3><ul><li>volatile 是<strong>变量</strong>修饰符；synchronized 是修饰<strong>类、方法、代码块</strong></li><li>volatile 仅能实现变量的修改可见性，不能保证原子性；synchronized 则可以保证变量的修改可见性和原子性</li><li>volatile 不会造成线程的阻塞；synchronized 可能会造成线程的阻塞</li></ul><h3 id="54-synchronized-和-Lock-有什么区别？"><a href="#54-synchronized-和-Lock-有什么区别？" class="headerlink" title="54. synchronized 和 Lock 有什么区别？"></a>54. synchronized 和 Lock 有什么区别？</h3><ul><li>synchronized 可以给类、方法、代码块加锁；Lock 只能给代码块加锁</li><li>synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；Lock 需要自己加锁和释放锁，如果使用不当，没有 unLock() 释放锁，就会造成死锁</li><li>通过 Lock 可以指导有没有成功获取锁，synchronized 无法知道</li></ul><h3 id="55-synchronized-和-ReentrantLock-区别是什么？"><a href="#55-synchronized-和-ReentrantLock-区别是什么？" class="headerlink" title="55. synchronized 和 ReentrantLock 区别是什么？"></a>55. synchronized 和 ReentrantLock 区别是什么？</h3><ul><li>ReentrantLock 使用起来比较灵活，但是必须有释放锁配合动作</li><li>ReentrantLock 必须手动获取与释放锁；synchronized 不需要手动释放和开启锁</li><li>ReentrantLock 只使用于代码块锁；synchronized 可用于修饰方法、代码块等</li><li>ReentrantLock 标记的变量不会被编译器优化；synchronized 标记的变量可以被编译器优化</li></ul><h3 id="56-atomic-原理"><a href="#56-atomic-原理" class="headerlink" title="56. atomic 原理"></a>56. atomic 原理</h3><p>atomic 主要利用 CAS（Compare And Swap）和 volatile 和 native 方法俩保证原子操作，从而避免 synchronized 的高开销，执行效率提升</p><h2 id="Java-反射模块"><a href="#Java-反射模块" class="headerlink" title="Java 反射模块"></a>Java 反射模块</h2><h3 id="57-什么是反射？"><a href="#57-什么是反射？" class="headerlink" title="57.什么是反射？"></a>57.什么是反射？</h3><p>反射是在运行状态中，对任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 Java 语言的反射机制</p><h3 id="58-什么是-Java-序列化？什么情况下需要序列化？"><a href="#58-什么是-Java-序列化？什么情况下需要序列化？" class="headerlink" title="58. 什么是 Java 序列化？什么情况下需要序列化？"></a>58. 什么是 Java 序列化？什么情况下需要序列化？</h3><p>Java 序列化是为了保存各种对象在内存中的状态，并且可以把保存的对象状态再读出来</p><ul><li>想把内存中的对象保存到一个文件中或者数据库中的时候</li><li>想用套接字在网络上传送对象的时候</li><li>想通过 RMI（远程方法调用）传输对象的时候</li></ul><h3 id="59-动态代理是什么？有哪些应用？"><a href="#59-动态代理是什么？有哪些应用？" class="headerlink" title="59. 动态代理是什么？有哪些应用？"></a>59. 动态代理是什么？有哪些应用？</h3><p>动态代理是运行时动态生成的代理类</p><p>动态代理的应用有 spring aop，hibernate 数据查询，RPC，Java 注解对象获取等</p><h3 id="60-怎么实现动态代理？"><a href="#60-怎么实现动态代理？" class="headerlink" title="60. 怎么实现动态代理？"></a>60. 怎么实现动态代理？</h3><p>JDK 原生动态代理和 cglib 动态代理：</p><ul><li>JDK 原生动态代理是基于接口实现的</li><li>cglib 是基于继承当前类的子类实现的</li></ul><h2 id="Java-对象拷贝模块"><a href="#Java-对象拷贝模块" class="headerlink" title="Java 对象拷贝模块"></a>Java 对象拷贝模块</h2><h3 id="61-为什么要试用克隆？"><a href="#61-为什么要试用克隆？" class="headerlink" title="61. 为什么要试用克隆？"></a>61. 为什么要试用克隆？</h3><p>克隆的对象可能包含一些已经修改过的属性，而 new 出来的对象的属性都还是初始化时候的值，所以当需要一个新的对象来保存当前对象的 “状态” 就靠克隆方法了</p><h3 id="62-如何实现对象克隆？"><a href="#62-如何实现对象克隆？" class="headerlink" title="62. 如何实现对象克隆？"></a>62. 如何实现对象克隆？</h3><ul><li>实现 Cloneable 接口并重写 Object 类的 clone 方法</li><li>实现 Serializable 接口，通过对象的序列化和反序列化实现克隆，可以实现真正的深度克隆</li></ul><h3 id="63-深拷贝和浅拷贝的区别是什么？"><a href="#63-深拷贝和浅拷贝的区别是什么？" class="headerlink" title="63. 深拷贝和浅拷贝的区别是什么？"></a>63. 深拷贝和浅拷贝的区别是什么？</h3><ul><li>浅拷贝：当对象被复制时，复制他本身和其中包含的值类型的成员变量，而引用类型的成员对象并没有复制</li><li>深拷贝：除了对象本身被复制外，对象所包含的所有成员变量也将复制</li></ul><h2 id="Java-Web-模块"><a href="#Java-Web-模块" class="headerlink" title="Java Web 模块"></a>Java Web 模块</h2><h3 id="64-JSP-和-servlet-有什么区别？"><a href="#64-JSP-和-servlet-有什么区别？" class="headerlink" title="64. JSP 和 servlet 有什么区别？"></a>64. JSP 和 servlet 有什么区别？</h3><p>JSP 是 servlet 技术的扩展，本质就是 servlet 的简易方式。servlet 和 JSP 最主要的不通点在于，servlet 的应用逻辑是在 Java 文件中，并且完全从表现层中的 html 中分离开来，而 JSP 的情况是 Java 和 html 可以组合成一个扩展名为 JSP 的文件。JSP 侧重于视图，servlet 侧重于控制逻辑。</p><h3 id="65-JSP-的-9-大内置对象"><a href="#65-JSP-的-9-大内置对象" class="headerlink" title="65. JSP 的 9 大内置对象"></a>65. JSP 的 9 大内置对象</h3><ul><li>request：封装客户端的请求，其中包含来自 get 或 post 请求的参数</li><li>response：封装服务器对客户端的响应</li><li>pageContext：通过该对象可以获取其他对象</li><li>session：封装用户会话的对象</li><li>application：封装服务器运行环境的对象</li><li>out：输出服务器响应的输出流对象</li><li>config：Web 应用的配置对象</li><li>page：JSP 页面本身（相当于 Java 程序中的 this）</li><li>exception：封装页面抛出异常的对象</li></ul><h3 id="66-JSP-的-4-中作用域"><a href="#66-JSP-的-4-中作用域" class="headerlink" title="66. JSP 的 4 中作用域"></a>66. JSP 的 4 中作用域</h3><ul><li>page：代表与一个页面相关的对象和属性</li><li>request：代表与客户端发出的一个请求相关的对象和属性</li><li>session：代表与某个用户与服务器简历的一次会话相关的对象和属性</li><li>application：代表与整个 Web 应用程序相关的对象和属性</li></ul><h3 id="67-session-和-cookie-有什么区别-？"><a href="#67-session-和-cookie-有什么区别-？" class="headerlink" title="67. session 和 cookie 有什么区别 ？"></a>67. session 和 cookie 有什么区别 ？</h3><p><strong>session：</strong>是一种将会话状态保存到服务器端的技术</p><p><strong>cookie：</strong>是在 HTTP 协议下，Web 服务器保存在用户浏览器（客户端）上的小文本文件，可以包含有关用户信息。无论何时用户连接得到服务器，Web 站点都可以访问 Cookie 信息</p><ul><li><strong>存储位置不同：</strong>session 存储在服务器端；cookie 存储在浏览器端</li><li><strong>安全性不同：</strong>cookie 安全性一般，可以被伪造和修改</li><li><strong>容量和个数不同：</strong>cookie 有容量限制，每个站点下的 cookie 也有个数限制</li><li><strong>存储的多样性：</strong>session 可以存储在 Redis 中，数据库中，应用程序中；cookie 只能存储在浏览器中</li></ul><h3 id="68-session-工作原理"><a href="#68-session-工作原理" class="headerlink" title="68. session 工作原理"></a>68. session 工作原理</h3><p>session 是客户端登录完成之后，服务器会创建对应的 session，创建完成之后，会把 session 的 id 发送给客户端，客户端再存储到浏览器中。</p><p>客户端每次访问服务器的时候，都会带着 sessionid，服务器拿到 session 之后，在内存中找到与之对应的 session 就可以正常工作了</p><h3 id="69-如果客户端禁止-cookie-能实现-session-还能用吗？"><a href="#69-如果客户端禁止-cookie-能实现-session-还能用吗？" class="headerlink" title="69. 如果客户端禁止 cookie 能实现 session 还能用吗？"></a>69. 如果客户端禁止 cookie 能实现 session 还能用吗？</h3><p>可以用，session 只是依赖 cookie 存储 sessionid，如果 cookie 被禁用了，可以使用 url 中添加 sessionid 的方式保证 session 能正常使用</p><h3 id="71-如何避免-SQL-注入？"><a href="#71-如何避免-SQL-注入？" class="headerlink" title="71. 如何避免 SQL 注入？"></a>71. 如何避免 SQL 注入？</h3><ul><li>使用预处理 PreparedStratement</li><li>使用正则表达式过滤掉字符中的特殊字符</li></ul><h3 id="72-什么是-XSS-攻击，如何避免？"><a href="#72-什么是-XSS-攻击，如何避免？" class="headerlink" title="72. 什么是 XSS 攻击，如何避免？"></a>72. 什么是 XSS 攻击，如何避免？</h3><p>XSS 攻击：跨站脚本攻击，是 Web 程序中常见的漏洞。原理是攻击者往 Web 页面里面插入恶意脚代码，当用户浏览该页面时，嵌入其中的脚本会被执行，从而达到恶意攻击用户的目的，比如盗取 cookie，破坏页面结构，重定向到其他网站等</p><p>预防 XSS 的核心是必须对输入的数据做过滤处理</p><h3 id="73-什么是-CSRF-攻击，如何避免？"><a href="#73-什么是-CSRF-攻击，如何避免？" class="headerlink" title="73. 什么是 CSRF 攻击，如何避免？"></a>73. 什么是 CSRF 攻击，如何避免？</h3><p>CSRF：Cross-Site Request Forgery（中文：跨站请求伪造），可以理解为攻击者盗用了你的身份，以你的名义发送恶意请求</p><p>防御手段：</p><ul><li>验证请求来源地址</li><li>关键操作添加验证码</li><li>在请求地址添加 token 并验证</li></ul><h2 id="Java-异常模块"><a href="#Java-异常模块" class="headerlink" title="Java 异常模块"></a>Java 异常模块</h2><h3 id="74-throw-和-throws-的区别"><a href="#74-throw-和-throws-的区别" class="headerlink" title="74. throw 和 throws 的区别"></a>74. throw 和 throws 的区别</h3><ul><li>throw：是真实抛出一个异常（语句的开头）</li><li>throws：是声明可能会抛出一个异常（方法声明的尾部）</li></ul><h3 id="75-final、finally、finalize-有什么区别？"><a href="#75-final、finally、finalize-有什么区别？" class="headerlink" title="75. final、finally、finalize 有什么区别？"></a>75. final、finally、finalize 有什么区别？</h3><ul><li>final：是修饰符，如果修饰类，此类不能别继承；如果修饰方法和变量，表示这个方法和变量不能被重写和覆盖</li><li>finally：是 try{} catch{} finally{} 最后一部分，表示不论发生任何情况都会执行，finally 部分可以省略，但如果 finally部分存在，则一定会执行 finally 里面的代码</li><li>finalize：是 Object 的 protected 方法，子类可以覆盖该方法，以实现资源清理工作，GC 在回收对象之前调用该方法</li></ul><h3 id="76-try-catch-finally-中那个部分可以省略？"><a href="#76-try-catch-finally-中那个部分可以省略？" class="headerlink" title="76. try-catch-finally 中那个部分可以省略？"></a>76. try-catch-finally 中那个部分可以省略？</h3><p>try-catch-finally 其中 catch 和 finally 都可以被省略，但是不能同时省略，也就是说有 try 的时候，必须后面跟着一个 catch 或者 finally</p><h3 id="77-try-catch-finally-中，如果-catch-中-return-了，finally-还会执行吗？"><a href="#77-try-catch-finally-中，如果-catch-中-return-了，finally-还会执行吗？" class="headerlink" title="77. try-catch-finally 中，如果 catch 中 return 了，finally 还会执行吗？"></a>77. try-catch-finally 中，如果 catch 中 return 了，finally 还会执行吗？</h3><p>finally 一定会执行成功，即使是 catch 中 return 了，catch 中的 return 会等 finally 中的代码执行完之后，才会执行</p><h3 id="78-常见的异常类有哪些？"><a href="#78-常见的异常类有哪些？" class="headerlink" title="78. 常见的异常类有哪些？"></a>78. 常见的异常类有哪些？</h3><ul><li>NullPointerException <strong>空指针异常</strong></li><li>ClassNotFoundException <strong>指定类不存在</strong></li><li>NumberFormatException <strong>字符串转换为数字异常</strong></li><li>IndexOutOfBoundsException <strong>数组下标越界异常</strong></li><li>ClassCastException <strong>数据类型转换异常</strong></li><li>FileNotFoundException <strong>文件未找到异常</strong></li><li>NoSuchMethodException <strong>方法不存在异常</strong></li><li>IOException <strong>IO 异常</strong></li><li>SocketException <strong>Socket 异常</strong></li></ul><h2 id="网络模块"><a href="#网络模块" class="headerlink" title="网络模块"></a>网络模块</h2><h3 id="http-响应码-301-和-302-代表的是什么？有什么区别？"><a href="#http-响应码-301-和-302-代表的是什么？有什么区别？" class="headerlink" title="http 响应码 301 和 302 代表的是什么？有什么区别？"></a>http 响应码 301 和 302 代表的是什么？有什么区别？</h3><ul><li>301：永久重定向；302：暂时重定向</li><li>301 对搜索引擎优化（SEO）更加有利；302 又被提示为网络拦截的风险</li></ul><h3 id="80-forward-和-redirect-的区别？"><a href="#80-forward-和-redirect-的区别？" class="headerlink" title="80. forward 和 redirect 的区别？"></a>80. forward 和 redirect 的区别？</h3><ul><li>forward 是转发和 redirect 是重定向</li><li>地址栏 url 显示：forward url 不会发生改变，redirect url 会发生改变</li><li>数据共享：forward 可以共享 request 里的数据，redirect 不能</li><li>效率：forward 比 redirect 效率高</li></ul><h3 id="81-简述-tcp-和-udp-的区别"><a href="#81-简述-tcp-和-udp-的区别" class="headerlink" title="81. 简述 tcp 和 udp 的区别"></a>81. 简述 tcp 和 udp 的区别</h3><p>tcp 和 udp 是 OSI 模型中的运输层中的协议，tcp 提供可靠的通信传输，而 udp 则常被用于广播，细节控制交给应用的通信传输，区别如下：</p><ul><li>tcp 面向连接；udp 面向非连接，即发送数据前不需要建立链接</li><li>tcp 提供可靠的服务（数据传输）；udp 无法保证</li><li>tcp 面向字节流；udp 面向报文</li><li>tcp 数据传输慢；udp 数据传输快</li></ul><h3 id="82-tcp-为什么要三次握手，两次握手不行吗？为什么？"><a href="#82-tcp-为什么要三次握手，两次握手不行吗？为什么？" class="headerlink" title="82. tcp 为什么要三次握手，两次握手不行吗？为什么？"></a>82. tcp 为什么要三次握手，两次握手不行吗？为什么？</h3><p>第一次握手：A 给 B 打电话说，你可以听到我吗？</p><p>第二次握手：B 收到 A 的信息，然后对 A 说：听得到你说话，你能听到我吗？</p><p>第三次握手：A 收到 B 的信息，然后说可以</p><p>在第三次捂手后，A 和 B 都确定了：我说话，你听得到，你说话，我听得到。就算是建立通信了</p><p>如果使用两次握手，name只要服务器发出确认数据包就会建立连接，但由于客户端此时并未响应服务器的请求，那此时服务器端就会一直在等待客户端，这样服务器端就白白浪费了一定的资源。若采用了三次握手，服务器端没有收到来自客户端的在此确认，则就会知道客户端并没有要求建立请求，就不会浪费服务器的资源</p><h3 id="83-说一下-tcp-粘包是怎么产生的"><a href="#83-说一下-tcp-粘包是怎么产生的" class="headerlink" title="83. 说一下 tcp 粘包是怎么产生的"></a>83. 说一下 tcp 粘包是怎么产生的</h3><p>tcp 粘包可能发生在发送端或者接收端：</p><ul><li>发送端粘包：发送端需要等缓冲区满才发送出去，造成粘包</li><li>接收方粘包：接收方不及时接收缓冲区的包，造成多个包接收</li></ul><h3 id="84-OSI-的七层模型都有哪些？"><a href="#84-OSI-的七层模型都有哪些？" class="headerlink" title="84. OSI 的七层模型都有哪些？"></a>84. OSI 的七层模型都有哪些？</h3><p>物理层：利用传输介质为数据链路层提供物理连接，实现比特流的透明传输</p><p>数据链路层：负责建立和管理节点间的链路</p><p>网络层：通过路由选择算法，为报文或分组通过通信子网选择最适当的路径</p><p>传输层：向用户提供可靠的端到端的查错和流量控制，保证报文的正确传输</p><p>会话层：向两个实体的表示层提供建立和使用练连接的方法</p><p>表示层：处理用户信息的表示问题，如编码，数据格式转换和加密解密等</p><p>应用层：直接向用户提供服务，完成用户希望在网络完成的各种工作</p><h3 id="85-get-和-post-请求有哪些区别？"><a href="#85-get-和-post-请求有哪些区别？" class="headerlink" title="85. get 和 post 请求有哪些区别？"></a>85. get 和 post 请求有哪些区别？</h3><p>get 请求会被浏览器主动缓存，而 post 不会</p><p>get 传递的参数有大小限制，而 post 没有</p><p>post 参数传输更安全，get 的参数会铭文限制在 url 上，post 不会</p><h2 id="设计模式模块"><a href="#设计模式模块" class="headerlink" title="设计模式模块"></a>设计模式模块</h2><h3 id="88-说一下常用设计模式"><a href="#88-说一下常用设计模式" class="headerlink" title="88. 说一下常用设计模式"></a>88. 说一下常用设计模式</h3><ul><li>单例模式：保证被创建一次，节省系统的开销</li><li>工厂模式（简单工厂，抽象工厂）：解耦代码</li><li>观察者模式：定义了对象之间的一对多依赖，当一个对象改变时，它的所有依赖者都会收到通知并自动更新</li><li>模板方法模式：定义了一个算法的骨架，将一些步骤延迟到子类中，模板方法使得子类可以在不改变算法结构的情况下，重新定义算法的步骤</li><li>状态模式：允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它的类</li></ul><h3 id="89-简单工厂和抽象工厂有什么区别？"><a href="#89-简单工厂和抽象工厂有什么区别？" class="headerlink" title="89. 简单工厂和抽象工厂有什么区别？"></a>89. 简单工厂和抽象工厂有什么区别？</h3><p>简单工厂：用来生产同一级结构中的任意产品，对于新增的产品无能为力</p><p>工厂方法：用来生产同一级结构中的固定产品，支持增加任意产品</p><p>抽象工厂：用来生产不同产品族的全部产品，对于新增的产品无能为力；支持增加产品族</p><h2 id="Spring-Spring-MVC-模块"><a href="#Spring-Spring-MVC-模块" class="headerlink" title="Spring / Spring MVC 模块"></a>Spring / Spring MVC 模块</h2><h3 id="90-为什么要使用-spring？"><a href="#90-为什么要使用-spring？" class="headerlink" title="90. 为什么要使用 spring？"></a>90. 为什么要使用 spring？</h3><p>spring 提供 IOC 技术，容器会管理依赖的对象，从而不需要自己创建和管理对象，更轻松实现程序的解耦</p><p>spring 提供了事务的支持，使得事务操作更加方便</p><p>spring 提供了面向切面编程，这样可以更方便的处理某一类的问题</p><p>更方便的框架集成，可以集成其他的框架，比如 MyBatis 等</p><h3 id="91-解释一下什么是-AOP？"><a href="#91-解释一下什么是-AOP？" class="headerlink" title="91. 解释一下什么是 AOP？"></a>91. 解释一下什么是 AOP？</h3><p>AOP 是面向切面编程，通过预编译方式和运行期间动态代理实现程序功能的统一维护的一种技术。</p><p>简单来说就是统一处理某一 “切面”（类）的问题的编程思想，比如处理日志、异常等</p><h3 id="92-解释一下什么是-IOC？"><a href="#92-解释一下什么是-IOC？" class="headerlink" title="92. 解释一下什么是 IOC？"></a>92. 解释一下什么是 IOC？</h3><p>IOC（Inversionof Control 控制反转）：是 spring 的核心，对弈 spring 框架来说，就是由 spring 来负责控制对象的生命周期和对象之间的关系</p><p>简单来说，控制指的是当前对象对内部成员的控制权</p><p>控制反转指的是，这种控制权不由当前对象管理了，由其他（类，第三方容器）来管理</p><h3 id="93-spring-有哪些主要模块？"><a href="#93-spring-有哪些主要模块？" class="headerlink" title="93. spring 有哪些主要模块？"></a>93. spring 有哪些主要模块？</h3><ul><li>spring core：框架的最基础部分，提供 IOC 和依赖注入的特性</li><li>spring context：构建于 core 封装包基础上的 context 封装包，提供了一种框架式的对象访问方法</li><li>spring dao：Data Access Object 提供了 JDBC 的抽象层</li><li>spring aop：提供了面向切面的编程实现，让你可以自定义拦截器、切点等</li><li>spring Web：提供了针对 Web 开发集成特性，例如文件上传，利用 Servlet Listeners 进行 IoC 容器初始化和针对 Web 的 ApplicationContext</li><li>spring Web mvc：spring 中的 mvc 封装包提供了 Web 应用的 Model-View-Controller（MVC）的实现</li></ul><h3 id="94-spring-常用的注入方式有哪些？"><a href="#94-spring-常用的注入方式有哪些？" class="headerlink" title="94. spring 常用的注入方式有哪些？"></a>94. spring 常用的注入方式有哪些？</h3><ul><li>setter：属性注入</li><li>构造方法注入</li><li>注解方式注入</li></ul><h3 id="95-spring-中的-bean-是线程安全的吗？"><a href="#95-spring-中的-bean-是线程安全的吗？" class="headerlink" title="95. spring 中的 bean 是线程安全的吗？"></a>95. spring 中的 bean 是线程安全的吗？</h3><p>spring 中的 bean 默认是单例模式，spring 框架并没有对单例 bean 进行多线程的封装处理</p><p>实际上大部分时候 spring bean 无状态的（比如 dao 类），所有某种程度多行来说 bean 也是安全的，但如果 bean 有状态的话（比如 view model 对象），那就要开发者自己去保证线程安全了，最简单的就是改变 bean 的作用域，把 “singleton” 变更为 “prototype”，这样请求 bean 相当于 new Bean() 了，所以就可以保证线程安全了</p><p>有状态就是有数据存储功能</p><p>无状态就是不会保存数据</p><h3 id="96-spring-支持几种-bean-的作用域？"><a href="#96-spring-支持几种-bean-的作用域？" class="headerlink" title="96. spring 支持几种 bean 的作用域？"></a>96. spring 支持几种 bean 的作用域？</h3><ul><li>singleton：spring IoC 容器中只存在一个 bean 实例，bean 以单例模式存在，是系统默认值</li><li>prototype：每次从容器调用 bean 时都会创建一个新的实例，即每次 getBean() 相当于执行 new Bean() 操作（但是会造成很大的性能开销）</li><li>request：每次 http 请求都会创建一个 bean</li><li>session：同一个 http session 共享一个 bean 实例</li><li>global-seesion：用于 protlet 容器，因为每个 protlet 有单独的 session，globalsession 提供一个全局性的 http session</li></ul><h3 id="97-spring-自动装配-bean-有哪些方式？"><a href="#97-spring-自动装配-bean-有哪些方式？" class="headerlink" title="97. spring 自动装配 bean 有哪些方式？"></a>97. spring 自动装配 bean 有哪些方式？</h3><ul><li>no：默认值，表示没有自动装配</li><li>byName：它根据 bean 的名称注入对象依赖项</li><li>byType：它根据类型注入对象依赖项</li><li>构造函数：通过构造函数来注入依赖项，需要设置大量的参数</li><li>autodetect：容器首先通过构造函数使用 autowire 装配，如果不能，则通过 byType 自动装配</li></ul><h3 id="98-spring-事务实现方式有哪些？"><a href="#98-spring-事务实现方式有哪些？" class="headerlink" title="98. spring 事务实现方式有哪些？"></a>98. spring 事务实现方式有哪些？</h3><p>声明式事务：声明式事务也有两种实现方式，基于 xml 配置文件的方式和注解方式（在类上添加 @Transaction 注解）</p><p>编码方式：提供编码形式的管理和维护事务</p><h3 id="99-说一下-spring-的事务隔离？"><a href="#99-说一下-spring-的事务隔离？" class="headerlink" title="99. 说一下 spring 的事务隔离？"></a>99. 说一下 spring 的事务隔离？</h3><p>spring 有五大隔离级别：</p><ul><li>ISOLATION_DEFAULT：用底层数据库的设置隔离级别，数据库是什么就用什么</li><li>ISOLATIONREADUNCOMMITTED：未提交读，最低隔离级别、事务未提交前，就可被其他事务读取（会出现幻读，脏读，不可重复读）</li><li>ISOATIONREADCOMMITTED：提交读，一个事务提交后才能被其他事务读取到（会造成幻读，不可重复读），SQL Server 的默认级别</li><li>ISOLATIONREPEATABLEREAD：课重复读，保证多次读取统一个数据时，其值都和事务开始时候的内容是一致，进制读取到别的事务未提交的数据（会造成幻读），MySQL 默认级别</li><li>ISOLATIO_SERIALIZABLE：序列化，代价最高最可靠的隔离级别，该隔离级别能防止脏读，不可重复读，幻读</li></ul><p>脏读：表示一个事务能够读取另一个事务中还未提交的数据。比如，某个事务尝试插入记录 A，此时该事务还未提交，然后另一个事务尝试读取到了记录 A</p><p>不可重复读：是指在一个事务内，多次读同一数据</p><p>幻读：指同一个事务内多次查询返回的结果集不一样。比如用一个事务 A 第一次查询的时候有 n 条记录，但是第二次同等条件下查询却有 n + 1 条记录。发送幻读的原因也是另外一个事务新增或者删除或者修改了第一个事务结果集里面的数据，同一个记录的数据内容被修改了，所有数据行的记录就变多或者变少了</p><h3 id="100-说一下-spring-mvc-运行流程？"><a href="#100-说一下-spring-mvc-运行流程？" class="headerlink" title="100. 说一下 spring mvc 运行流程？"></a>100. 说一下 spring mvc 运行流程？</h3><ol><li>spring mvc 先将请求发送给 DispatcherServlet</li><li>DispatcherServlet 查询一个或多个 HandlerMapping，找到处理请求的 Controller</li><li>DispatcherServlet 再把请求提交到对应的 Controller</li><li>Controller 进行业务逻辑处理后，会返回一个 ModelAndView</li><li>Dispathcher 查询一个或者多个 ViewResolver 视图解析器，找到 ModelAndView 对象指定的视图对象。</li><li>视图对象负责渲染返回给客户端</li></ol><h3 id="101-spring-mvc-有哪些组件"><a href="#101-spring-mvc-有哪些组件" class="headerlink" title="101. spring mvc 有哪些组件"></a>101. spring mvc 有哪些组件</h3><ul><li><p>前端控制器 DispatcherServlet</p></li><li><p>映射控制器 HandlerMapping</p></li><li><p>处理器 Controller</p></li><li><p>模型和视图 ModelAndView</p></li><li><p>视图解析器 ViewResolver</p></li></ul><h3 id="102-RequestMapping-的作用是什么？"><a href="#102-RequestMapping-的作用是什么？" class="headerlink" title="102. @RequestMapping 的作用是什么？"></a>102. @RequestMapping 的作用是什么？</h3><p>将 http 请求映射到响应的类 / 方法上</p><h3 id="103-Autowired-的作用是什么？"><a href="#103-Autowired-的作用是什么？" class="headerlink" title="103. @Autowired 的作用是什么？"></a>103. @Autowired 的作用是什么？</h3><p>@Autowired 它可以对类成员变量，方法及构造函数进行标注，完成自动装配的工作，通过 @Autowired 的使用来消除 set  / get 方法</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Java 多线程模块&lt;/li&gt;
&lt;li&gt;Java 反射模块&lt;/li&gt;
&lt;li&gt;Java 对象拷贝模块&lt;/li&gt;
&lt;li&gt;Java 异常模块&lt;/li&gt;
&lt;li&gt;网络模块&lt;/li&gt;
&lt;li&gt;设计模式模块&lt;/li&gt;
&lt;li&gt;Spring / Spring MVC 模块&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;题库来源于 &lt;a href=&quot;https://www.javazhiyin.com/42272.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Java知音&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Java" scheme="https://www.chentyit.com/categories/Java/"/>
    
    
      <category term="面试" scheme="https://www.chentyit.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>Java面试题01</title>
    <link href="https://www.chentyit.com/2019/09/20/Java%E9%9D%A2%E8%AF%95%E9%A2%9801/"/>
    <id>https://www.chentyit.com/2019/09/20/Java面试题01/</id>
    <published>2019-09-20T07:33:47.000Z</published>
    <updated>2019-09-23T00:32:11.271Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Java 基础模块</li><li>Java 容器模块</li><li>Java多线程模块</li></ul><p>题库来源于 <a href="https://www.javazhiyin.com/42272.html" target="_blank" rel="noopener">Java知音</a></p><a id="more"></a><h2 id="Java-基础模块"><a href="#Java-基础模块" class="headerlink" title="Java 基础模块"></a>Java 基础模块</h2><h3 id="1-JDK-和-JRE-的区别"><a href="#1-JDK-和-JRE-的区别" class="headerlink" title="1. JDK 和 JRE 的区别"></a>1. JDK 和 JRE 的区别</h3><ul><li>JDK（Java Development Kit），Java 开发工具包，提供了 Java 开发环境和运行环境</li><li>JRE（Java Runtime Environment），Java 运行环境，为 Java 的运行提供了运行时所需要的环境</li></ul><p>JDK 包含了 JRE，同时包含了编译 Java 编码的编译器 Javac，包含了很多 Java 程序调试和分析的工具</p><h3 id="2-和-equals-的区别"><a href="#2-和-equals-的区别" class="headerlink" title="2. == 和 equals 的区别"></a>2. == 和 equals 的区别</h3><ul><li><p>==：对于基本类型和引用类型，效果是不同的</p><p>基本类型：比较值是否相同</p><p>引用类型：比较引用是否相同</p></li><li><p>equals：本质就是 ==，只不过 String 和 Integer 等类重写了 equals 方法，变成了值比较</p><p><strong>Object 中的 equals</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">this</span> == obj);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>String 中重写了 equals</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object anObject)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 先判断两个对象的地址是否相同，相同直接返回 true，否则继续判断</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span> == anObject) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 判断另一个对象到底是不是字符串</span></span><br><span class="line">    <span class="keyword">if</span> (anObject <span class="keyword">instanceof</span> String) &#123;</span><br><span class="line">        <span class="comment">// 转化为字符串类型</span></span><br><span class="line">        String anotherString = (String)anObject;</span><br><span class="line">        <span class="keyword">int</span> n = value.length;</span><br><span class="line">        <span class="comment">// 判断两个字符串的长度</span></span><br><span class="line">        <span class="keyword">if</span> (n == anotherString.value.length) &#123;</span><br><span class="line">            <span class="keyword">char</span> v1[] = value;</span><br><span class="line">            <span class="keyword">char</span> v2[] = anotherString.value;</span><br><span class="line">            <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">            <span class="comment">// 字符串长度相等就一个一个的判断字符串中每个字符是否相等</span></span><br><span class="line">            <span class="keyword">while</span> (n-- != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (v1[i] != v2[i])</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="3-两个对象的-hashCode-相同，则-equals-也一定为-true-吗？"><a href="#3-两个对象的-hashCode-相同，则-equals-也一定为-true-吗？" class="headerlink" title="3. 两个对象的 hashCode() 相同，则 equals() 也一定为 true 吗？"></a>3. 两个对象的 hashCode() 相同，则 equals() 也一定为 true 吗？</h3><p>两个对象的 hashCode 相同，equals() 不一定相同</p><p>hashCode 相同表示两个键值对的哈希值相同，哈希值相等并不代表键值对相等d</p><h3 id="4-final-在-Java-中有什么作用？"><a href="#4-final-在-Java-中有什么作用？" class="headerlink" title="4. final 在 Java 中有什么作用？"></a>4. final 在 Java 中有什么作用？</h3><ul><li>修饰的类叫做最终类</li><li>修饰的方法不能被重写</li><li>修饰的变量叫常量，常量必须初始化，初始化后值不能被修改</li></ul><h3 id="5-Java-中的-Math-round-1-5-等于多少？"><a href="#5-Java-中的-Math-round-1-5-等于多少？" class="headerlink" title="5. Java 中的 Math.round(-1, 5) 等于多少？"></a>5. Java 中的 Math.round(-1, 5) 等于多少？</h3><p>等于 -1，round() 是四舍五入（直接理解成向上取整）</p><h3 id="6-String-属于基础数据类型吗？"><a href="#6-String-属于基础数据类型吗？" class="headerlink" title="6. String 属于基础数据类型吗？"></a>6. String 属于基础数据类型吗？</h3><p>不属于，基础类型只有 8 种：bit，short，int，long，float，double，char，boolean</p><h3 id="7-Java-中操作字符串的都有哪些类？有什么区别？"><a href="#7-Java-中操作字符串的都有哪些类？有什么区别？" class="headerlink" title="7. Java 中操作字符串的都有哪些类？有什么区别？"></a>7. Java 中操作字符串的都有哪些类？有什么区别？</h3><ul><li>String</li><li>StringBuilder</li><li>StringBuffer</li></ul><p>StringBuilder 和 StringBuffer 都继承抽象列 AbstractStringBuilder</p><p>String 声明的是不可变的对象，每次操作都会生成新的对象，然后向指正指向新的对象</p><p>StringBuilder 和 StringBuffer 存储数据的字符数组没有被 final 修饰，值可以修改，AbstractStringBuilder 提供了一个自动扩容机制（默认初始长度为 16）长度不够的时候会自动扩容，扩展容量为原来的2倍加2，拼接字符串的效率比 Stirng 高</p><p>StringBuilder 执行效率高，但是线程不安全</p><p>StringBuffer 每个方法都用 synchronize 修饰，加锁和释放锁消耗资源，效率比 StringBuilder 低，但是线程安全</p><p>三者执行速度比较：StringBuilder &gt; StringBuffer &gt; String</p><h3 id="8-String-str-“i”-与-String-str-new-String-“i”-一样吗？"><a href="#8-String-str-“i”-与-String-str-new-String-“i”-一样吗？" class="headerlink" title="8. String str = “i” 与 String str = new String(“i”) 一样吗？"></a>8. String str = “i” 与 String str = new String(“i”) 一样吗？</h3><p>不一样，因为内存分配方式不一样</p><p><strong>String str = “i”：</strong>Java 虚拟机会将其分配到常量池中，如果池中有 “i” 就直接返回地址，没有就创建再返回地址</p><p><strong>String str = new String(“i”) ：</strong>直接在堆内存中开辟新空间</p><h3 id="9-字符串反转"><a href="#9-字符串反转" class="headerlink" title="9. 字符串反转"></a>9. 字符串反转</h3><p>使用 StringBuilder 或者 StringBuffer 的 reverse() 方法</p><h3 id="10-String-类的常用方法都有哪些？"><a href="#10-String-类的常用方法都有哪些？" class="headerlink" title="10. String 类的常用方法都有哪些？"></a>10. String 类的常用方法都有哪些？</h3><ul><li>indexOf()：返回指定字符的索引</li><li>charAt()：返回指定索引的字符</li><li>replace()：字符串替换</li><li>trim()：取出字符串两端空白</li><li>split()：分割字符串，返回一个分割后的字符串数组</li><li>getBytes()：返回字符串的 byte 类型数组</li><li>length()：返回字符串长度</li><li>toLowerCase()：将字符串转成小写字母</li><li>toUpperCase()：将字符串转成大小字母</li><li>substring()：截取字符串</li><li>equals()：字符串比较</li></ul><h3 id="11-抽象类必须有抽象方法吗？"><a href="#11-抽象类必须有抽象方法吗？" class="headerlink" title="11. 抽象类必须有抽象方法吗？"></a>11. 抽象类必须有抽象方法吗？</h3><p>不一定，但是包含抽象方法的<strong>类</strong>一定是抽象类</p><h3 id="12-普通类和抽象类有什么区别？"><a href="#12-普通类和抽象类有什么区别？" class="headerlink" title="12. 普通类和抽象类有什么区别？"></a>12. 普通类和抽象类有什么区别？</h3><ol><li>普通类不能有抽象方法，抽象类有抽象方法</li><li>抽象类不能被实例化，普通类可以</li><li>一个类继承抽象类，必须要重写抽象方法，如果不重写，说明子类也是抽象类</li></ol><h3 id="13-抽象类能用-final-修饰吗？"><a href="#13-抽象类能用-final-修饰吗？" class="headerlink" title="13. 抽象类能用 final 修饰吗？"></a>13. 抽象类能用 final 修饰吗？</h3><p>抽象类被定义就是用来被继承实现的，被 final 修饰的类不能被继承，所以 abstract 和 final 不能共存于一个类中，抽象类不能用 final 修饰</p><h3 id="14-接口和抽象类有什么区别？"><a href="#14-接口和抽象类有什么区别？" class="headerlink" title="14. 接口和抽象类有什么区别？"></a>14. 接口和抽象类有什么区别？</h3><ul><li>结构：抽象类用 abstract 修饰；接口用 interface 修饰</li><li>继承：抽象类可以继承抽象类，实现接口，继承普通类（前提是被继承的类必须要有构造方法）；接口只能继承接口</li><li>实现：抽象类的子类使用 extends 继承；接口使用 implement 实现</li><li>构造方法：抽象类可以有构造方法；接口没有</li><li>实现数量：抽象类只能单继承；接口可以多实现</li><li>变量：抽象类中的变量可以是普通变量；接口里面的变量只能是公共静态变量</li><li>方法类型：抽象类中的方法可以有实现，也可以是抽象方法；接口的只能是抽象方法</li><li>访问修饰符：接口中的方法默认使用 public abstract；抽象类的方法可以使用 public 和 protected 修饰，如果用 private 就会报错</li></ul><p><strong>接口是设计的结果，抽象类是重构的结果</strong></p><h3 id="15-Java-中的-IO-流分为几种"><a href="#15-Java-中的-IO-流分为几种" class="headerlink" title="15. Java 中的 IO 流分为几种"></a>15. Java 中的 IO 流分为几种</h3><ul><li><p>按功能分：<strong>输入流</strong>和<strong>输出流</strong></p></li><li><p>按类型分：<strong>字节流</strong>和<strong>字符流</strong></p><p>区别是字节流按 8 位二进制字节为单位传输，字符流以 16 为二进制字符为单位传输</p></li></ul><h3 id="16-BIO、NIO-和-AIO-有什么区别？"><a href="#16-BIO、NIO-和-AIO-有什么区别？" class="headerlink" title="16. BIO、NIO 和 AIO 有什么区别？"></a>16. BIO、NIO 和 AIO 有什么区别？</h3><ul><li><strong>BIO：</strong>Block IO 同步阻塞式 IO，传统 IO，模式简单使用方便，并发处理能力低</li><li><strong>NIO：</strong>New IO 同步非阻塞式 IO，BIO 升级，客户端和服务端通过 Channel 通讯，实现多路复用</li><li><strong>AIO：</strong>Asynchronous IO 是 BIO 升级，也叫 NIO2，异步非阻塞 IO，异步 IO 的操作基于事件和回调机制</li></ul><h3 id="17-Files-的常用方法都有哪些"><a href="#17-Files-的常用方法都有哪些" class="headerlink" title="17. Files 的常用方法都有哪些"></a>17. Files 的常用方法都有哪些</h3><ul><li>Files.exists()：检测文件路径是否存在</li><li>Files.createFile()：创建文件</li><li>Files.createDirectory()：创建文件夹</li><li>Files.delete()：删除一个文件或目录</li><li>Files.copy()：复制文件</li><li>Files.move()：移动文件</li><li>Files.size()：查看文件个数</li><li>Files.read()：读取文件</li><li>Files.write()：写入文件</li></ul><h2 id="Java-容器模块"><a href="#Java-容器模块" class="headerlink" title="Java 容器模块"></a>Java 容器模块</h2><h3 id="18-Java-容器都有哪些？"><a href="#18-Java-容器都有哪些？" class="headerlink" title="18. Java 容器都有哪些？"></a>18. Java 容器都有哪些？</h3><p>Java 容器分为 Collection 和 Map 两大类：</p><ul><li>Collection：<ul><li>List</li><li>ArrayList</li><li>LinkedList</li><li>Vector</li><li>Stack</li><li>Set</li><li>HashSet</li><li>LinkedHashSet</li><li>TreeSet</li></ul></li><li>Map:<ul><li>HashMap</li><li>LinkedHasMap</li><li>TreeMap</li><li>ConcurrentHashMap</li><li>Hashtable</li></ul></li></ul><h3 id="19-Collection-和-Collections-有什么区别？"><a href="#19-Collection-和-Collections-有什么区别？" class="headerlink" title="19. Collection 和 Collections 有什么区别？"></a>19. Collection 和 Collections 有什么区别？</h3><p>Collection 是一个集合接口，提供了对集合对象进行基本操作的通用接口</p><p>Collections 是一个包装类，包含了很多静态方法，不能被实例化，是一个工具类，时间对集合的查找、排序、替换、线程安全化等操作</p><h3 id="20-List、Set、Map-之间的区别是什么？"><a href="#20-List、Set、Map-之间的区别是什么？" class="headerlink" title="20. List、Set、Map 之间的区别是什么？"></a>20. List、Set、Map 之间的区别是什么？</h3><p>List、Set、Map 的区别主要体现在连个方面：元素是否有序，是否允许元素重复</p><table><thead><tr><th><strong>比较</strong></th><th><strong>List</strong></th><th><strong>Set</strong></th><th><strong>Map</strong></th></tr></thead><tbody><tr><td>继承接口</td><td>Collection</td><td>Collection</td><td></td></tr><tr><td>常见实现类</td><td>AbstractList(其常用子类有ArrayList、LinkedList、Vector)</td><td>AbstractSet(其常用子类有HashSet、LinkedHashSet、TreeSet)</td><td>HashMap、HashTable</td></tr><tr><td>常见方法</td><td>add( )、remove( )、clear( )、get( )、contains( )、size( )</td><td>add( )、remove( )、clear( )、contains( )、size( )</td><td>put( )、get( )、remove( )、clear( )、containsKey( )、containsValue( )、keySet( )、values( )、size( )</td></tr><tr><td>元素</td><td>可重复</td><td>不可重复(用<code>equals()</code>判断)</td><td>不可重复</td></tr><tr><td>顺序</td><td>有序</td><td>无序(实际上由HashCode决定)</td><td></td></tr><tr><td>线程安全</td><td>Vector线程安全</td><td></td><td>Hashtable线程安全</td></tr></tbody></table><h3 id="21-HashMap-和-Hashtable-有什么区别？"><a href="#21-HashMap-和-Hashtable-有什么区别？" class="headerlink" title="21. HashMap 和 Hashtable 有什么区别？"></a>21. HashMap 和 Hashtable 有什么区别？</h3><p>HashMap 是继承自 AbstractMap 类，HashTable 是继承自 Dictionary 类，不过他们都实现了 map，Cloneable（可复制），Serializable（可序列化）三个接口</p><p>HashTable 比 HashMap 多提供了 elments() 和 contains() 两个方法</p><p><strong>底层结构：</strong></p><ul><li>HashMap：底层是哈希表数据结构，是线程不同步的，可以存储 null-null 键值对，替代了 HashTable，正因为可以存储 null-null 键值对，当使用 get 获取到 value 的值为 null 时，无法判断是不存在 key，还是这个 key 本身就是 null，所以不能通过 get 来判断 HashMap 中是否存在某个键，应该使用 containsKey() 方法来判断</li><li>Hashtable：底层是哈希表结构，是线程同步的，只支持 key-value 键值对</li></ul><p><strong>容量以及扩容：</strong></p><ul><li>Hashtable：初始容量是 11，每次扩充为原来的 2n + 1</li><li>HashMap：初始容量为 16，每次扩容为 2n</li></ul><p><strong>存储结构的哈希值：</strong></p><ul><li>HashTable：直接使用对象的 hasCode，hashCode 是 JDK 根据对象的地址或者字符串或者数字算出来的 int 类型的额数值（<a href="https://www.cnblogs.com/williamjie/p/9099141.html" target="_blank" rel="noopener">知识点链接</a>）</li><li>HashMap：哈希表 + 链表（<strong>可能会转化为红黑树</strong>）</li></ul><h3 id="22-如何决定使用-HashMap-还是-TreeMap？"><a href="#22-如何决定使用-HashMap-还是-TreeMap？" class="headerlink" title="22. 如何决定使用 HashMap 还是 TreeMap？"></a>22. 如何决定使用 HashMap 还是 TreeMap？</h3><ul><li>HashMap：在 Map 中插入、删除、定位一个元素这类操作</li><li>TreeMap：对一个 key 集合进行有序遍历</li></ul><h3 id="23-HashMap-实现原理"><a href="#23-HashMap-实现原理" class="headerlink" title="23. HashMap 实现原理"></a>23. HashMap 实现原理</h3><p>HashMap 基于 Hash 算法实现，通过 put(key, value) 存储，get(key) 来获取</p><p>当传入 key 值时，HashMap 会根据 key.hashCode()  计算出 hash 值，根据 hash 值将 value 保存在 buket 里，如果哈希值相同（哈希冲突），当 hash 冲突个数比较小的时候，就用链表，多的话自动转化为红黑树</p><h3 id="24-HashSet-实现原理"><a href="#24-HashSet-实现原理" class="headerlink" title="24. HashSet 实现原理"></a>24. HashSet 实现原理</h3><p>HashSet 基于 HashMap 实现的，HashSet 底层使用 HashMap 来保存元素，相关操作直接调用底层 HashMap 的相关方法实现，HashSet 不允许有重复值出现</p><h3 id="25-ArrayList-和-LinkedList-的区别是什么？"><a href="#25-ArrayList-和-LinkedList-的区别是什么？" class="headerlink" title="25. ArrayList 和 LinkedList 的区别是什么？"></a>25. ArrayList 和 LinkedList 的区别是什么？</h3><p><strong>数据结构实现：</strong></p><ul><li>ArrayList：是动态数组的数据结构实现</li><li>LinkedList：是双向链表的数据结构实现</li></ul><p><strong>随机访问效率：</strong></p><p>ArrayList 比 LinkedList 在随机访问的时候效率要高，因为 LinkedList 是线性的数据存储结构，每次访问的时间复杂度都是  O(n)</p><p><strong>删除和增加效率：</strong></p><p>在非首尾的增加和删除操作，LinkedList 比 ArrayList 效率高，因为 ArrayList 增加后删除操作要移动被操作位置以后的元素</p><p><strong>总结：</strong></p><p>频繁读取使用 ArrayList，频繁更改使用 LinkedList</p><h3 id="26-如何实现数组和-List-之间的转换？"><a href="#26-如何实现数组和-List-之间的转换？" class="headerlink" title="26. 如何实现数组和 List 之间的转换？"></a>26. 如何实现数组和 List 之间的转换？</h3><p>数组转 List：使用 <strong>Arrays.asList(array)</strong> 进行转化</p><p>List 转数组：使用 List 自带的 <strong>toArray()</strong> 方法</p><h3 id="27-ArrayList-和-Vector-的区别是什么？"><a href="#27-ArrayList-和-Vector-的区别是什么？" class="headerlink" title="27.ArrayList 和 Vector 的区别是什么？"></a>27.ArrayList 和 Vector 的区别是什么？</h3><p><strong>线程安全：</strong></p><ul><li>Vector 使用了 Synchronized 来实现线程同步，线程是安全的</li><li>ArrayList 是非线程安全的</li></ul><p><strong>性能：</strong></p><p>ArrayList 性能要优于 Vector</p><p><strong>扩容：</strong></p><p>ArrayList 和 Vector 都会根据实际的需要动态调整容量，Vector 会增加 1 倍，ArrayList 会增加 50%</p><h3 id="28-Array-和-ArrayList-有什么区别？"><a href="#28-Array-和-ArrayList-有什么区别？" class="headerlink" title="28. Array 和 ArrayList 有什么区别？"></a>28. Array 和 ArrayList 有什么区别？</h3><p>Array 可以存储基本数据类型和对象，ArrayList 只能存储对象</p><p>Array 有固定大小，ArrayList 可以自动扩展</p><p>ArrayList 的内置方法比 Array 多</p><h3 id="29-在-Queue-中-poll-和-remove-有什么区别？"><a href="#29-在-Queue-中-poll-和-remove-有什么区别？" class="headerlink" title="29. 在 Queue 中 poll() 和 remove() 有什么区别？"></a>29. 在 Queue 中 poll() 和 remove() 有什么区别？</h3><p><strong>相同点：</strong>都是返回第一个元素，并在队列中删除返回的对象</p><p><strong>不同点：</strong>如果没有元素，remove() 会直接抛出 NoSuchElementException 异常，poll() 会返回 null</p><h3 id="30-哪些集合类是线程安全的？"><a href="#30-哪些集合类是线程安全的？" class="headerlink" title="30. 哪些集合类是线程安全的？"></a>30. 哪些集合类是线程安全的？</h3><p>线程安全：Vector，Hashtable，Stack</p><p>线程不安全：HashMap（在 JDK 1.5 后，Java.util.concurrent 并发包中有了对应的安全类 ConcurrentHashMap）</p><h3 id="31-迭代器-Iterator-是什么？"><a href="#31-迭代器-Iterator-是什么？" class="headerlink" title="31. 迭代器 Iterator 是什么？"></a>31. 迭代器 Iterator 是什么？</h3><p>Iterator 接口提供任何 Collection 的接口，可以从一个 Collection 中使用迭代器方法来获取迭代器实例</p><p>迭代器取代了 Java 集合框架中的 Enumeration，允许调用者在迭代过程中移除元素</p><h3 id="32-Iterator-怎么使用？有什么特点？"><a href="#32-Iterator-怎么使用？有什么特点？" class="headerlink" title="32. Iterator 怎么使用？有什么特点？"></a>32. Iterator 怎么使用？有什么特点？</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">Iterator&lt;String&gt; it = list.iterator();</span><br><span class="line"><span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">    String obj = it.next();</span><br><span class="line">    System.out.println(obj);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>特点：更加安全，可以确保在当前遍历的集合元素被更改的时候，会抛出 ConcurrentModificationException 异常</p><h3 id="33-Iterator-和-Listlterator"><a href="#33-Iterator-和-Listlterator" class="headerlink" title="33. Iterator 和 Listlterator"></a>33. Iterator 和 Listlterator</h3><ul><li>Iterator 可以遍历 Set 和 List 集合，ListIterator 只能遍历 List</li><li>Iterator 只能单向遍历，而 ListIterator 可以双向遍历</li><li>ListIterator 实现 Iterator，然后添加了一些额外的功能，比如添加，替换，获取前后节点索引</li></ul><h3 id="34-怎么确保一个集合不能被修改"><a href="#34-怎么确保一个集合不能被修改" class="headerlink" title="34. 怎么确保一个集合不能被修改"></a>34. 怎么确保一个集合不能被修改</h3><p>可以使用 Collection.unmodifiableCollection(Collection c) 方法来创建一个只读集合，一旦发生改变，就会抛出 Java.lang.UnsupportedOperationException 异常</p><h2 id="Java-多线程模块"><a href="#Java-多线程模块" class="headerlink" title="Java 多线程模块"></a>Java 多线程模块</h2><h3 id="35-并行和并发有什么区别？"><a href="#35-并行和并发有什么区别？" class="headerlink" title="35. 并行和并发有什么区别？"></a>35. 并行和并发有什么区别？</h3><p><strong>并行：</strong>多个处理器或多核处理器同时处理多个任务</p><p><strong>并发：</strong>多个任务在同一个 CPU 核上，按细分的时间片轮流执行，从逻辑上看是同时执行的</p><h3 id="36-线程和进程的区别？"><a href="#36-线程和进程的区别？" class="headerlink" title="36. 线程和进程的区别？"></a>36. 线程和进程的区别？</h3><p>一个程序至少有一个进程，一个进程下至少有一个线程，一个进程下也可以有多个线程来增加程序的执行速度</p><h3 id="37-守护线程是什么？"><a href="#37-守护线程是什么？" class="headerlink" title="37. 守护线程是什么？"></a>37. 守护线程是什么？</h3><p>守护线程是运行在后台的一种特殊进程，独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件</p><p>在 Java 中的垃圾回收线程就是特殊的守护线程</p><h3 id="38-线程有几种实现方式？"><a href="#38-线程有几种实现方式？" class="headerlink" title="38. 线程有几种实现方式？"></a>38. 线程有几种实现方式？</h3><ol><li>继承 Thread 类</li><li>实现 Runnable 接口</li><li>实现 Callable 接口，通过 FutureTask 包装器来创建 Thread 线程</li><li>通过线程池创建线程，使用线程池接口 ExecutorService 结合 Callable、Future 事件有返回结果的多线程</li></ol><p>前两种 <strong>无返回值</strong>：重写 run 方法，run 方法返回值 void</p><p>后两种 <strong>有返回值</strong>：通过 Callable 接口，要实现 call 方法，这个方法返回值为 Object，可以保存返回结果</p><h3 id="39-Runnable-和-Callable-有什么区别？"><a href="#39-Runnable-和-Callable-有什么区别？" class="headerlink" title="39. Runnable 和 Callable 有什么区别？"></a>39. Runnable 和 Callable 有什么区别？</h3><p>Runnable 没有返回值，Callable 有返回值，Callable 可以看作是 Runnable 的补充</p><h3 id="40-线程有哪些状态？"><a href="#40-线程有哪些状态？" class="headerlink" title="40. 线程有哪些状态？"></a>40. 线程有哪些状态？</h3><ol><li>初始：新创建了一个线程对象，但还没有调用 start() 方法</li><li>运行：Java 线程中将就绪（ready）和运行中（running）两种状态统称为 ”运行“，线程对象创建后，其他线程调用了该对象的 start() 方法，该对象的线程位于可运行线程池中，等待被线程调用选中，获取 CPU 资源，此时处于就绪状态（ready）。就绪状态的线程在获得 CPU 时间片后变为运行状态（running）</li><li>阻塞：表示线程阻塞于锁</li><li>等待：进入该状态的线程需要等待其他线程做出一些特定的动作（通知或中断）</li><li>超时等待：不同于 waiting，它可以在指定时间后自行返回</li><li>终止：表示该线程已经执行完毕</li></ol><h3 id="41-sleep-和-wait-有什么区别？"><a href="#41-sleep-和-wait-有什么区别？" class="headerlink" title="41. sleep() 和 wait() 有什么区别？"></a>41. sleep() 和 wait() 有什么区别？</h3><ul><li>类不同：sleep 来自Thread，wait 来自Object</li><li>释放锁：sleep 不释放锁，wait 释放锁</li><li>用法不同：sleep 时间到会自动回复，wait 可以使用 notify() 或 notifyAll() 直接唤醒</li></ul><h3 id="42-notify-和-notifyAll-有什么区别？"><a href="#42-notify-和-notifyAll-有什么区别？" class="headerlink" title="42. notify() 和 notifyAll() 有什么区别？"></a>42. notify() 和 notifyAll() 有什么区别？</h3><p>notifyAll() 会唤醒所有线程，notify() 只会唤醒一个线程</p><p>notifyAll() 调用后，会将全部线程由等待池移到锁池，然后参与锁的竞争，竞争成功则继续执行，不成功则留在锁池等待锁被释放后重新竞争</p><p>notify() 只能唤醒一个线程，具体唤醒哪一个线程，由虚拟机控制</p><h3 id="43-线程的-run-和-start-有什么区别"><a href="#43-线程的-run-和-start-有什么区别" class="headerlink" title="43.线程的 run() 和 start() 有什么区别"></a>43.线程的 run() 和 start() 有什么区别</h3><p>start() 用于启动线程，run() 用于执行线程的运行时代码</p><p>run() 可以重复调用，而 start() 执行嗲用一次</p><h3 id="44-线程池创建方式"><a href="#44-线程池创建方式" class="headerlink" title="44. 线程池创建方式"></a>44. 线程池创建方式</h3><p>最核心的是最后一种：</p><ul><li>newSingleThreadExecutor()：它的特点在于工作线程数目被限制为 1，操作一个无界的工作队列，所以它保证了所有任务的都是被顺序执行，最多会有一个任务处于活动状态，并且不允许使用者改动线程池实例，因此可以避免其改变线程数目；</li><li>newCachedThreadPool()：它是一种用来处理大量短时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置的时间超过 60 秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用 SynchronousQueue 作为工作队列；</li><li>newFixedThreadPool(int nThreads)：重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有 nThreads 个工作线程是活动的。这意味着，如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的工作线程被创建，以补足指定的数目 nThreads；</li><li>newSingleThreadScheduledExecutor()：创建单线程池，返回 ScheduledExecutorService，可以进行定时或周期性的工作调度；</li><li>newScheduledThreadPool(int corePoolSize)：和newSingleThreadScheduledExecutor()类似，创建的是个 ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程；</li><li>newWorkStealingPool(int parallelism)：这是一个经常被人忽略的线程池，Java 8 才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序；</li><li><strong>ThreadPoolExecutor()：是最原始的线程池创建，上面1-3创建方式都是对ThreadPoolExecutor的封装。</strong></li></ul><h3 id="45-线程池都有哪些状态"><a href="#45-线程池都有哪些状态" class="headerlink" title="45. 线程池都有哪些状态"></a>45. 线程池都有哪些状态</h3><ul><li>RUNNING：接收新任务，处理等待队列中的任务</li><li>SHUTDOWN：不接收新的任务提交，会处理继续等待队列中的任务</li><li>STOP：不接收新的任务提交，不再处理等待队列中的任务，中断正在执行的线程</li><li>TIDYING：所有任务都销毁了，workCount 为 0，线程池的状态转化为 TIDYING 时，会执行钩子方法 terminated()</li><li>TERMINATED：terminated() 方法结束后的状态</li></ul><h3 id="46-线程池中-submit-和-execute-方法有什么区别？"><a href="#46-线程池中-submit-和-execute-方法有什么区别？" class="headerlink" title="46. 线程池中 submit() 和 execute() 方法有什么区别？"></a>46. 线程池中 submit() 和 execute() 方法有什么区别？</h3><ul><li>execute()：只能执行 Runnable 类型的任务</li><li>submit()：可以执行 Runnable 和 Callable 类型的任务</li></ul><h3 id="47-在-Java-程序中怎么保证多线程的运行安全？"><a href="#47-在-Java-程序中怎么保证多线程的运行安全？" class="headerlink" title="47. 在 Java 程序中怎么保证多线程的运行安全？"></a>47. 在 Java 程序中怎么保证多线程的运行安全？</h3><ol><li>使用安全类，java.util.concurrent 的类</li><li>使用自动锁 synchronized</li><li>使用手动锁 Lock</li></ol><h3 id="48-多线程中-synchronized-锁升级的原理是什么？"><a href="#48-多线程中-synchronized-锁升级的原理是什么？" class="headerlink" title="48. 多线程中 synchronized 锁升级的原理是什么？"></a>48. 多线程中 synchronized 锁升级的原理是什么？</h3><p>synchronized 锁升级原理：在锁对象的对象头里面有一个 threadid 字段，在第一次访问的时候 threadid 为空，jvm 让其持有偏向锁，并将 threadid 设置为其线程 id，再次进入的时候会先判断 threadid 是否与其线程 id 一致，如果一致则可以直接使用此对象，如果不一致，则升级偏向锁为轻量级锁，通过自旋循环一定次数来获取锁，执行一定次数之后，如果还没有正常获取到要使用的对象，此时就会把锁从轻量级升级为重量级锁，此过程就构成了 synchronized 锁的升级。</p><p><strong>锁的升级的目的：</strong>锁升级是为了减低了锁带来的性能消耗。在 Java 6 之后优化 synchronized 的实现方式，使用了偏向锁升级为轻量级锁再升级到重量级锁的方式，从而减低了锁带来的性能消耗。</p><h3 id="49-什么是死锁？"><a href="#49-什么是死锁？" class="headerlink" title="49. 什么是死锁？"></a>49. 什么是死锁？</h3><p>当线程 A 独占锁 a，并尝试去获取独占锁 b 的同时，线程 B 独占锁 b，并尝试获取独占锁 a 的情况，就会发生 AB 两个线程由于互相持有对方需要的锁，而发生的阻塞现象</p><h3 id="50-怎么防止死锁？"><a href="#50-怎么防止死锁？" class="headerlink" title="50. 怎么防止死锁？"></a>50. 怎么防止死锁？</h3><ul><li>尽量使用 tryLock(long timeout, TimeUnit unit) 的方法（ReentrantLock、ReetrantReadWriteLock），设置超时时间，超时可以退出防止死锁</li><li>尽量使用 Java.util.concurrent 并发类代替自己手写锁</li><li>尽量降低锁的使用力度</li><li>尽量不要几个功能使用同一把锁</li><li>尽量减少同步代码块</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Java 基础模块&lt;/li&gt;
&lt;li&gt;Java 容器模块&lt;/li&gt;
&lt;li&gt;Java多线程模块&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;题库来源于 &lt;a href=&quot;https://www.javazhiyin.com/42272.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Java知音&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Java" scheme="https://www.chentyit.com/categories/Java/"/>
    
    
      <category term="面试" scheme="https://www.chentyit.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>Java知识点day02(常用关键字)</title>
    <link href="https://www.chentyit.com/2019/09/19/Java%E7%9F%A5%E8%AF%86%E7%82%B9day02-%E5%B8%B8%E7%94%A8%E5%85%B3%E9%94%AE%E5%AD%97/"/>
    <id>https://www.chentyit.com/2019/09/19/Java知识点day02-常用关键字/</id>
    <published>2019-09-19T12:05:49.000Z</published>
    <updated>2019-09-19T14:50:00.181Z</updated>
    
    <content type="html"><![CDATA[<h2 id="static"><a href="#static" class="headerlink" title="static"></a>static</h2><h3 id="修饰："><a href="#修饰：" class="headerlink" title="修饰："></a>修饰：</h3><p>类的变量，方法，方法块</p><ul><li><strong>修饰变量时：</strong>如果该变量时 public 时，则可以使用 <strong>类名.修饰变量名</strong> 调用该变量，static 修饰的变量可能会有线程安全的问题，当 static 修饰了共享的变量，在现场交互中就有可能造成安全问题，解决办法：<ol><li>将被修饰的对象换成线程安全的对象</li><li>手动加锁</li></ol></li><li><strong>修饰方法时：</strong>表示该方法与类无关，任何类都可以直接访问，但是被 static 修饰的方法只能调用被 static 修饰的变量，static 修饰的方法没有线程问题，方法中的局部变量保存在栈中，每个栈都是隔离的，不会有问题</li><li><strong>修饰方法块：</strong>静态代码块，加载 .class 到内存中的时候，先初始化 static 的代码块，常常用于初始化一些值</li></ul><h3 id="父类和子类加载顺序："><a href="#父类和子类加载顺序：" class="headerlink" title="父类和子类加载顺序："></a>父类和子类加载顺序：</h3><ol><li>父类静态变量初始化</li><li>父类静态代码块初始化</li><li>子类静态变量初始化</li><li>子类静态代码块初始化</li><li>父类构造方法</li><li>子类构造方法</li></ol><h3 id="规律："><a href="#规律：" class="headerlink" title="规律："></a>规律：</h3><ul><li><strong>父类的静态变量和静态代码块比子类优先初始化</strong></li><li><strong>静态变量和静态代码块比类构造器优先初始化</strong></li></ul><h2 id="final"><a href="#final" class="headerlink" title="final"></a>final</h2><p>定义：不变的，不可改变的</p><h3 id="修饰：-1"><a href="#修饰：-1" class="headerlink" title="修饰："></a>修饰：</h3><ul><li><strong>类：</strong>表示该类是无法被继承的</li><li><strong>方法：</strong>表示该方法是无法被重写（Override）</li><li><strong>变量：</strong>内存地址不可改变，且在声明的时候初始化就必须要完成</li></ul><p>被 final 的修饰的对象，对象的内存地址不可以更改，但是对象中的内容可以更改</p><h2 id="try-amp-catch-amp-finally"><a href="#try-amp-catch-amp-finally" class="headerlink" title="try &amp; catch &amp; finally"></a>try &amp; catch &amp; finally</h2><p>用于捕捉异常的一套流程</p><ul><li>try：用来确定代码指定的范围</li><li>catch：捕捉可能有可能会发生的异常</li><li>finally：用来执行一定要执行的代码块，无论有没有异常发生，总要执行 finally 语句，为程序提供了一个统一的出口，使程序能正常退出</li></ul><p>如果 catch 中发生了异常，finally 还会继续执行，finally 中的代码执行完成后，才会抛出 catch 中的异常</p><h2 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h2><p>用来修饰某个共享变量，意思是当前共享变量的值被修改后，会及时通知到其他线程上，其他线程就能知道当前共享的变量已经被修改了</p><h2 id="transient"><a href="#transient" class="headerlink" title="transient"></a>transient</h2><p>用来修饰类变量，意思是当前变量是无需进行序列化的，在序列化时，就会忽略该变量</p><h2 id="default"><a href="#default" class="headerlink" title="default"></a>default</h2><p>一般用在接口的方法上，意思是对于该接口，实现类无需强制实现，但自己必须有默认实现</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;static&quot;&gt;&lt;a href=&quot;#static&quot; class=&quot;headerlink&quot; title=&quot;static&quot;&gt;&lt;/a&gt;static&lt;/h2&gt;&lt;h3 id=&quot;修饰：&quot;&gt;&lt;a href=&quot;#修饰：&quot; class=&quot;headerlink&quot; title=&quot;修饰：
      
    
    </summary>
    
      <category term="Java" scheme="https://www.chentyit.com/categories/Java/"/>
    
    
      <category term="面试" scheme="https://www.chentyit.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>Java知识点day01(String &amp; Long)</title>
    <link href="https://www.chentyit.com/2019/09/18/Java%E7%9F%A5%E8%AF%86%E7%82%B9day01-String-Long/"/>
    <id>https://www.chentyit.com/2019/09/18/Java知识点day01-String-Long/</id>
    <published>2019-09-18T12:20:36.000Z</published>
    <updated>2019-09-20T07:48:35.792Z</updated>
    
    <content type="html"><![CDATA[<h2 id="String"><a href="#String" class="headerlink" title="String"></a>String</h2><h3 id="不变性"><a href="#不变性" class="headerlink" title="不变性"></a>不变性</h3><p><strong>源码</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">![second_hello](D:\Blog\myblog\source\_posts\Java知识点day01-String-Long\second_hello.png)![second_hello](D:\Blog\myblog\source\_posts\Java知识点day01-String-Long\second_hello.png)<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    String str = <span class="string">"hello"</span>;</span><br><span class="line">    str = <span class="string">"hello"</span>;</span><br><span class="line">    str = <span class="string">"world"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一次 hello</p><p><img src="/2019/09/18/Java知识点day01-String-Long/first_hello.png" alt="first_hello"></p><p>第二次 hello</p><p><img src="/2019/09/18/Java知识点day01-String-Long/second_hello.png" alt="second_hello"></p><p>第三次 world</p><p><img src="/2019/09/18/Java知识点day01-String-Long/third_world.png" alt="third_world"></p><p>可以看出，str 对象并没有变化，但是指向的内存地址改变了</p><p><strong>原因</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">String</span> <span class="keyword">implements</span> <span class="title">java</span>.<span class="title">io</span>.<span class="title">Serializable</span>, <span class="title">Comparable</span>&lt;<span class="title">String</span>&gt;, <span class="title">CharSequence</span> </span>&#123;</span><br><span class="line">    <span class="comment">/** The value is used for character storage. */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">char</span> value[];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>String 类被 <strong>final 修饰</strong>，不可以被继承</li><li>value[] 也被 final 修饰，说明这个是一个常量数组，一旦被赋值，<strong>内存地址</strong>就不可以再修改</li><li>value[] 被声明为<strong>私有属性</strong>，外部无法访问到，也没有 set / get 方法</li></ol><p>所以针对于当前 String 的对象锁做的操作都是无法影响到 value[]，也就是当前对象指向的那个值，比如 replace，split，substring 等等都无法影响到当前值，只有把生成的新值返回给对象才会有效（其实是在字符串常量池中重新生成了一个新的字符串而已，原来的字符串也还在）</p><h3 id="乱码"><a href="#乱码" class="headerlink" title="乱码"></a>乱码</h3><p><strong>原因</strong></p><ol><li>当前所用的编码集不包含当前语言的编码</li><li>二进制转化操作时，并没有强制规定字符编码</li></ol><h3 id="相等判断"><a href="#相等判断" class="headerlink" title="相等判断"></a>相等判断</h3><p><strong>源码</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object anObject)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 先判断两个对象的地址是否相同，相同直接返回 true，否则继续判断</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span> == anObject) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 判断另一个对象到底是不是字符串</span></span><br><span class="line">    <span class="keyword">if</span> (anObject <span class="keyword">instanceof</span> String) &#123;</span><br><span class="line">        <span class="comment">// 转化为字符串类型</span></span><br><span class="line">        String anotherString = (String)anObject;</span><br><span class="line">        <span class="keyword">int</span> n = value.length;</span><br><span class="line">        <span class="comment">// 判断两个字符串的长度</span></span><br><span class="line">        <span class="keyword">if</span> (n == anotherString.value.length) &#123;</span><br><span class="line">            <span class="keyword">char</span> v1[] = value;</span><br><span class="line">            <span class="keyword">char</span> v2[] = anotherString.value;</span><br><span class="line">            <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">            <span class="comment">// 字符串长度相等就一个一个的判断字符串中每个字符是否相等</span></span><br><span class="line">            <span class="keyword">while</span> (n-- != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (v1[i] != v2[i])</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="字符串操作"><a href="#字符串操作" class="headerlink" title="字符串操作"></a>字符串操作</h3><p>建议使用谷歌的第三方工具包 Guava，里面提供了一系列方法，操作类似于 Scala 语言，不过多说明</p><h2 id="Long"><a href="#Long" class="headerlink" title="Long"></a>Long</h2><p>Long 类型有缓存，他实现了一种缓存机制，缓存了从 -128 到 127 内所有的 Lang 值，也就是说如果使用这个范围内的值，直接冲缓存中拿</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">LongCache</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">LongCache</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 缓存，范围从 -128 到 127，+1 是因为还有 0</span></span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">final</span> Long cache[] = <span class="keyword">new</span> Long[-(-<span class="number">128</span>) + <span class="number">127</span> + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 静态代码块，Jvm 加载类的时候优先初始化这部分代码</span></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; cache.length; i++)</span><br><span class="line">            cache[i] = <span class="keyword">new</span> Long(i - <span class="number">128</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用 Long 时，推荐使用 ValueOf 方法，因为 ValueOf 会从缓存中去拿，如果命中缓存，会减小资源的开销</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;String&quot;&gt;&lt;a href=&quot;#String&quot; class=&quot;headerlink&quot; title=&quot;String&quot;&gt;&lt;/a&gt;String&lt;/h2&gt;&lt;h3 id=&quot;不变性&quot;&gt;&lt;a href=&quot;#不变性&quot; class=&quot;headerlink&quot; title=&quot;不变性
      
    
    </summary>
    
      <category term="Java" scheme="https://www.chentyit.com/categories/Java/"/>
    
    
      <category term="面试" scheme="https://www.chentyit.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper安装和Hadoop安装</title>
    <link href="https://www.chentyit.com/2019/09/15/Zookeeper%E5%AE%89%E8%A3%85%E5%92%8CHadoop%E5%AE%89%E8%A3%85/"/>
    <id>https://www.chentyit.com/2019/09/15/Zookeeper安装和Hadoop安装/</id>
    <published>2019-09-15T03:44:51.000Z</published>
    <updated>2019-09-15T03:51:13.612Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Zookeeper-安装"><a href="#Zookeeper-安装" class="headerlink" title="Zookeeper 安装"></a>Zookeeper 安装</h2><p><strong>关闭防火墙！！！</strong></p><h3 id="下载-JDK-1-8-版本"><a href="#下载-JDK-1-8-版本" class="headerlink" title="下载 JDK 1.8 版本"></a>下载 JDK 1.8 版本</h3><h3 id="下载-Zookeeper-安装包"><a href="#下载-Zookeeper-安装包" class="headerlink" title="下载 Zookeeper 安装包"></a>下载 Zookeeper 安装包</h3><p><a href="https://zookeeper.apache.org/releases.html" target="_blank" rel="noopener">下载链接</a></p><h3 id="解压到-Linux-的路径下（一般是-usr-local-）"><a href="#解压到-Linux-的路径下（一般是-usr-local-）" class="headerlink" title="解压到 Linux 的路径下（一般是 /usr/local ）"></a>解压到 Linux 的路径下（一般是 /usr/local ）</h3><h3 id="配置-Linux-环境变量"><a href="#配置-Linux-环境变量" class="headerlink" title="配置 Linux 环境变量"></a>配置 Linux 环境变量</h3><p>配置 JAVA_HOME 和 ZOOKEERER_HOME</p><h3 id="配置-Zookeeper-单节点"><a href="#配置-Zookeeper-单节点" class="headerlink" title="配置  Zookeeper 单节点"></a>配置  Zookeeper 单节点</h3><ol><li><p>在 zookeeper 文件夹下创建 data 文件夹和 logs 文件夹，用于存放数据和日志</p></li><li><p>进入 conf 中，将 zoo_sample.cfg 复制一份并重命名为 zoo.cfg</p></li><li><p>配置 zoo.fg</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">dataLogDir=/usr/local/zookeeper*/logs</span><br><span class="line">dataDir=/usr/local/zookeeper*/data</span><br><span class="line">clientPort=2181</span><br></pre></td></tr></table></figure></li><li><p>配置完成后即可开启单节点模式</p></li></ol><h3 id="配置-Zookeeper-多节点模式（分布式）"><a href="#配置-Zookeeper-多节点模式（分布式）" class="headerlink" title="配置 Zookeeper 多节点模式（分布式）"></a>配置 Zookeeper 多节点模式（分布式）</h3><ol><li><p>先多搞几台虚拟机（具体多少台开心就好，最好 3 台起步，之后用 scp 复制文件到其他虚拟机）</p></li><li><p>配置 hosts 文件</p></li><li><p>配置 ssh 免密登录</p></li><li><p>给所有虚拟机搞上 JDK 和 Zookeeper</p></li><li><p>每台虚拟机的 Zookeeper 的目录下创建 data 和 logs 目录</p></li><li><p>配置 zoo.cfg（和单节点一样搞出来的）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">dataDir=/var/lib/zookeeper</span><br><span class="line">clientPort=2181</span><br><span class="line">initLimit=5</span><br><span class="line">syncLimit=2</span><br><span class="line"><span class="meta">#</span> 主机名、心跳端口、数据端口</span><br><span class="line">server.1=zoo1:2888:3888</span><br><span class="line">server.2=zoo2:2888:3888</span><br><span class="line">server.3=zoo3:2888:3888</span><br></pre></td></tr></table></figure></li><li><p>给每个节点加上 myid（myid 是放到 zookeeper 配置的 dataDir 的路径下的）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iweb5 ： echo '1'&gt;/usr/local/zookeeper/data/myid</span><br><span class="line">iweb6 ： echo '2'&gt;/usr/local/zookeeper/data/myid</span><br><span class="line">iweb7 ： echo '3'&gt;/usr/local/zookeeper/data/myid</span><br></pre></td></tr></table></figure></li><li><p>最后就可以启动集群了</p></li></ol><h2 id="Hadoop-安装"><a href="#Hadoop-安装" class="headerlink" title="Hadoop 安装"></a>Hadoop 安装</h2><p><strong>关闭防火墙！！！</strong></p><p>这玩意儿环境配置老费劲了</p><p>直接跳过单节点模式，要是 hadoop 玩单节点就没有必要搞集群了</p><p><strong>PS：</strong>Hadoop 是在 Zookeeper 环境配置好了的基础上安装的</p><h3 id="JDK-环境变量-SSH-免密-hosts-文件配置"><a href="#JDK-环境变量-SSH-免密-hosts-文件配置" class="headerlink" title="JDK || 环境变量 || SSH 免密 || hosts 文件配置"></a>JDK || 环境变量 || SSH 免密 || hosts 文件配置</h3><p>这里要把 HADOOP_HOME 配置进去，添加到 PATH 里面的时候要把 /bin 和 /sbin 都配置进去</p><h3 id="架设虚拟机（实验用的两台，加上-Zookeeper-一共-5-台）"><a href="#架设虚拟机（实验用的两台，加上-Zookeeper-一共-5-台）" class="headerlink" title="架设虚拟机（实验用的两台，加上 Zookeeper 一共 5 台）"></a>架设虚拟机（实验用的两台，加上 Zookeeper 一共 5 台）</h3><h3 id="开始配置-Hadoop"><a href="#开始配置-Hadoop" class="headerlink" title="开始配置 Hadoop"></a>开始配置 Hadoop</h3><ol><li><p>先配置 hadoop*/etc/hadoop/hadoop-env.sh，将里面的 ${JAVA_HOME} 改成当前 JAVA_HOME 的路径</p></li><li><p>配置 hadoop*/etc/hadoop/core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定hdfs的ns为ns --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultF S<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ns<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop临时目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop-2.8.4/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定zookeeper地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata1:2181,bigdata2:2181,bigdata3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置 hadoop*/etc/hadoop/hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定HDFS副本的数量默认3个 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定hdfs的nameservices为ns，需要和core-site.xml中的保持一致 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>ns<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- ns下面有两个NameNode，分别是nn1，nn2 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- nn1的RPC通信地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata5:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- namenode-1的http通信地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata5:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- namenode-2的RPC通信地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata6:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- namenode-2的http通信地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata6:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://bigdata1:8485;bigdata2:8485;bigdata3:8485/ns<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop-2.8.4/journaldata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 开启NameNode失败自动切换 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置失败自动切换实现方式 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">            sshfence</span><br><span class="line">            shell(/bin/true)</span><br><span class="line">        <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置sshfence隔离机制超时时间 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置 hadoop*/etc/hadoop/mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定mr框架为yarn方式 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置 hadoop*/etc/hadoop/yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 开启RM高可用 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的cluster id --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yrc<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定RM的名字 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 分别指定RM的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定zk集群地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigdata1:2181,bigdata2:2181,bigdata3:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoopMaster:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoopMaster:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoopMaster:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="同步配置"><a href="#同步配置" class="headerlink" title="同步配置"></a>同步配置</h3><p>通过 scp 命令将 hadoop*/ 发送给所有主机</p><h3 id="集群启动准备工作（所有命令官方文档都有）"><a href="#集群启动准备工作（所有命令官方文档都有）" class="headerlink" title="集群启动准备工作（所有命令官方文档都有）"></a>集群启动准备工作（所有命令官方文档都有）</h3><ol><li><p>在 Zookeeper 的主机上启动 journalnode</p><p>（这里我必须要说一句，一定要先启动 journalnode，我就是忘记要启动 journalnode，最后耗费了我一个多小时才启动成功）</p></li><li><p>格式化 namenode （在 active 主机上执行）</p><p>hdfs namenode -format</p></li><li><p>格式化 zkfc（在 active 主机上执行）</p><p>hdfs zkfc -formatZK</p></li><li><p>启动 active 上的 namenode</p><p>hadoop-daemon.sh start namenode</p></li><li><p>在 standby 执行</p><p>hdfs namenode -bootstrapStandby</p></li><li><p>手动启动以下程序</p><p>[ hdfs ]</p><ul><li>namenode</li><li>zkfc</li><li>datanode</li></ul><p>[ yarn ]</p><ul><li>resourcemanager</li><li>nodemanager</li></ul></li><li><p>启动 Web 客户端查看信息</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Zookeeper-安装&quot;&gt;&lt;a href=&quot;#Zookeeper-安装&quot; class=&quot;headerlink&quot; title=&quot;Zookeeper 安装&quot;&gt;&lt;/a&gt;Zookeeper 安装&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;关闭防火墙！！！&lt;/strong&gt;&lt;/p&gt;

      
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Zookeeper" scheme="https://www.chentyit.com/tags/Zookeeper/"/>
    
      <category term="Hadoop" scheme="https://www.chentyit.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper概念</title>
    <link href="https://www.chentyit.com/2019/09/15/Zookeeper%E6%A6%82%E5%BF%B5/"/>
    <id>https://www.chentyit.com/2019/09/15/Zookeeper概念/</id>
    <published>2019-09-15T03:44:26.000Z</published>
    <updated>2019-10-09T01:01:10.220Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>分布式协调服务，为其他的分布式程序提供协调服务</p><p>本身就是分布式程序</p><p>提供的服务包含：</p><ul><li>主从协调</li><li>服务器节点动态上下线</li><li>统一配置管理</li><li>分布式共享锁</li><li>统一名称服务</li></ul><p>底层其实只包含两层服务</p><ul><li><strong>管理</strong>（存储和读取）用户程序提交的<strong>数据</strong></li><li>为用户程序<strong>提供</strong>数据节点<strong>监听服务</strong></li></ul><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ol><li>一个 leader，多个 follower 组成的集群</li><li>全局数据一致：每个 ZK 服务器的数据都是一致的，无论哪个客户端连接到 ZK，获得的数据都是一样的</li><li>分布式读写：更新请求转发，由 leader 实施</li><li>顺序执行：来自同一个客户的更新请求按照发送顺序执行</li><li>数据更新原子性：要么成功，要么失败</li><li>实时性：在一定时间范围内，客户端能得到最新的数据</li></ol><p><img src="/2019/09/15/Zookeeper概念/zkservice.jpg" alt="zkservice"></p><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><ol><li>层次化结构，和文件系统差不多</li><li>每个节点叫做 znode，并且有唯一路径标识</li><li>每个 znode 可以包含数据和子节点（EPHEMERAL 不能有子节点，因为是短暂节点，连接断开后会自己删除）</li><li>客户端可以在节点上设置监视器</li></ol><h2 id="Znode-节点类型"><a href="#Znode-节点类型" class="headerlink" title="Znode 节点类型"></a>Znode 节点类型</h2><ul><li>有两种节点类型<ol><li>短暂（EPHEMERAL）断开连接自己删除</li><li>持久（PERSISTENT）断开连接不删除</li></ol></li><li>org.apache.zookeeper.CreateMode中定义了<strong>四种节点类型</strong><ol><li>PERSISTENT：永久节点</li><li>EPHEMERAL：临时节点</li><li>PERSISTENT_SEQUENTIAL：永久节点、序列化</li><li>EPHEMERAL_SEQUENTIAL：临时节点、序列化</li></ol></li><li>创建 znode 是设置顺序表示，znode 名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护，设置顺序是为了对所有事件进行全局排序，客户端就可以通过顺序推断事件的顺序</li></ul><h2 id="分布式共享锁"><a href="#分布式共享锁" class="headerlink" title="分布式共享锁"></a>分布式共享锁</h2><p><strong>作用：</strong>做到一次只有指定个数的客户端访问服务器的某些资源</p><p>实现步骤：</p><ol><li>客户端上线就向 ZK 注册，创建一把锁</li><li>判断是否只有一个客户端在工作，是则该客户端处理业务</li><li>获取父节点下注册的所有锁，判断自己是否是注册号码最小的，是则处理业务</li></ol><p><strong>当业务处理完成后必须要释放锁</strong></p><h2 id="ZooKeeper-中的时间"><a href="#ZooKeeper-中的时间" class="headerlink" title="ZooKeeper 中的时间"></a>ZooKeeper 中的时间</h2><ul><li><p>Zxid</p><p>致使 ZooKeeper 节点状态改变的每一个操作都将使节点接收到一个 zxid 格式的时间戳，并且<strong>这个时间戳全局有序</strong>。</p><ul><li>cZxid：是节点的<strong>创建时间</strong>所对应的 Zxid 格式时间戳。</li><li>mZxid：是节点的<strong>修改时间</strong>所对应的 Zxid 格式时间戳，与其子节点无关。</li><li>pZxid：<strong>该节点的子节点</strong>（或该节点）的最近一次 创建 / 删除 的修改时间所对应的 cZxid 格式时间戳（注：只与 本节点 / 该节点的子节点，有关；<strong>与孙子节点无关</strong></li></ul></li><li><p>版本号</p><p>对节点的每一个操作都将致使这个节点的版本号增加。每个节点维护着三个版本号，他们分别为：</p><ul><li>version 节点数据版本号</li><li>cversion 子节点版本号</li><li>aversion 节点所拥有的 ACL 版本号</li></ul></li></ul><h2 id="Zookeeper-投票机制"><a href="#Zookeeper-投票机制" class="headerlink" title="Zookeeper 投票机制"></a>Zookeeper 投票机制</h2><p>用例子比较直观（配置 3 台机器）：</p><p>每台机器的 “票” 结构：（myid，zxid）</p><ul><li><p>情况一</p><p>T1（1，0）    T2（2，0）    T3（3，0）</p><p>T1 启动给自己投一票</p><p>T2 启动给自己投一票，收到 T1 的票，并将自己的票发给 T1</p><p>—— 判断（如果 zxid 相同，则 myid 大的作为 leader）T2 作为 leader</p><p>T3 启动已经有 leader 了，不再参与选举直接指定 T2 作为leader</p></li><li><p>情况二</p><p>T1 （1，3）    T2（2，10）    T3（3，5）</p><p>T2 作为 leader 然后嗝屁了</p><p>T1 和 T3 选举</p><p>—— 判断（如果 zxid 不同，则直接判断 zxid，和 myid 无关）T3 作为 leader</p><p>T2 重新上线后由于 T3 已经是 leader，直接指定 T3 为 leader</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;分布式协调服务，为其他的分布式程序提供协调服务&lt;/p&gt;
&lt;p&gt;本身就是分布式程序&lt;/p&gt;
&lt;p&gt;提供的服务包含：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="大数据" scheme="https://www.chentyit.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Zookeeper" scheme="https://www.chentyit.com/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>MyBatis-Plus高级操作</title>
    <link href="https://www.chentyit.com/2019/09/11/MyBatis-Plus%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C/"/>
    <id>https://www.chentyit.com/2019/09/11/MyBatis-Plus高级操作/</id>
    <published>2019-09-11T01:12:02.000Z</published>
    <updated>2019-09-11T07:25:59.331Z</updated>
    
    <content type="html"><![CDATA[<h2 id="推荐视频"><a href="#推荐视频" class="headerlink" title="推荐视频"></a>推荐视频</h2><p><a href="https://www.imooc.com/learn/1130" target="_blank" rel="noopener">MyBatis 入门教程</a></p><p><a href="https://www.imooc.com/learn/1171" target="_blank" rel="noopener">MyBatis 进阶教程</a></p><a id="more"></a><h2 id="逻辑删除"><a href="#逻辑删除" class="headerlink" title="逻辑删除"></a>逻辑删除</h2><h3 id="介绍："><a href="#介绍：" class="headerlink" title="介绍："></a>介绍：</h3><p>修改某一行的数据中的某一列的标志值，用值来表示是否已经 <strong>“逻辑删除”</strong> 并不是真的在表中真正的删除这条数据</p><ol><li><p>先在 application.yml 中配置标志值</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mybatis-plus:</span></span><br><span class="line"><span class="attr">  global-config:</span></span><br><span class="line"><span class="attr">    db-config:</span></span><br><span class="line">      <span class="comment"># 未删除的值</span></span><br><span class="line"><span class="attr">      logic-not-delete-value:</span> <span class="number">0</span></span><br><span class="line">      <span class="comment"># 已删除的值</span></span><br><span class="line"><span class="attr">      logic-delete-value:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure></li><li><p>创建配置类 MyBatisPlusConfiguration.class</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBatisPlusConfiguration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 在 mybatis-plus 3.1.0 以上的版本中不再需要添加这一步</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ISqlInjector <span class="title">sqlInjector</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> LogicSqlInjector();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在实体类中添加注解 @TableLogic</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@TableName</span>(<span class="string">"user_pro"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Long id;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> Integer age;</span><br><span class="line">    <span class="keyword">private</span> String email;</span><br><span class="line">    <span class="keyword">private</span> Long managerId;</span><br><span class="line">    <span class="keyword">private</span> LocalDateTime createTime;</span><br><span class="line">    <span class="keyword">private</span> LocalDateTime updateTime;</span><br><span class="line">    <span class="keyword">private</span> Integer version;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@TableLogic</span></span><br><span class="line">    <span class="meta">@TableField</span>(select = <span class="keyword">false</span>, value = <span class="string">"deleted"</span>)</span><br><span class="line">    <span class="keyword">private</span> Integer deleted;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="使用："><a href="#使用：" class="headerlink" title="使用："></a>使用：</h3><p>这些方法在使用的时候都会自己在后面添加一个 and deleted = 0，表示只操作删除标志位为 0（未删除） 的数据</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.chentyit.mp2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cn.chentyit.mp2.dao.UserMapper;</span><br><span class="line"><span class="keyword">import</span> cn.chentyit.mp2.entity.User;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"><span class="keyword">import</span> org.junit.runner.RunWith;</span><br><span class="line"><span class="keyword">import</span> org.springframework.boot.test.context.SpringBootTest;</span><br><span class="line"><span class="keyword">import</span> org.springframework.test.context.junit4.SpringRunner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.annotation.Resource;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 09:22</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RunWith</span>(SpringRunner.class)</span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyLogicDelete</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Resource</span></span><br><span class="line">    <span class="keyword">private</span> UserMapper userMapper;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 会把删除标志位置 1</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteById</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> rows = userMapper.deleteById(<span class="number">1094592041087729666L</span>);</span><br><span class="line">        System.out.println(<span class="string">"影响行数："</span> + rows);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 只查询出删除标志位为 0 的数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">select</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        List&lt;User&gt; users = userMapper.selectList(<span class="keyword">null</span>);</span><br><span class="line">        users.forEach(System.out::println);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 只更新标志位为 0 的数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">updateById</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        User user = <span class="keyword">new</span> User();</span><br><span class="line">        user.setAge(<span class="number">26</span>);</span><br><span class="line">        user.setId(<span class="number">1088248166370832385L</span>);</span><br><span class="line">        <span class="keyword">int</span> rows = userMapper.updateById(user);</span><br><span class="line">        System.out.println(<span class="string">"影响行数："</span> + rows);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="自动填充"><a href="#自动填充" class="headerlink" title="自动填充"></a>自动填充</h2><h3 id="介绍：-1"><a href="#介绍：-1" class="headerlink" title="介绍："></a>介绍：</h3><p>自动填充值以及一些额外的数据</p><h3 id="实现："><a href="#实现：" class="headerlink" title="实现："></a>实现：</h3><ol><li><p>先在实体类中添加注解 @TableField(fill = FieldFill.INSERT) 和 @TableField(fill = FieldFill.UPDATE)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@TableName</span>(<span class="string">"user_pro"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Long id;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Integer age;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String email;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Long managerId;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@TableField</span>(fill = FieldFill.INSERT)</span><br><span class="line">    <span class="keyword">private</span> LocalDateTime createTime;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@TableField</span>(fill = FieldFill.UPDATE)</span><br><span class="line">    <span class="keyword">private</span> LocalDateTime updateTime;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Integer version;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@TableLogic</span></span><br><span class="line">    <span class="meta">@TableField</span>(select = <span class="keyword">false</span>, value = <span class="string">"deleted"</span>)</span><br><span class="line">    <span class="keyword">private</span> Integer deleted;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>实现 MetaObjectHandler 接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 10:13</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMetaObjectHandler</span> <span class="keyword">implements</span> <span class="title">MetaObjectHandler</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 在插入的时候填充</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> metaObject</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insertFill</span><span class="params">(MetaObject metaObject)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 判断数据库中是否有这个列</span></span><br><span class="line">        <span class="keyword">boolean</span> hasSetter = metaObject.hasSetter(<span class="string">"createTime1"</span>);</span><br><span class="line">        <span class="comment">// 如果有就向这个列插入数据</span></span><br><span class="line">        <span class="comment">// 如果没有就不插</span></span><br><span class="line">        <span class="keyword">if</span> (hasSetter) &#123;</span><br><span class="line">            setInsertFieldValByName(<span class="string">"createTime"</span>, LocalDateTime.now(), metaObject);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 在更新的时候填充</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> metaObject</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">updateFill</span><span class="params">(MetaObject metaObject)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 判断是否已经设置了值</span></span><br><span class="line">        Object val = getFieldValByName(<span class="string">"updateTime"</span>, metaObject);</span><br><span class="line">        <span class="comment">// 如果设置了就不进行自动填充</span></span><br><span class="line">        <span class="keyword">if</span> (val == <span class="keyword">null</span>) &#123;</span><br><span class="line">            setUpdateFieldValByName(<span class="string">"updateTime"</span>, LocalDateTime.now(), metaObject);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>测试</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 10:39</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RunWith</span>(SpringRunner.class)</span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FillTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Resource</span></span><br><span class="line">    <span class="keyword">private</span> UserMapper userMapper;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        User user = <span class="keyword">new</span> User();</span><br><span class="line">        user.setName(<span class="string">"陈天翼"</span>);</span><br><span class="line">        user.setAge(<span class="number">21</span>);</span><br><span class="line">        user.setEmail(<span class="string">"chentyit@qq.com"</span>);</span><br><span class="line">        user.setManagerId(<span class="number">1088248166370832385L</span>);</span><br><span class="line">        <span class="keyword">int</span> rows = userMapper.insert(user);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"影响行数："</span> + rows);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">updateById</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        User user = <span class="keyword">new</span> User();</span><br><span class="line">        user.setAge(<span class="number">27</span>);</span><br><span class="line">        user.setId(<span class="number">1171615225418350594L</span>);</span><br><span class="line">        <span class="keyword">int</span> rows = userMapper.updateById(user);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"影响行数："</span> + rows);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="乐观锁插件"><a href="#乐观锁插件" class="headerlink" title="乐观锁插件"></a>乐观锁插件</h2><p>（注：多写的情况下使用悲观锁，多读的场景使用乐观锁）</p><h3 id="介绍：-2"><a href="#介绍：-2" class="headerlink" title="介绍："></a>介绍：</h3><p>取出记录时，获取当前 version</p><p>更新时，带上这个 version</p><p>版本正确更新成功，错误更新失败</p><h3 id="使用：-1"><a href="#使用：-1" class="headerlink" title="使用："></a>使用：</h3><ol><li><p>添加插件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBatisPlusConfiguration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 添加乐观锁插件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> OptimisticLockerInterceptor <span class="title">optimisticLockerInterceptor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> OptimisticLockerInterceptor();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在实体类中的版本属性上面添加 @Verison</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@TableName</span>(<span class="string">"user_pro"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ****</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Version</span></span><br><span class="line">    <span class="keyword">private</span> Integer version;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// ****</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>测试</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 11:16</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@RunWith</span>(SpringRunner.class)</span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OptTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Resource</span></span><br><span class="line">    <span class="keyword">private</span> UserMapper userMapper;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 如果使用条件构造器 则添加构造器不能复用</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">updateById</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> version = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">        User user = <span class="keyword">new</span> User();</span><br><span class="line">        user.setEmail(<span class="string">"chentyit2@qq.com"</span>);</span><br><span class="line">        user.setId(<span class="number">1171617683297271809L</span>);</span><br><span class="line">        user.setVersion(version);</span><br><span class="line">        <span class="keyword">int</span> rows = userMapper.updateById(user);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"影响行数："</span> + rows);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="性能分析插件"><a href="#性能分析插件" class="headerlink" title="性能分析插件"></a>性能分析插件</h2><h3 id="介绍：-3"><a href="#介绍：-3" class="headerlink" title="介绍："></a>介绍：</h3><p>输出每条 SQL 语句的执行时间，只在测试和开发环境使用，因为开销很大</p><h3 id="使用：-2"><a href="#使用：-2" class="headerlink" title="使用："></a>使用：</h3><ol><li><p>在配置类中添加性能分析插件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBatisPlusConfiguration</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ******</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 添加性能分析插件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="meta">@Profile</span>(&#123;<span class="string">"dev"</span>, <span class="string">"test"</span>&#125;)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> PerformanceInterceptor <span class="title">performanceInterceptor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        PerformanceInterceptor performanceInterceptor = <span class="keyword">new</span> PerformanceInterceptor();</span><br><span class="line">        <span class="comment">// 格式化 SQL 语句</span></span><br><span class="line">        performanceInterceptor.setFormat(<span class="keyword">true</span>);</span><br><span class="line">        <span class="comment">// 设置执行最大时间</span></span><br><span class="line">        performanceInterceptor.setMaxTime(<span class="number">5</span>);</span><br><span class="line">        <span class="keyword">return</span> performanceInterceptor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>配置 JVM 参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 表示使用开发环境</span></span><br><span class="line">-Dspring.profiles.active=dev</span><br></pre></td></tr></table></figure></li></ol><h3 id="执行-SQL-分析打印"><a href="#执行-SQL-分析打印" class="headerlink" title="执行 SQL 分析打印"></a>执行 SQL 分析打印</h3><ol><li><p>先添加依赖包</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>p6spy<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>p6spy<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.8.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>修改配置文件</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">driver-class-name:</span> <span class="string">com.p6spy.engine.spy.P6SpyDriver</span></span><br><span class="line"><span class="attr">url:</span> <span class="attr">jdbc:p6spy:mysql://127.0.0.1:3306/mp?characterEncoding=utf8&amp;useSSL=true&amp;serverTimezone=Asia/Shanghai</span></span><br></pre></td></tr></table></figure></li><li><p>添加配置文件 spy.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">module.log=com.p6spy.engine.logging.P6LogFactory,com.p6spy.engine.outage.P6OutageFactory</span><br><span class="line"># 自定义日志打印</span><br><span class="line">logMessageFormat=com.baomidou.mybatisplus.extension.p6spy.P6SpyLogger</span><br><span class="line"># 日志输出到控制台</span><br><span class="line">appender=com.baomidou.mybatisplus.extension.p6spy.StdoutLogger</span><br><span class="line"># 使用日志系统记录 sql</span><br><span class="line">#appender=com.p6spy.engine.spy.appender.Slf4JLogger</span><br><span class="line"># 设置 p6spy driver 代理</span><br><span class="line">deregisterdrivers=true</span><br><span class="line"># 取消JDBC URL前缀</span><br><span class="line">useprefix=true</span><br><span class="line"># 配置记录 Log 例外,可去掉的结果集有error,info,batch,debug,statement,commit,rollback,result,resultset.</span><br><span class="line">excludecategories=info,debug,result,batch,resultset</span><br><span class="line"># 日期格式</span><br><span class="line">dateformat=yyyy-MM-dd HH:mm:ss</span><br><span class="line"># 实际驱动可多个</span><br><span class="line">#driverlist=org.h2.Driver</span><br><span class="line"># 是否开启慢SQL记录</span><br><span class="line">outagedetection=true</span><br><span class="line"># 慢SQL记录标准 2 秒</span><br><span class="line">outagedetectioninterval=2</span><br></pre></td></tr></table></figure></li></ol><h3 id="修改日志输出路径"><a href="#修改日志输出路径" class="headerlink" title="修改日志输出路径"></a>修改日志输出路径</h3><ol><li><p>先修改配置文件 spy.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 日志输出到控制台</span><br><span class="line"># appender=com.baomidou.mybatisplus.extension.p6spy.StdoutLogger</span><br></pre></td></tr></table></figure></li><li><p>再添加参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logfile=log.log</span><br></pre></td></tr></table></figure></li></ol><p>日志就会打印到 log.log 文件里面了</p><h2 id="多租户"><a href="#多租户" class="headerlink" title="多租户"></a>多租户</h2><h3 id="介绍：-4"><a href="#介绍：-4" class="headerlink" title="介绍："></a>介绍：</h3><p>多个用户间使用同一套程序，但每个用户之间实现数据隔离</p><h3 id="实现：-1"><a href="#实现：-1" class="headerlink" title="实现："></a>实现：</h3><ol><li><p>添加分页插件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 09:18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBatisPlusConfiguration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// **********</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 添加分页插件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> PaginationInterceptor <span class="title">paginationInterceptor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        PaginationInterceptor paginationInterceptor = <span class="keyword">new</span> PaginationInterceptor();</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;ISqlParser&gt; sqlParserList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        TenantSqlParser tenantSqlParser = <span class="keyword">new</span> TenantSqlParser();</span><br><span class="line">        tenantSqlParser.setTenantHandler(<span class="keyword">new</span> TenantHandler() &#123;</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 添加租户信息的值</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Expression <span class="title">getTenantId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> LongValue(<span class="number">1088248166370832385L</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 哪个字段添加信息</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> String <span class="title">getTenantIdColumn</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="string">"manager_id"</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 是否向某个表中添加租户信息</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> tableName</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@return</span> true 表示过滤掉，不增加； false 表示不过滤，添加租户信息</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">doTableFilter</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="string">"user_pro"</span>.equals(tableName)) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        sqlParserList.add(tenantSqlParser);</span><br><span class="line">        paginationInterceptor.setSqlParserList(sqlParserList);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> paginationInterceptor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="特定-SQL-过滤"><a href="#特定-SQL-过滤" class="headerlink" title="特定 SQL 过滤"></a>特定 SQL 过滤</h3><ul><li><p>方法一：在上面的代码中添加以下代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">paginationInterceptor.setSqlParserFilter(<span class="keyword">new</span> ISqlParserFilter() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 是否执行过滤</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> metaObject</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> false 代表不增加过滤信息；true 代表增加过滤信息</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">doFilter</span><span class="params">(MetaObject metaObject)</span> </span>&#123;</span><br><span class="line">        MappedStatement ms = SqlParserHelper.getMappedStatement(metaObject);</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"cn.chentyit.mp2.dao.UserMapper.selectById"</span>.equals(ms.getId())) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></li><li><p>方法二：在 Mapper 的自定义方法上添加注解 @SqlParser(filter = true)，在查询的时候不需要添加租户信息</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 09:09</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">UserMapper</span> <span class="keyword">extends</span> <span class="title">BaseMapper</span>&lt;<span class="title">User</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SqlParser</span>(filter = <span class="keyword">true</span>)</span><br><span class="line">    <span class="meta">@Select</span>(<span class="string">"select * from user_pro $&#123;ew.customSqlSegment&#125;"</span>)</span><br><span class="line">    <span class="function">List&lt;User&gt; <span class="title">mySelectList</span><span class="params">(@Param(Constants.WRAPPER)</span>Wrapper&lt;User&gt; wrapper)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ol><h2 id="动态表名-SQL-解析器"><a href="#动态表名-SQL-解析器" class="headerlink" title="动态表名 SQL 解析器"></a>动态表名 SQL 解析器</h2><h3 id="介绍：-5"><a href="#介绍：-5" class="headerlink" title="介绍："></a>介绍：</h3><p>分表存储</p><h3 id="使用：-3"><a href="#使用：-3" class="headerlink" title="使用："></a>使用：</h3><ol><li><p>添加动态表名插件</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 09:18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyBatisPlusConfiguration</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> ThreadLocal&lt;String&gt; myTableName = <span class="keyword">new</span> ThreadLocal&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ******************</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 添加分页插件</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> PaginationInterceptor <span class="title">paginationInterceptor</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        PaginationInterceptor paginationInterceptor = <span class="keyword">new</span> PaginationInterceptor();</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;ISqlParser&gt; sqlParserList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置动态表名</span></span><br><span class="line">        DynamicTableNameParser dynamicTableNameParser =  <span class="keyword">new</span> DynamicTableNameParser();</span><br><span class="line">        Map&lt;String, ITableNameHandler&gt; tableNameHandlerMap = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">16</span>);</span><br><span class="line">        <span class="comment">// 返回值为 MyBatisPlusConfiguration.myTableName.set("user_2019"); 设置的表名</span></span><br><span class="line">        tableNameHandlerMap.put(<span class="string">"user_pro"</span>, (metaObject, sql, tableName) -&gt; myTableName.get());</span><br><span class="line">        dynamicTableNameParser.setTableNameHandlerMap(tableNameHandlerMap);</span><br><span class="line">        sqlParserList.add(dynamicTableNameParser);</span><br><span class="line"></span><br><span class="line">        paginationInterceptor.setSqlParserList(sqlParserList);</span><br><span class="line">        paginationInterceptor.setSqlParserFilter(<span class="keyword">new</span> ISqlParserFilter() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 是否执行过滤</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> metaObject</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@return</span> false 代表不增加过滤信息；true 代表增加过滤信息</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">doFilter</span><span class="params">(MetaObject metaObject)</span> </span>&#123;</span><br><span class="line">                MappedStatement ms = SqlParserHelper.getMappedStatement(metaObject);</span><br><span class="line">                <span class="keyword">if</span> (<span class="string">"cn.chentyit.mp2.dao.UserMapper.selectById"</span>.equals(ms.getId())) &#123;</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> paginationInterceptor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在代码中调用</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">select</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 设置表名</span></span><br><span class="line">    MyBatisPlusConfiguration.myTableName.set(<span class="string">"user_2019"</span>);</span><br><span class="line">    List&lt;User&gt; users = userMapper.selectList(<span class="keyword">null</span>);</span><br><span class="line">    users.forEach(System.out::println);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="SQL-注入器"><a href="#SQL-注入器" class="headerlink" title="SQL 注入器"></a>SQL 注入器</h2><h3 id="介绍：-6"><a href="#介绍：-6" class="headerlink" title="介绍："></a>介绍：</h3><p>自定义 SQL</p><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ol><li><p>创建定义方法的类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 14:55</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DeleteAllMethod</span> <span class="keyword">extends</span> <span class="title">AbstractMethod</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> MappedStatement <span class="title">injectMappedStatement</span><span class="params">(Class&lt;?&gt; mapperClass, Class&lt;?&gt; modelClass, TableInfo tableInfo)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 指定的 SQL</span></span><br><span class="line">        String sql = <span class="string">"delete from "</span> + tableInfo.getTableName();</span><br><span class="line">        <span class="comment">// mapper 接口方法名</span></span><br><span class="line">        String method = <span class="string">"deleteAll"</span>;</span><br><span class="line"></span><br><span class="line">        SqlSource sqlSource = languageDriver.createSqlSource(configuration, sql, modelClass);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> addDeleteMappedStatement(mapperClass, method, sqlSource);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>创建注入器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 14:58</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySqlInjector</span> <span class="keyword">extends</span> <span class="title">DefaultSqlInjector</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;AbstractMethod&gt; <span class="title">getMethodList</span><span class="params">(Class&lt;?&gt; mapperClass)</span> </span>&#123;</span><br><span class="line">        List&lt;AbstractMethod&gt; methodList = <span class="keyword">super</span>.getMethodList(mapperClass);</span><br><span class="line">        methodList.add(<span class="keyword">new</span> DeleteAllMethod());</span><br><span class="line">        <span class="keyword">return</span> methodList;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>在 Mapper 中加入自定义方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 15:09</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">MyMapper</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">BaseMapper</span>&lt;<span class="title">T</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">deleteAll</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span> Chentyit</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span> 2019/9/11 09:09</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>:</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">UserMapper</span> <span class="keyword">extends</span> <span class="title">MyMapper</span>&lt;<span class="title">User</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SqlParser</span>(filter = <span class="keyword">true</span>)</span><br><span class="line">    <span class="meta">@Select</span>(<span class="string">"select * from user_pro $&#123;ew.customSqlSegment&#125;"</span>)</span><br><span class="line">    <span class="function">List&lt;User&gt; <span class="title">mySelectList</span><span class="params">(@Param(Constants.WRAPPER)</span>Wrapper&lt;User&gt; wrapper)</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;推荐视频&quot;&gt;&lt;a href=&quot;#推荐视频&quot; class=&quot;headerlink&quot; title=&quot;推荐视频&quot;&gt;&lt;/a&gt;推荐视频&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.imooc.com/learn/1130&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MyBatis 入门教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.imooc.com/learn/1171&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MyBatis 进阶教程&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Java" scheme="https://www.chentyit.com/categories/Java/"/>
    
    
      <category term="MyBatis-Plus" scheme="https://www.chentyit.com/tags/MyBatis-Plus/"/>
    
  </entry>
  
</feed>
